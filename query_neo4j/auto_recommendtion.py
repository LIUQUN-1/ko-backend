import sys
import time
import hashlib
from django.http import HttpResponse, Http404, StreamingHttpResponse
from django.utils.encoding import escape_uri_path
import zipfile
import os
import tqdm
from minio import Minio, InvalidResponseError, S3Error
from neo4j import GraphDatabase
import zhconv
import shutil
import json
import chardet
import pymysql
import ctypes
from query_neo4j.search_urls import search_urls
import os
import re
import time
import json
import requests
from typing import Optional, Dict
from django.http import HttpRequest  # æ·»åŠ è¿™è¡Œå¯¼å…¥
from django.http import HttpResponse as DjangoHttpResponse  # å¦‚æœéœ€è¦

class MySQLDatabase:
    def __init__(self, host, user, password, database, charset="utf8mb4"):
        """
        åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        """
        self.config = {
            "host": host,
            "user": user,
            "password": password,
            "database": database,
            "charset": charset
        }
        self.connection = None

    def connect(self):
        """
        å»ºç«‹æ•°æ®åº“è¿æ¥
        """
        try:
            self.connection = pymysql.connect(**self.config)
            print("æ•°æ®åº“è¿æ¥æˆåŠŸï¼")
        except pymysql.MySQLError as e:
            print(f"æ•°æ®åº“è¿æ¥å¤±è´¥ï¼š{e}")
            raise

    def insert_data(self, table_name, data):
        try:
            # å…ˆæ£€æŸ¥ä¸»é”®æ˜¯å¦å­˜åœ¨
            primary_key = list(data.keys())[0]  # å‡è®¾ä¸»é”®åœ¨ç¬¬ä¸€ä¸ªä½ç½®
            primary_key_value = data[primary_key]

            # ç”Ÿæˆæ£€æŸ¥ä¸»é”®æ˜¯å¦å­˜åœ¨çš„ SQL æŸ¥è¯¢
            check_query = f"SELECT COUNT(*) FROM {table_name} WHERE {primary_key} = %s"
            with self.connection.cursor() as cursor:
                cursor.execute(check_query, (primary_key_value,))
                result = cursor.fetchone()

                if result[0] > 0:
                    print(f"ä¸»é”® {primary_key_value} å·²å­˜åœ¨ï¼Œè·³è¿‡æ’å…¥æ“ä½œã€‚")
                    return  # ä¸»é”®å·²å­˜åœ¨ï¼Œè·³è¿‡æ’å…¥æ“ä½œ

            # ç”Ÿæˆæ’å…¥ SQL è¯­å¥
            columns = ", ".join(data.keys())
            placeholders = ", ".join(["%s"] * len(data))
            insert_query = f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})"

            # æ‰§è¡Œæ’å…¥æ“ä½œ
            with self.connection.cursor() as cursor:
                cursor.execute(insert_query, tuple(data.values()))
                self.connection.commit()
                print("æ•°æ®æ’å…¥æˆåŠŸï¼")
        except pymysql.MySQLError as e:
            print(f"æ’å…¥æ•°æ®å¤±è´¥ï¼š{e}")
            self.connection.rollback()  # å›æ»šäº‹åŠ¡

    def insert_relation(self, table_name, data):
        try:
            # å…ˆæ£€æŸ¥ä¸»é”®æ˜¯å¦å­˜åœ¨
            primary_key = list(data.keys())[0]  # å‡è®¾ä¸»é”®åœ¨ç¬¬ä¸€ä¸ªä½ç½®
            last_key = list(data.keys())[-1]
            primary_key_value = data[primary_key]
            last_key_value = data[last_key]

            # ç”Ÿæˆæ£€æŸ¥ä¸»é”®æ˜¯å¦å­˜åœ¨çš„ SQL æŸ¥è¯¢
            check_query = f"SELECT COUNT(*) FROM {table_name} WHERE {primary_key} = %s AND {last_key} = %s "
            with self.connection.cursor() as cursor:
                cursor.execute(check_query, (primary_key_value, last_key_value,))
                result = cursor.fetchone()

                if result[0] > 0:
                    print(f"ä¸»é”® {primary_key_value} å·²å­˜åœ¨ï¼Œè·³è¿‡æ’å…¥æ“ä½œã€‚")
                    return  # ä¸»é”®å·²å­˜åœ¨ï¼Œè·³è¿‡æ’å…¥æ“ä½œ

            # ç”Ÿæˆæ’å…¥ SQL è¯­å¥
            columns = ", ".join(data.keys())
            placeholders = ", ".join(["%s"] * len(data))
            insert_query = f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})"

            # æ‰§è¡Œæ’å…¥æ“ä½œ
            with self.connection.cursor() as cursor:
                cursor.execute(insert_query, tuple(data.values()))
                self.connection.commit()
                print("æ•°æ®æ’å…¥æˆåŠŸï¼")
        except pymysql.MySQLError as e:
            print(f"æ’å…¥æ•°æ®å¤±è´¥ï¼š{e}")
            self.connection.rollback()  # å›æ»šäº‹åŠ¡

    def query_tables(self, query):
        try:
            with self.connection.cursor() as cursor:
                cursor.execute(query)
                result = cursor.fetchall()
                return result
        except pymysql.MySQLError as e:
            print(f"æŸ¥è¯¢å¤±è´¥ï¼š{e}")
            raise

    def close(self):
        """
        å…³é—­æ•°æ®åº“è¿æ¥
        """
        if self.connection:
            self.connection.close()
            print("æ•°æ®åº“è¿æ¥å·²å…³é—­ï¼")


class Bucket:

    def __init__(self, minio_address, minio_admin, minio_password):
        # é€šè¿‡ip è´¦å· å¯†ç  è¿æ¥minio server
        # Httpè¿æ¥ å°†secureè®¾ç½®ä¸ºFalse
        self.minioClient = Minio(endpoint=minio_address,
                                 access_key=minio_admin,
                                 secret_key=minio_password,
                                 secure=False)

    def create_one_bucket(self, bucket_name):
        # åˆ›å»ºæ¡¶(è°ƒç”¨make_bucket apiæ¥åˆ›å»ºä¸€ä¸ªæ¡¶)
        """
        æ¡¶å‘½åè§„åˆ™ï¼šå°å†™å­—æ¯ï¼Œå¥ç‚¹ï¼Œè¿å­—ç¬¦å’Œæ•°å­— å…è®¸ä½¿ç”¨ é•¿åº¦è‡³å°‘3ä¸ªå­—ç¬¦
        ä½¿ç”¨å¤§å†™å­—æ¯ã€ä¸‹åˆ’çº¿ç­‰ä¼šæŠ¥é”™
        """
        try:
            # bucket_existsï¼šæ£€æŸ¥æ¡¶æ˜¯å¦å­˜åœ¨
            if self.minioClient.bucket_exists(bucket_name=bucket_name):
                print("è¯¥å­˜å‚¨æ¡¶å·²ç»å­˜åœ¨")
            else:
                self.minioClient.make_bucket(bucket_name=bucket_name)
                print(f"{bucket_name}æ¡¶åˆ›å»ºæˆåŠŸ")
        except InvalidResponseError as err:
            print(err)

    def remove_one_bucket(self, bucket_name):
        # åˆ é™¤æ¡¶(è°ƒç”¨remove_bucket apiæ¥åˆ›å»ºä¸€ä¸ªå­˜å‚¨æ¡¶)
        try:
            if self.minioClient.bucket_exists(bucket_name=bucket_name):
                self.minioClient.remove_bucket(bucket_name)
                print("åˆ é™¤å­˜å‚¨æ¡¶æˆåŠŸ")
            else:
                print("è¯¥å­˜å‚¨æ¡¶ä¸å­˜åœ¨")
        except InvalidResponseError as err:
            print(err)

    def upload_file_to_bucket(self, bucket_name, file_name, file_path):
        """
        å°†æ–‡ä»¶ä¸Šä¼ åˆ°bucket
        :param bucket_name: minioæ¡¶åç§°
        :param file_name: å­˜æ”¾åˆ°minioæ¡¶ä¸­çš„æ–‡ä»¶åå­—(ç›¸å½“äºå¯¹æ–‡ä»¶è¿›è¡Œäº†é‡å‘½åï¼Œå¯ä»¥ä¸åŸæ–‡ä»¶åä¸åŒ)
                            file_nameå¤„å¯ä»¥åˆ›å»ºæ–°çš„ç›®å½•(æ–‡ä»¶å¤¹) ä¾‹å¦‚ /example/file_name
                            ç›¸å½“äºåœ¨è¯¥æ¡¶ä¸­æ–°å»ºäº†ä¸€ä¸ªexampleæ–‡ä»¶å¤¹ å¹¶æŠŠæ–‡ä»¶æ”¾åœ¨å…¶ä¸­
        :param file_path: æœ¬åœ°æ–‡ä»¶çš„è·¯å¾„
        """
        # æ¡¶æ˜¯å¦å­˜åœ¨ ä¸å­˜åœ¨åˆ™æ–°å»º
        check_bucket = self.minioClient.bucket_exists(bucket_name)
        if not check_bucket:
            self.minioClient.make_bucket(bucket_name)

        try:
            self.minioClient.fput_object(bucket_name=bucket_name,
                                         object_name=file_name,
                                         file_path=file_path)
        except FileNotFoundError as err:
            print('upload_failed: ' + str(err))
        except S3Error as err:
            print("upload_failed:", err)

    def download_file_from_bucket(self, bucket_name, minio_file_path, download_file_path):
        """
        ä»bucketä¸‹è½½æ–‡ä»¶
        :param bucket_name: minioæ¡¶åç§°
        :param minio_file_path: å­˜æ”¾åœ¨minioæ¡¶ä¸­æ–‡ä»¶åå­—
                            file_nameå¤„å¯ä»¥åŒ…å«ç›®å½•(æ–‡ä»¶å¤¹) ä¾‹å¦‚ /example/file_name
        :param download_file_path: æ–‡ä»¶è·å–åå­˜æ”¾çš„è·¯å¾„
        """
        # æ¡¶æ˜¯å¦å­˜åœ¨
        check_bucket = self.minioClient.bucket_exists(bucket_name)
        if check_bucket:
            try:
                self.minioClient.fget_object(bucket_name=bucket_name,
                                             object_name=minio_file_path,
                                             file_path=download_file_path)
                return 1
            except FileNotFoundError as err:
                print('download_failed: ' + str(err))
                return 0
            except S3Error as err:
                print("download_failed:", err)
                return 0

    def remove_object(self, bucket_name, object_name):
        """
        ä»bucketåˆ é™¤æ–‡ä»¶
        :param bucket_name: minioæ¡¶åç§°
        :param object_name: å­˜æ”¾åœ¨minioæ¡¶ä¸­çš„æ–‡ä»¶åå­—
                            object_nameå¤„å¯ä»¥åŒ…å«ç›®å½•(æ–‡ä»¶å¤¹) ä¾‹å¦‚ /example/file_name
        """
        # æ¡¶æ˜¯å¦å­˜åœ¨
        check_bucket = self.minioClient.bucket_exists(bucket_name)
        if check_bucket:
            try:
                self.minioClient.remove_object(bucket_name=bucket_name,
                                               object_name=object_name)
            except FileNotFoundError as err:
                print('upload_failed: ' + str(err))
            except S3Error as err:
                print("upload_failed:", err)

    # è·å–æ‰€æœ‰çš„æ¡¶
    def get_all_bucket(self):
        buckets = self.minioClient.list_buckets()
        ret = []
        for _ in buckets:
            ret.append(_.name)
        return ret

    # è·å–ä¸€ä¸ªæ¡¶ä¸­çš„æ‰€æœ‰ä¸€çº§ç›®å½•å’Œæ–‡ä»¶
    def get_list_objects_from_bucket(self, bucket_name):
        # æ¡¶æ˜¯å¦å­˜åœ¨
        check_bucket = self.minioClient.bucket_exists(bucket_name)
        if check_bucket:
            # è·å–åˆ°è¯¥æ¡¶ä¸­çš„æ‰€æœ‰ç›®å½•å’Œæ–‡ä»¶
            objects = self.minioClient.list_objects(bucket_name=bucket_name)
            ret = []
            for _ in objects:
                ret.append(_.object_name)
            return ret

    # è·å–æ¡¶é‡ŒæŸä¸ªç›®å½•ä¸‹çš„æ‰€æœ‰ç›®å½•å’Œæ–‡ä»¶
    def get_list_objects_from_bucket_dir(self, bucket_name, dir_name):
        # æ¡¶æ˜¯å¦å­˜åœ¨
        check_bucket = self.minioClient.bucket_exists(bucket_name)
        if check_bucket:
            # è·å–åˆ°bucket_æ‰€nameæ¡¶ä¸­çš„dir_nameä¸‹çš„æœ‰ç›®å½•å’Œæ–‡ä»¶
            # prefix è·å–çš„æ–‡ä»¶è·¯å¾„éœ€åŒ…å«è¯¥å‰ç¼€
            objects = self.minioClient.list_objects(bucket_name=bucket_name,
                                                    prefix=dir_name,
                                                    recursive=True)
            ret = []
            for obj in objects:
                object_name = obj.object_name
                # è·å–å¯¹è±¡çš„å†…å®¹
                content = self.minioClient.get_object(bucket_name=bucket_name,
                                                      object_name=object_name)
                ret.append(content.data.decode())
            return ret


def get_sha1_hash(file_name):
    shal_hash = hashlib.sha1(file_name.encode()).hexdigest()
    return shal_hash


def file_iterator(file_path, chunk_size=512):
    """
    æ–‡ä»¶ç”Ÿæˆå™¨,é˜²æ­¢æ–‡ä»¶è¿‡å¤§ï¼Œå¯¼è‡´å†…å­˜æº¢å‡º
    :param file_path: æ–‡ä»¶ç»å¯¹è·¯å¾„
    :param chunk_size: å—å¤§å°
    :return: ç”Ÿæˆå™¨
    """
    with open(file_path, mode='rb') as f:
        while True:
            c = f.read(chunk_size)
            if c:
                yield c
            else:
                break


def deletezip(directory):
    for filename in os.listdir(directory):
        # åˆ¤æ–­æ˜¯å¦ä»¥ .zip ç»“å°¾
        if filename.endswith('.zip'):
            # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
            file_path = os.path.join(directory, filename)
            # åˆ é™¤æ–‡ä»¶
            os.remove(file_path)


def zipDir(dirpath, outFullName):
    """
    å‹ç¼©æŒ‡å®šæ–‡ä»¶å¤¹
    :param dirpath: ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„
    :param outFullName: å‹ç¼©æ–‡ä»¶ä¿å­˜è·¯å¾„+xxxx.zip
    :return: æ— 
    """
    zip = zipfile.ZipFile(outFullName, "a", zipfile.ZIP_DEFLATED)
    flag=False
    for path, dirnames, filenames in os.walk(dirpath):
        # å»æ‰ç›®æ ‡è·Ÿè·¯å¾„ï¼Œåªå¯¹ç›®æ ‡æ–‡ä»¶å¤¹ä¸‹è¾¹çš„æ–‡ä»¶åŠæ–‡ä»¶å¤¹è¿›è¡Œå‹ç¼©
        fpath = dirpath
        # print(dirpath.split("\\")[-1])
        # zip.write(path, os.path.relpath(path, os.path.dirname(dirpath)))
        for filename in filenames:
            flag=True
            print(str(path)+" "+str(dirnames)+" "+str(filenames))
            zip.write(os.path.join(path, filename), os.path.join(fpath, filename))
    if flag==False:
        print(dirpath)
        zip.write(dirpath, dirpath)
        # zip.writestr(os.path.basename(dirpath) + '/.keep', '')
    zip.close()

def search_content_to_file(content, db):
    """
    æ ¹æ®ç»™å®šçš„ content æŸ¥è¯¢ entity_to_file è¡¨ä¸­çš„ file_idï¼Œ
    å¹¶è¿”å› file è¡¨ä¸­æ‰€æœ‰åŒ¹é…çš„è¡Œçš„ä¿¡æ¯ã€‚
    """
    query = """
    SELECT ef.sim, f.*
    FROM file AS f
    JOIN entity_to_file AS ef ON f.id = ef.file_id
    WHERE ef.entity = %s;
    """
    try:
        with db.connection.cursor() as cursor:
            cursor.execute(query, (content,))
            result = cursor.fetchall()
            return result
    except pymysql.MySQLError as e:
        print(f"æŸ¥è¯¢å¤±è´¥ï¼š{e}")
        raise


def check_node_exists(name, node_id, userID, session):
    result = session.run(
        """
        MATCH (n)
        WHERE id(n) = $node_id and n.name=$name and n.user_id=$userID
        RETURN CASE WHEN COUNT(n) > 0 THEN 1 ELSE 0 END AS exists
        """,
        node_id=int(node_id),
        name=name,
        userID=int(userID)
    )
    return result.single()[0]


def check_node_strict(name, node_id, userID, session):
    result = session.run(
        """
        MATCH (n)
        WHERE id(n) = $node_id and n.name=$name and n.user_id=$userID
        RETURN CASE WHEN COUNT(n) > 0 THEN 1 ELSE 0 END AS exists
        """,
        node_id=int(node_id),
        name=name,
        userID=int(userID)
    )
    return result.single()[0]
    # return result.single()[0]  # è¿”å›æŸ¥è¯¢ç»“æœçš„ç¬¬ä¸€ä¸ªå€¼


def chuli_file_dire(under_dire, under_dire_to_file, session):
    for i in under_dire:
        query_word = '''
            MATCH (h:KOCategory)<-[r]-(t)
    WHERE id(h) = {arg_1} AND t:KOCategory
    RETURN h AS h, r AS r, t AS t, NULL AS fullPath, 0 AS pathLength
    LIMIT 100

    UNION ALL

    MATCH (h:KOCategory)
    WHERE id(h) = {arg_1}
    CALL apoc.path.expandConfig(h, {
        relationshipFilter: "<edge",
        labelFilter: ">KOCategory",
        maxLevel: 5,
        limit: 300,
        direction: "incoming"
    })
    YIELD path
    WITH h, path, relationships(path) AS rels  // ä¿ç•™ h å’Œ path
    UNWIND rels AS rel
    WITH h, rel, startNode(rel) AS t, endNode(rel) AS ko, path, length(path) AS pathLength  // è®¡ç®—è·¯å¾„é•¿åº¦
    WHERE ko:KOCategory AND t <> h  // ç¡®ä¿ t ä¸æ˜¯ h
    RETURN h AS h, rel AS r, ko AS t, path AS fullPath, pathLength
    ORDER BY pathLength DESC  // æŒ‰è·¯å¾„é•¿åº¦é™åºæ’åˆ—
    LIMIT 1  // ä»…è¿”å›æœ€é•¿çš„è·¯å¾„
        '''
        results_place = session.run(query_word, parameters={"arg_1": int(i)})
        dire_relation = []
        for res in results_place:
            if res["fullPath"] is not None:
                for edge in res["fullPath"]:
                    dire_relation.append((edge.start_node.id, edge.end_node.id))
                    # print(node)
        for pair in dire_relation:
            if int(pair[0]) not in under_dire_to_file:
                temp = []
            else:
                temp = under_dire_to_file[int(pair[0])]
            temp.append(int(pair[1]))
            under_dire_to_file[int(pair[0])] = temp
    return under_dire_to_file


def check_node_name(session, i):
    node1 = id_get_node(session, i)
    if "wikipage" in node1.labels:
        node_name = node1["name"] + "_ç»´åŸº.html"
    elif "baidupage" in node1.labels:
        node_name = node1["name"] + "_ç™¾åº¦.html"
    else:
        node_name = node1["name"]
    return node_name


def id_get_node(session, id):
    result = session.run(
        """
        MATCH (n)
        WHERE id(n) = $node_id
        RETURN n
        """,
        node_id=int(id)
    )
    # print(id)
    return result.single()["n"]

def compress_file(file_path, zip_path):
    # æ£€æŸ¥æŒ‡å®šçš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.isfile(file_path):
        print(f"æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ã€‚")
        return

    # åˆ›å»ºä¸€ä¸ªå‹ç¼©åŒ…å¹¶å°†æ–‡ä»¶æ·»åŠ åˆ°å…¶ä¸­
    with zipfile.ZipFile(zip_path, 'a') as zipf:
        zipf.write(file_path, os.path.basename(file_path))
        print(f"æ–‡ä»¶ {file_path} å·²æˆåŠŸå‹ç¼©åˆ° {zip_path}ã€‚")

def search_fileId_in_mysql(db,fileid,userID):
    query = """
SELECT
    de.dir_private,
    f.name
FROM
    dir_file AS df
JOIN
    dir_entity AS de ON df.dir_id = de.id and de.userID=%s
JOIN
    file AS f ON df.file_id = f.id
WHERE
    df.file_id = %s
        """
    try:
        with db.connection.cursor() as cursor:
            cursor.execute(query, (int(userID),int(fileid),))
            result = cursor.fetchall()
            return result
    except pymysql.MySQLError as e:
        print(f"æŸ¥è¯¢å¤±è´¥ï¼š{e}")
        raise
def search_dire_in_mysql(db,content,userid):
    query = """
    SELECT
    de.dir_private
FROM
    xiaoqi_new AS xn
JOIN
    dir_entity AS de ON xn.xiaoqi_id = de.entity_id and de.userid=%s
WHERE
    xn.xiaoqi_name=%s
            """
    try:
        with db.connection.cursor() as cursor:
            cursor.execute(query, (int(userid),str(content)))
            result = cursor.fetchall()
            return result
    except pymysql.MySQLError as e:
        print(f"æŸ¥è¯¢å¤±è´¥ï¼š{e}")
        raise
def search_nodeIds(userID,content):
    db = MySQLDatabase(
            host="114.213.234.179",
            user="koroot",  # æ›¿æ¢ä¸ºæ‚¨çš„ç”¨æˆ·å
            password="DMiC-4092",  # æ›¿æ¢ä¸ºæ‚¨çš„å¯†ç 
            database="db_hp"  # æ›¿æ¢ä¸ºæ‚¨çš„æ•°æ®åº“å
        )
    db.connect()
    query = """
    SELECT
            xn.xiaoqi_id,
        de.id AS directory_id,
        de.dir_private,
        x.file_id AS xiaoqi_file_id,
            f.name AS file_name,
            f.private As file_private
    FROM
        xiaoqi_new xn
    JOIN
        xiaoqi_to_file x ON xn.xiaoqi_id = x.xiaoqi_id
    JOIN
        dir_file df ON x.file_id = df.file_id
    JOIN
        dir_entity de ON df.dir_id = de.id and de.entity_id=xn.xiaoqi_id and de.userid=%s
    JOIN
        file f ON x.file_id = f.id and (f.private=0 or f.userid=%s)
    WHERE
        xn.xiaoqi_name = %s;
        """
    try:
        with db.connection.cursor() as cursor:
            cursor.execute(query, (int(userID), int(userID), str(content)))
            result = cursor.fetchall()
    except pymysql.MySQLError as e:
        print(f"æŸ¥è¯¢å¤±è´¥ï¼š{e}")
        raise
    nodes=[]
    for i in result:
        nodes.append(int(i[3]))
    return nodes

def test(content, userID):
    print(f"[DEBUG] å¼€å§‹å¤„ç†: {content}, ç”¨æˆ·ID: {userID}")

    base_dir = os.path.join('D:\\data\\Auto_recommendtion', content)
    print(f"[DEBUG] åŸºç¡€ç›®å½•: {base_dir}")

    if not os.path.exists(base_dir):
        os.makedirs(base_dir)
        print(f"[DEBUG] åˆ›å»ºç›®å½•: {base_dir}")
    else:
        print(f"[DEBUG] ç›®å½•å·²å­˜åœ¨: {base_dir}")

    # æ•°æ®åº“è¿æ¥
    print("[DEBUG] è¿æ¥æ•°æ®åº“...")
    db = MySQLDatabase(host="114.213.234.179", user="koroot", password="DMiC-4092", database="db_hp")
    db.connect()

    # è·å–èŠ‚ç‚¹ID
    print("[DEBUG] æŸ¥è¯¢èŠ‚ç‚¹ID...")
    node_id = search_nodeIds(userID, content)
    print(f"[DEBUG] æ‰¾åˆ°èŠ‚ç‚¹ID: {node_id}")

    # åˆå§‹åŒ– MinIO bucket - ç¡®ä¿è¿™ä¸ªåœ¨å‡½æ•°å†…éƒ¨å®šä¹‰
    print("[DEBUG] åˆå§‹åŒ–MinIOè¿æ¥...")
    bucket = Bucket(
        minio_address="114.213.232.140:19000",
        minio_admin="minioadmin",
        minio_password="minioadmin"
    )

    # æ£€æŸ¥ bucket è¿æ¥
    try:
        buckets = bucket.get_all_bucket()
        print(f"[DEBUG] å¯ç”¨çš„buckets: {buckets}")
        if 'kofiles' not in buckets:
            print("[ERROR] kofiles bucket ä¸å­˜åœ¨!")
            return []
    except Exception as e:
        print(f"[ERROR] MinIOè¿æ¥å¤±è´¥: {e}")
        return []

    # åˆ›å»ºç›®å½•
    result_dire = search_dire_in_mysql(db, content, userID)
    print(f"[DEBUG] æ•°æ®åº“ç›®å½•ç»“æœ: {result_dire}")

    for i in result_dire:
        dir_path = os.path.join(base_dir, i[0])
        print(f"[DEBUG] å¤„ç†ç›®å½•: {dir_path}")
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)
            print(f"[DEBUG] åˆ›å»ºå­ç›®å½•: {dir_path}")

    # ä¸‹è½½æ–‡ä»¶
    address = []
    for i in node_id:
        print(f"[DEBUG] å¤„ç†èŠ‚ç‚¹ {i}...")
        result = search_fileId_in_mysql(db, i, userID)
        print(f"[DEBUG] æ–‡ä»¶æŸ¥è¯¢ç»“æœ: {result}")

        for res in result:
            dire_name = res[0]
            node_path = "bb/" + res[1]
            target_dir = os.path.join(base_dir, dire_name)
            filename = node_path.split('/')[-1]
            full_path = os.path.join(target_dir, filename)

            print(f"[DEBUG] ä¸‹è½½: {node_path} -> {full_path}")

            # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
            if not os.path.exists(target_dir):
                os.makedirs(target_dir)
                print(f"[DEBUG] åˆ›å»ºä¸‹è½½ç›®å½•: {target_dir}")

            # ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„ bucket å˜é‡
            k = bucket.download_file_from_bucket('kofiles', node_path, full_path)
            if k == 1:
                print(f"[DEBUG] ä¸‹è½½æˆåŠŸ: {full_path}")
                address.append(full_path)
            else:
                print(f"[ERROR] ä¸‹è½½å¤±è´¥: {node_path}")

    print(f"[DEBUG] æœ€ç»ˆæ–‡ä»¶åˆ—è¡¨: {address}")
    return address

##########################################################################################################################################################################################
# print(test("å´ä¿¡ä¸œ1",6000622))#ç¬¬ä¸€ä¸ªå‚æ•°ä»£è¡¨æ¶ˆæ­§å®ä½“åç§°ï¼Œå¯¹åº”xiaoqi_newè¡¨ä¸­çš„nameï¼Œç¬¬äºŒä¸ªæ˜¯ç”¨æˆ·IDï¼ˆè¿™ä¸ªåˆ°æ—¶å€™å‰ç«¯ä¹Ÿä¼šæä¾›ï¼‰
#ç¬¬ä¸€ä¸ªæ¨¡å—ï¼šå‰ç«¯ä¼ å…¥ å®ä½“å å’Œ å¯¹åº”ID,è°ƒç”¨testè¿”å›ä¸‹è½½åœ¨æœ¬åœ°æ–‡ä»¶çš„åœ°å€ï¼ˆtest å‡½æ•°é€šè¿‡ æ•°æ®åº“æŸ¥è¯¢ â†’ æœ¬åœ°ç›®å½•åˆ›å»º â†’ MinIO æ–‡ä»¶ä¸‹è½½ çš„æµç¨‹ï¼‰
#######################################
#######################################

#######################################
import os
import re
import pdfplumber
from docx import Document
from bs4 import BeautifulSoup
import requests
from typing import List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import chardet


def read_file_content(filepath: str) -> str:
    try:
        if filepath.endswith(".pdf"):
            with pdfplumber.open(filepath) as pdf:
                return "\n".join(page.extract_text() or '' for page in pdf.pages)
        elif filepath.endswith(".docx"):
            return "\n".join([para.text for para in Document(filepath).paragraphs])
        elif filepath.endswith(".doc"):
            print(f"[è·³è¿‡] ä¸æ”¯æŒ .doc æ–‡ä»¶: {filepath}")
            return ''
        elif filepath.endswith((".html", ".htm", ".shtml")):
            with open(filepath, "rb") as f:
                raw_data = f.read()
                encoding = chardet.detect(raw_data)["encoding"] or "utf-8"
                text = raw_data.decode(encoding, errors="ignore")
                soup = BeautifulSoup(text, "html.parser")
                return soup.get_text(separator='\n', strip=True)
        else:
            print(f"[è·³è¿‡] ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {filepath}")
            return ''
    except Exception as e:
        print(f"[è¯»å–å¤±è´¥] {filepath} é”™è¯¯: {str(e)}")
        return ''


import random
import os
def select_files_by_category(folder_path, target_entity):
    """
    ä»æ¯ä¸ªå­æ–‡ä»¶å¤¹ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªæ–‡ä»¶ï¼Œå¹¶è¿”å›æ–‡ä»¶è·¯å¾„å’Œç±»åˆ«ä¿¡æ¯

    å‚æ•°:
    - folder_path: ä¸»æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¦‚ï¼šD:\data\Auto_recommendtion\å‘¨é¹1ï¼‰
    - target_entity: ç›®æ ‡å®ä½“åï¼ˆå¦‚ï¼šå‘¨é¹ï¼‰

    è¿”å›:
    - list: åŒ…å«(æ–‡ä»¶è·¯å¾„, ç±»åˆ«å)çš„å…ƒç»„åˆ—è¡¨
    """
    selected_files = []

    if not os.path.exists(folder_path):
        print(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
        return selected_files

    # è·å–æ‰€æœ‰å­æ–‡ä»¶å¤¹
    subdirs = [d for d in os.listdir(folder_path)
               if os.path.isdir(os.path.join(folder_path, d))]

    print(f"ğŸ“ æ‰¾åˆ° {len(subdirs)} ä¸ªå­æ–‡ä»¶å¤¹: {subdirs}")

    for subdir in subdirs:
        subdir_path = os.path.join(folder_path, subdir)

        # è·å–è¯¥æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        all_files = [f for f in os.listdir(subdir_path)
                     if os.path.isfile(os.path.join(subdir_path, f))]

        if not all_files:
            print(f"âš ï¸  æ–‡ä»¶å¤¹ '{subdir}' ä¸­æ²¡æœ‰æ–‡ä»¶ï¼Œè·³è¿‡")
            continue

        # éšæœºé€‰æ‹©ä¸€ä¸ªæ–‡ä»¶
        selected_file = random.choice(all_files)
        selected_file_path = os.path.join(subdir_path, selected_file)

        selected_files.append((selected_file_path, subdir))
        print(f"âœ… é€‰æ‹©æ–‡ä»¶: {subdir}/{selected_file}")

    return selected_files


def read_file_with_strategy(file_path, max_length=2500):
    """
    æ ¹æ®æ–‡ä»¶ç±»å‹å’Œé•¿åº¦æ™ºèƒ½è¯»å–æ–‡ä»¶å†…å®¹

    å‚æ•°:
    - file_path: æ–‡ä»¶è·¯å¾„
    - max_length: æœ€å¤§è¯»å–é•¿åº¦

    è¿”å›:
    - str: æ–‡ä»¶å†…å®¹
    """
    try:
        file_size = os.path.getsize(file_path)
        file_ext = os.path.splitext(file_path)[1].lower()

        print(f"ğŸ“„ è¯»å–æ–‡ä»¶: {os.path.basename(file_path)} (å¤§å°: {file_size} bytes, ç±»å‹: {file_ext})")

        # æ–‡æœ¬æ–‡ä»¶å¤„ç†
        if file_ext in ['.txt', '.html', '.htm', '.xml', '.json']:
            return read_text_file(file_path, max_length)

        # PDFæ–‡ä»¶å¤„ç†
        elif file_ext == '.pdf':
            return read_pdf_file(file_path, max_length)

        # Wordæ–‡æ¡£å¤„ç†
        elif file_ext in ['.docx', '.doc']:
            return read_word_file(file_path, max_length)

        # å…¶ä»–æ–‡ä»¶ç±»å‹
        else:
            print(f"âš ï¸  ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}ï¼Œå°è¯•ä½œä¸ºæ–‡æœ¬æ–‡ä»¶è¯»å–")
            return read_text_file(file_path, max_length)

    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶ {os.path.basename(file_path)} å¤±è´¥: {e}")
        return ""


def read_text_file(file_path, max_length):
    """è¯»å–æ–‡æœ¬æ–‡ä»¶"""
    try:
        # å°è¯•å¤šç§ç¼–ç 
        encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    content = f.read()

                    # å¦‚æœæ–‡ä»¶å¤ªå¤§ï¼Œæˆªå–å‰ä¸€åŠæˆ–æœ€å¤§é•¿åº¦
                    if len(content) > max_length:
                        # å–å‰ä¸€åŠæˆ–æœ€å¤§é•¿åº¦ï¼Œå–è¾ƒå°å€¼
                        half_length = min(len(content) // 2, max_length)
                        content = content[:half_length]
                        print(f"ğŸ“ æ–‡æœ¬è¿‡é•¿ï¼Œæˆªå–å‰ {half_length} å­—ç¬¦")

                    return content
            except UnicodeDecodeError:
                continue
        return ""
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {e}")
        return ""


def read_pdf_file(file_path, max_length):
    """è¯»å–PDFæ–‡ä»¶"""
    try:
        with pdfplumber.open(file_path) as pdf:
            total_text = ""
            page_count = min(len(pdf.pages), 5)  # æœ€å¤šè¯»å–5é¡µ

            for i in range(page_count):
                page_text = pdf.pages[i].extract_text() or ''
                total_text += page_text + "\n"

                # å¦‚æœå·²ç»è¾¾åˆ°æœ€å¤§é•¿åº¦ï¼Œåœæ­¢è¯»å–
                if len(total_text) >= max_length:
                    total_text = total_text[:max_length]
                    print(f"ğŸ“ PDFæ–‡ä»¶è¿‡å¤§ï¼Œæˆªå–å‰ {max_length} å­—ç¬¦")
                    break

            return total_text
    except Exception as e:
        print(f"âŒ è¯»å–PDFæ–‡ä»¶å¤±è´¥: {e}")
        return ""


def read_word_file(file_path, max_length):
    """è¯»å–Wordæ–‡æ¡£"""
    try:
        doc = Document(file_path)
        total_text = ""

        for para in doc.paragraphs:
            if para.text.strip():
                total_text += para.text + "\n"

                # å¦‚æœå·²ç»è¾¾åˆ°æœ€å¤§é•¿åº¦ï¼Œåœæ­¢è¯»å–
                if len(total_text) >= max_length:
                    total_text = total_text[:max_length]
                    print(f"ğŸ“ Wordæ–‡æ¡£è¿‡å¤§ï¼Œæˆªå–å‰ {max_length} å­—ç¬¦")
                    break

        return total_text
    except Exception as e:
        print(f"âŒ è¯»å–Wordæ–‡æ¡£å¤±è´¥: {e}")
        return read_text_file(file_path, max_length)  # å¤±è´¥æ—¶å°è¯•ä½œä¸ºæ–‡æœ¬æ–‡ä»¶è¯»å–


def generate_prompt(person_name: str, raw_text: str, max_keywords: int = 15) -> str:
    """
    åŸæœ‰çš„ä¸“ä¸šæç¤ºè¯ç”Ÿæˆå‡½æ•°ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
    """
    return f"""ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æ£€ç´¢ä¸“å®¶ï¼Œè¯·ä»ä»¥ä¸‹åŸå§‹æ–‡æœ¬ä¸­ï¼Œ**é€å­—æå–å‡º**æœ€å¤š {max_keywords} ä¸ªä¸äººç‰© [{person_name}] é«˜åº¦ç›¸å…³ã€ä¾¿äºç½‘ç»œæœç´¢çš„å…³é”®è¯çŸ­è¯­ã€‚

# è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹çº¦æŸï¼š

1. æ‰€æœ‰å…³é”®è¯å¿…é¡»**é€å­—å‡ºç°åœ¨åŸæ–‡ä¸­**ï¼Œä¸èƒ½ç¼–é€ ã€æ¦‚æ‹¬æˆ–æ‰©å±•ï¼›
2. ä¼˜å…ˆæå–ä»¥ä¸‹ä¸¤ç±»å†…å®¹ï¼š
   - **æœºæ„/å•ä½**ï¼šå¦‚"æ¸…åå¤§å­¦""è‡ªåŠ¨åŒ–ç³»"
   - **è¾ƒå¤§çš„ç ”ç©¶æ–¹å‘**ï¼šå¦‚"äººå·¥æ™ºèƒ½""ç”Ÿç‰©åŒ»å­¦""è®¡ç®—æœºç§‘å­¦"
3. å¦‚æœåŸæ–‡ä¸­æŸäº›æœºæ„ä¸ºé•¿ç»“æ„ï¼ˆå¦‚"å®‰å¾½å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢"ï¼‰ï¼Œè¯·**åˆç†æ‹†åˆ†ä¸ºå¤šä¸ªå…³é”®è¯**ï¼Œä¾‹å¦‚ï¼š
   - "å®‰å¾½å¤§å­¦"
   - "è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢"

# è¾“å‡ºæ ¼å¼ï¼š
- **åªè¾“å‡ºå…³é”®è¯ï¼Œä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šã€è¯´æ˜æˆ–å…¶ä»–æ–‡å­—**
- ä¸­æ–‡åˆ†å·åˆ†éš”ï¼ˆä½¿ç”¨ä¸­æ–‡åˆ†å·"ï¼›"ï¼‰
- æ¯æ¡å…³é”®è¯åº”å…·æœ‰ç‹¬ç«‹çš„æœç´¢ä»·å€¼
- æ‰€æœ‰å…³é”®è¯å¿…é¡»æ¥è‡ªåŸæ–‡ï¼Œä¸”ä¸è¶…è¿‡ {max_keywords} ä¸ª
- å¦‚æœåŸæ–‡ä¸­ç¡®å®æ²¡æœ‰ç›¸å…³å…³é”®è¯ï¼Œè¯·è¾“å‡ºï¼š**NO_KEYWORDS_FOUND**

åŸå§‹æ–‡æœ¬å¦‚ä¸‹ï¼š
{raw_text}
"""


def validate_and_extract_keywords(model_output, person_name):
    """
    éªŒè¯å¹¶æå–æ¨¡å‹è¾“å‡ºçš„å…³é”®è¯

    è¿”å›:
    - valid_keywords: æœ‰æ•ˆçš„å…³é”®è¯åˆ—è¡¨
    - is_valid: æ˜¯å¦æˆåŠŸæå–åˆ°æœ‰æ•ˆå…³é”®è¯
    """
    if not model_output or not model_output.strip():
        return [], False

    # æ£€æŸ¥æ˜¯å¦æ²¡æœ‰æ‰¾åˆ°å…³é”®è¯çš„ç‰¹æ®Šæ ‡è®°
    if "NO_KEYWORDS_FOUND" in model_output:
        return [], True  # æ˜ç¡®è¡¨ç¤ºæ²¡æœ‰å…³é”®è¯

    # æ£€æŸ¥æ˜¯å¦åŒ…å«è§£é‡Šæ€§æ–‡å­—ï¼ˆæ¨¡å‹æ²¡æœ‰éµå®ˆæŒ‡ä»¤ï¼‰
    explanation_indicators = [
        "æ ¹æ®æä¾›çš„åŸå§‹æ–‡æœ¬", "æœªæ‰¾åˆ°", "æœªæåŠ", "å»ºè®®æ£€æŸ¥", "æ–‡æœ¬å†…å®¹ä¸»è¦",
        "è¾“å‡ºä¸ºç©º", "æŠ±æ­‰", "æ— æ³•æå–", "å»ºè®®æä¾›", "ç¡®è®¤"
    ]

    for indicator in explanation_indicators:
        if indicator in model_output:
            print(f"âš ï¸  æ£€æµ‹åˆ°æ¨¡å‹è¾“å‡ºè§£é‡Šæ€§æ–‡å­—: {indicator}")
            return [], False

    # å°è¯•æŒ‰åˆ†å·åˆ†å‰²å…³é”®è¯
    keywords = [kw.strip() for kw in re.split(r"[ï¼›;]", model_output) if kw.strip()]

    # éªŒè¯å…³é”®è¯è´¨é‡
    valid_keywords = []
    for keyword in keywords:
        # è¿‡æ»¤æ‰è¿‡çŸ­æˆ–æ— æ„ä¹‰çš„å…³é”®è¯
        if (len(keyword) >= 2 and  # è‡³å°‘2ä¸ªå­—ç¬¦
                not keyword.isdigit() and  # ä¸æ˜¯çº¯æ•°å­—
                not keyword.startswith(('http://', 'https://')) and  # ä¸æ˜¯URL
                keyword != person_name):  # ä¸æ˜¯å®ä½“åæœ¬èº«
            valid_keywords.append(keyword)

    # æ£€æŸ¥æ˜¯å¦æå–åˆ°æœ‰æ•ˆå…³é”®è¯
    if not valid_keywords:
        print("âš ï¸  æœªæå–åˆ°æœ‰æ•ˆå…³é”®è¯")
        return [], False

    print(f"âœ… éªŒè¯é€šè¿‡çš„å…³é”®è¯: {valid_keywords}")
    return valid_keywords, True


def call_big_model(text: str, person_name: str, max_keywords: int, api_key: str) -> Optional[str]:
    """
    è°ƒç”¨å¤§æ¨¡å‹ APIï¼Œæ”¯æŒæ–°çš„æ™ºèƒ½æç¤ºæ ¼å¼
    """
    API_URL = "https://api.siliconflow.cn/v1/chat/completions"
    MODEL_NAME = "deepseek-ai/DeepSeek-V3"
    HEADERS = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    # æ£€æŸ¥æ˜¯å¦æ˜¯æ™ºèƒ½æç¤ºæ ¼å¼ï¼ˆåŒ…å«ç±»åˆ«ä¿¡æ¯ï¼‰
    if "=== " in text and "ç±»åˆ«æ–‡æ¡£å†…å®¹ ===" in text:
        # å·²ç»æ˜¯æ™ºèƒ½æç¤ºæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
        prompt = text
    else:
        # ä½¿ç”¨åŸæœ‰çš„æç¤ºè¯æ ¼å¼
        prompt = generate_prompt(person_name, text, max_keywords)

    payload = {
        "model": MODEL_NAME,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.1,
        "max_tokens": 200,
        "stop": ["\n\n"]
    }

    max_retries = 2
    for attempt in range(max_retries):
        try:
            print(f"ğŸ”„ APIè°ƒç”¨å°è¯• {attempt + 1}/{max_retries}")
            response = requests.post(API_URL, headers=HEADERS, json=payload, timeout=30)
            response.raise_for_status()

            result = response.json()['choices'][0]['message']['content'].strip()
            print(f"âœ… APIè°ƒç”¨æˆåŠŸï¼Œè¿”å›é•¿åº¦: {len(result)}")

            return result

        except Exception as e:
            print(f"âŒ APIè¯·æ±‚å¤±è´¥ ({attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(1)

    return None


def merge_all_texts(file_paths: list, max_workers: int = 10) -> str:
    """
    ç›´æ¥åˆå¹¶æ–‡ä»¶åˆ—è¡¨å†…å®¹ï¼ˆè·³è¿‡æ–‡ä»¶å¤¹æ‰«æï¼‰
    """

    def read_file(path):
        try:
            # æ£€æµ‹æ–‡ä»¶ç¼–ç 
            with open(path, 'rb') as f:
                raw_data = f.read()
                encoding = chardet.detect(raw_data)['encoding'] or 'utf-8'

            # ç”¨æ£€æµ‹åˆ°çš„ç¼–ç è¯»å–æ–‡ä»¶
            with open(path, 'r', encoding=encoding, errors='ignore') as f:
                content = f.read().strip()
                if content:
                    return path, content
                else:
                    print(f"âš ï¸ æ–‡ä»¶å†…å®¹ä¸ºç©º: {os.path.basename(path)}")
                    return path, None

        except UnicodeDecodeError:
            print(f"âš ï¸ ç¼–ç é—®é¢˜: {os.path.basename(path)}ï¼Œå°è¯•å…¶ä»–ç¼–ç ")
            # å°è¯•å¸¸è§ç¼–ç 
            for enc in ['gbk', 'gb2312', 'latin-1', 'iso-8859-1']:
                try:
                    with open(path, 'r', encoding=enc, errors='ignore') as f:
                        content = f.read().strip()
                        if content:
                            print(f"âœ… ä½¿ç”¨ {enc} ç¼–ç æˆåŠŸè¯»å–: {os.path.basename(path)}")
                            return path, content
                except:
                    continue
            print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶: {os.path.basename(path)}")
            return path, None
        except Exception as e:
            print(f"âŒ è¯»å–å¤±è´¥ {os.path.basename(path)}: {str(e)}")
            return path, None

    valid_contents = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(read_file, path): path for path in file_paths}
        for future in as_completed(futures):
            path, content = future.result()
            if content:
                valid_contents.append(content)

    return "\n\n".join(valid_contents).strip()


def clean_entity_name(entity: str) -> str:
    """
    è‡ªåŠ¨å»é™¤å®ä½“åæœ«å°¾çš„æ•°å­—
    ç¤ºä¾‹:
        "å´ä¿¡ä¸œ1" -> "å´ä¿¡ä¸œ"
        "å®ä½“123" -> "å®ä½“"
        "æ— æ•°å­—" -> "æ— æ•°å­—"
    """
    return re.sub(r'\d+$', '', entity)


def build_intelligent_prompt(selected_files, target_entity, max_keywords=15):
    """
    æ„å»ºæ™ºèƒ½æç¤ºæ–‡æœ¬ï¼Œæ•´åˆåŸæœ‰çš„ä¸“ä¸šæç¤ºè¯æ ¼å¼

    å‚æ•°:
    - selected_files: é€‰æ‹©çš„æ–‡ä»¶åˆ—è¡¨ï¼ŒåŒ…å«(æ–‡ä»¶è·¯å¾„, ç±»åˆ«å)
    - target_entity: ç›®æ ‡å®ä½“å
    - max_keywords: æœ€å¤§å…³é”®è¯æ•°é‡

    è¿”å›:
    - str: æ„å»ºå¥½çš„æç¤ºæ–‡æœ¬
    """
    # æ„å»ºå†…å®¹éƒ¨åˆ†
    content_parts = []
    total_length = 0
    max_total_length = 20000  # æ€»é•¿åº¦é™åˆ¶

    for file_path, category in selected_files:
        if total_length >= max_total_length:
            print("âš ï¸  å†…å®¹éƒ¨åˆ†è¿‡é•¿ï¼Œåœæ­¢æ·»åŠ æ›´å¤šæ–‡ä»¶")
            break

        # è¯»å–æ–‡ä»¶å†…å®¹
        content = read_file_with_strategy(file_path, max_length=2500)  # æ¯ä¸ªæ–‡ä»¶æœ€å¤š2500å­—ç¬¦

        if not content.strip():
            continue

        # æ·»åŠ ç±»åˆ«ä¿¡æ¯
        category_header = f"=== {category}ç±»åˆ«æ–‡æ¡£å†…å®¹ ===\n"
        file_content = category_header + content + "\n\n"

        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æ€»é•¿åº¦é™åˆ¶
        if total_length + len(file_content) > max_total_length:
            remaining = max_total_length - total_length
            if remaining > 100:  # è‡³å°‘ä¿ç•™100å­—ç¬¦
                file_content = file_content[:remaining]
                print(f"ğŸ“ æˆªæ–­å†…å®¹ï¼Œä¿ç•™å‰ {remaining} å­—ç¬¦")
            else:
                break

        content_parts.append(file_content)
        total_length += len(file_content)

    # å¦‚æœæ²¡æœ‰æå–åˆ°ä»»ä½•å†…å®¹
    if not content_parts:
        return generate_prompt(target_entity, "æ— ç›¸å…³æ–‡æ¡£å†…å®¹", max_keywords)

    # åˆå¹¶æ‰€æœ‰å†…å®¹
    all_content = "".join(content_parts)

    # ä½¿ç”¨åŸæœ‰çš„ä¸“ä¸šæç¤ºè¯æ ¼å¼
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æ£€ç´¢ä¸“å®¶ï¼Œè¯·ä»ä»¥ä¸‹ä¸åŒç±»åˆ«çš„æ–‡æ¡£å†…å®¹ä¸­ï¼Œ**é€å­—æå–å‡º**æœ€å¤š {max_keywords} ä¸ªä¸äººç‰© [{target_entity}] é«˜åº¦ç›¸å…³ã€ä¾¿äºç½‘ç»œæœç´¢çš„å…³é”®è¯çŸ­è¯­ã€‚

# è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹çº¦æŸï¼š

1. æ‰€æœ‰å…³é”®è¯å¿…é¡»**é€å­—å‡ºç°åœ¨åŸæ–‡ä¸­**ï¼Œä¸èƒ½ç¼–é€ ã€æ¦‚æ‹¬æˆ–æ‰©å±•ï¼›
2. ä¼˜å…ˆæå–ä»¥ä¸‹ä¸¤ç±»å†…å®¹ï¼š
   - **æœºæ„/å•ä½**ï¼šå¦‚"æ¸…åå¤§å­¦""è‡ªåŠ¨åŒ–ç³»"
   - **è¾ƒå¤§çš„ç ”ç©¶æ–¹å‘**ï¼šå¦‚"äººå·¥æ™ºèƒ½""ç”Ÿç‰©åŒ»å­¦""è®¡ç®—æœºç§‘å­¦"
3. å¦‚æœåŸæ–‡ä¸­æŸäº›æœºæ„ä¸ºé•¿ç»“æ„ï¼ˆå¦‚"å®‰å¾½å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢"ï¼‰ï¼Œè¯·**åˆç†æ‹†åˆ†ä¸ºå¤šä¸ªå…³é”®è¯**ï¼Œä¾‹å¦‚ï¼š
   - "å®‰å¾½å¤§å­¦"
   - "è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢"
4. **ä»”ç»†åˆ†ææ¯ä¸ªç±»åˆ«çš„æ–‡æ¡£**ï¼Œç¡®ä¿ä¸é—æ¼ä»»ä½•é‡è¦ä¿¡æ¯

# è¾“å‡ºæ ¼å¼ï¼š
- **åªè¾“å‡ºå…³é”®è¯ï¼Œä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šã€è¯´æ˜æˆ–å…¶ä»–æ–‡å­—**
- ä¸­æ–‡åˆ†å·åˆ†éš”ï¼ˆä½¿ç”¨ä¸­æ–‡åˆ†å·"ï¼›"ï¼‰
- æ¯æ¡å…³é”®è¯åº”å…·æœ‰ç‹¬ç«‹çš„æœç´¢ä»·å€¼
- æ‰€æœ‰å…³é”®è¯å¿…é¡»æ¥è‡ªåŸæ–‡ï¼Œä¸”ä¸è¶…è¿‡ {max_keywords} ä¸ª
- å¦‚æœæ‰€æœ‰æ–‡æ¡£ä¸­ç¡®å®æ²¡æœ‰ç›¸å…³å…³é”®è¯ï¼Œè¯·è¾“å‡ºï¼š**NO_KEYWORDS_FOUND**

# æ–‡æ¡£æ¥æºè¯´æ˜ï¼š
ä»¥ä¸‹å†…å®¹æ¥è‡ªè¯¥äººç‰©çš„å¤šä¸ªç›¸å…³æ–‡æ¡£ï¼ŒæŒ‰ç±»åˆ«åˆ†ç»„æ˜¾ç¤ºï¼š
{all_content}

"""

    print(f"ğŸ“‹ æ„å»ºçš„æç¤ºæ–‡æœ¬æ€»é•¿åº¦: {len(prompt)} å­—ç¬¦")
    return prompt


def filter_search_results_by_keywords(search_result, keywords):
    """
    æ ¹æ®å…³é”®è¯è¿‡æ»¤æœç´¢ç»“æœ
    :param search_result: æœç´¢ç»“æœå­—å…¸ï¼ŒåŒ…å«dataå­—æ®µ
    :param keywords: å…³é”®è¯åˆ—è¡¨ï¼Œè‡³å°‘éœ€è¦2ä¸ªå…³é”®è¯
    :return: è¿‡æ»¤åçš„æœç´¢ç»“æœ
    """
    if not keywords or len(keywords) < 2:
        print("âš ï¸ å…³é”®è¯ä¸è¶³ï¼Œè·³è¿‡è¿‡æ»¤")
        return search_result

    if not isinstance(search_result, dict) or 'data' not in search_result:
        print("âš ï¸ æœç´¢ç»“æœæ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡è¿‡æ»¤")
        return search_result

    filtered_data = []
    first_keyword = keywords[0]
    second_keyword = keywords[1]

    print(f"ğŸ” å¼€å§‹å…³é”®è¯è¿‡æ»¤: ç¬¬ä¸€å…³é”®è¯='{first_keyword}', ç¬¬äºŒå…³é”®è¯='{second_keyword}'")

    for item in search_result['data']:
        # æ£€æŸ¥titleæˆ–contentæ˜¯å¦åŒ…å«ç¬¬ä¸€ä¸ªå…³é”®è¯
        title_contains_first = first_keyword in item.get('title', '')
        content_contains_first = first_keyword in item.get('content', '')

        # å¦‚æœtitleæˆ–contentåŒ…å«ç¬¬ä¸€ä¸ªå…³é”®è¯ï¼Œå†æ£€æŸ¥æ˜¯å¦åŒ…å«ç¬¬äºŒä¸ªå…³é”®è¯
        if title_contains_first or content_contains_first:
            title_contains_second = second_keyword in item.get('title', '')
            content_contains_second = second_keyword in item.get('content', '')

            if title_contains_second or content_contains_second:
                filtered_data.append(item)

    print(f"âœ… å…³é”®è¯è¿‡æ»¤å®Œæˆ: ä» {len(search_result['data'])} ä¸ªç»“æœä¸­ç­›é€‰å‡º {len(filtered_data)} ä¸ªç›¸å…³ç»“æœ")

    # æ›´æ–°æœç´¢ç»“æœ
    search_result['data'] = filtered_data
    search_result['filtered_count'] = len(filtered_data)
    search_result['filter_keywords'] = [first_keyword, second_keyword]

    return search_result



# def auto_recommendtion(request: HttpRequest) -> dict:
#     """
#     ä¸»æ¨èå‡½æ•° - ä½¿ç”¨æ™ºèƒ½æ–‡ä»¶é€‰æ‹©ç­–ç•¥
#     """
#     name = request.GET.get("name", "").strip()
#     user_id = request.GET.get("user_id", "").strip()
#     api_key = request.GET.get("api_key", "sk-uwokmhxknecolbmvcrnfstfrcqzjeuekvnxfoghzrakeqybw")
#     max_keywords = int(request.GET.get("max_keywords", 5))
#     num_pages_to_crawl = int(request.GET.get("num_pages_to_crawl", 120))
#
#     print(f"ğŸ” å¼€å§‹å¤„ç†: name={name}, user_id={user_id}")
#
#     if not name or not user_id:
#         return {"status": "error", "message": "Missing required parameters: name or user_id"}
#
#     try:
#         # 1. ä¸‹è½½æ–‡ä»¶
#         print("ğŸ“¥ æ­¥éª¤1: ä¸‹è½½ç›¸å…³æ–‡ä»¶...")
#         folder = test(name, int(user_id))
#         base_dir = os.path.join('D:\\data\\Auto_recommendtion', name)
#         print(f"âœ… ä¸‹è½½å®Œæˆï¼Œæ–‡ä»¶ä¿å­˜åœ¨: {base_dir}")
#
#         # 2. æ™ºèƒ½é€‰æ‹©æ–‡ä»¶
#         print("ğŸ“ æ­¥éª¤2: ä»æ¯ä¸ªç±»åˆ«ä¸­éšæœºé€‰æ‹©æ–‡ä»¶...")
#         selected_files = select_files_by_category(base_dir, clean_entity_name(name))
#
#         if not selected_files:
#             print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ–‡ä»¶ï¼Œä½¿ç”¨å®ä½“åä½œä¸ºé»˜è®¤æŸ¥è¯¢")
#             combined_query = clean_entity_name(name)
#             keywords = []
#             keywords_status = "no_files"
#         else:
#             print(f"âœ… ä» {len(selected_files)} ä¸ªç±»åˆ«ä¸­é€‰æ‹©çš„æ–‡ä»¶")
#
#             # 3. æ„å»ºæ™ºèƒ½æç¤ºæ–‡æœ¬
#             print("ğŸ“ æ­¥éª¤3: æ„å»ºæ™ºèƒ½æç¤ºæ–‡æœ¬...")
#             intelligent_text = build_intelligent_prompt(selected_files, clean_entity_name(name), max_keywords)
#
#             if not intelligent_text.strip():
#                 print("âš ï¸ æœªèƒ½æå–åˆ°æœ‰æ•ˆæ–‡æœ¬å†…å®¹")
#                 combined_query = clean_entity_name(name)
#                 keywords = []
#                 keywords_status = "no_content"
#             else:
#                 # 4. æå–å…³é”®è¯
#                 print("ğŸ¤– æ­¥éª¤4: è°ƒç”¨å¤§æ¨¡å‹æå–å…³é”®è¯...")
#                 print(f"ğŸ“‹ å‘é€ç»™æ¨¡å‹çš„æ–‡æœ¬é•¿åº¦: {len(intelligent_text)} å­—ç¬¦")
#
#                 start_time = time.time()
#                 result = call_big_model(
#                     text=intelligent_text,
#                     person_name=clean_entity_name(name),
#                     max_keywords=max_keywords,
#                     api_key=api_key
#                 )
#                 end_time = time.time()
#                 print(f"â±ï¸ å…³é”®è¯æå–è€—æ—¶: {end_time - start_time:.2f}ç§’")
#
#                 if result:
#                     print(f"ğŸ”‘ æ¨¡å‹è¿”å›ç»“æœ: {result}")
#
#                     # éªŒè¯å’Œæå–å…³é”®è¯
#                     keywords, is_valid = validate_and_extract_keywords(result, clean_entity_name(name))
#
#                     if is_valid and keywords:
#                         print(f"âœ… æå–åˆ°çš„æœ‰æ•ˆå…³é”®è¯: {keywords}")
#                         if len(keywords) >= 2:
#                             combined_query = f"{clean_entity_name(name)} {keywords[0]}"#{keywords[1]}
#                             keywords_status = "valid_multiple"
#                         elif len(keywords) == 1:
#                             combined_query = f"{clean_entity_name(name)} {keywords[0]}"
#                             keywords_status = "valid_single"
#                         else:
#                             combined_query = clean_entity_name(name)
#                             keywords_status = "no_keywords_found"
#                     else:
#                         print("âŒ æ¨¡å‹è¿”å›æ— æ•ˆç»“æœï¼Œä½¿ç”¨å®ä½“å")
#                         combined_query = clean_entity_name(name)
#                         keywords = []
#                         keywords_status = "invalid_output"
#                 else:
#                     print("âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å®ä½“å")
#                     combined_query = clean_entity_name(name)
#                     keywords = []
#                     keywords_status = "api_failure"
#
#         print(f"ğŸ” æœ€ç»ˆæœç´¢æŸ¥è¯¢: {combined_query} (çŠ¶æ€: {keywords_status})")
#
#         # 5. è¿›è¡Œç½‘ç»œæœç´¢
#         print("ğŸŒ æ­¥éª¤5: è¿›è¡Œç½‘ç»œæœç´¢...")
#         mock_request = type('MockRequest', (), {'GET': {
#             "name": combined_query,
#             "num_pages_to_crawl": num_pages_to_crawl,
#             "userID": user_id,
#             "xiaoqi_name": clean_entity_name(name),
#             "enable_deduplication": "true"
#         }})
#
#         search_result = search_urls(mock_request)
#         print(
#             f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_result.get('data', [])) if isinstance(search_result, dict) else len(search_result)} ä¸ªç»“æœ")
#
#         print("ğŸ” æ­¥éª¤6: è¿›è¡Œå…³é”®è¯è¿‡æ»¤...")
#         combined_query_list = [clean_entity_name(name)] + keywords
#         search_result = filter_search_results_by_keywords(search_result, combined_query_list)
#
#
#
#         return {
#             "status": "success",
#             "data": search_result,
#             "keywords": keywords,
#             "combined_query": combined_query,
#             "file_count": len(selected_files),
#             "keywords_status": keywords_status,
#             "selected_categories": [category for _, category in selected_files] if selected_files else []
#         }
#
#     except Exception as e:
#         print(f"âŒ å‘ç”Ÿå¼‚å¸¸: {str(e)}")
#         import traceback
#         traceback.print_exc()
#         return {"status": "error", "message": str(e)}
#

def get_keywords_from_db(db, entity_name, user_id):
    """
    ä»æ•°æ®åº“è·å–å®ä½“å…³é”®è¯
    """
    # ç›´æ¥æŸ¥è¯¢ key_words å­—æ®µ
    query = """
    SELECT key_words 
    FROM xiaoqi_new 
    WHERE xiaoqi_name = %s
    LIMIT 1
    """

    try:
        with db.connection.cursor() as cursor:
            cursor.execute(query, (entity_name,))
            result = cursor.fetchone()

            if not result:
                print("âŒ æ•°æ®åº“ä¸­æœªæ‰¾åˆ°å¯¹åº”å®ä½“è®°å½•")
                return [], False

            key_words_value = result[0] if result else None

            if not key_words_value:
                print("âš ï¸ å…³é”®è¯å­—æ®µä¸ºç©º")
                return [], False

            print(f"ğŸ” åŸå§‹å…³é”®è¯æ•°æ®: {key_words_value} (ç±»å‹: {type(key_words_value)})")

            # è§£æå…³é”®è¯
            keywords = []

            if isinstance(key_words_value, str):
                if key_words_value.startswith('[') and key_words_value.endswith(']'):
                    # JSON æ•°ç»„æ ¼å¼
                    try:
                        keywords = json.loads(key_words_value)
                        print(f"âœ… è§£æä¸ºJSONæ•°ç»„: {keywords}")
                    except json.JSONDecodeError as e:
                        print(f"âŒ JSONè§£æå¤±è´¥: {e}")
                        # å°è¯•åˆ†å·åˆ†éš”
                        keywords = [kw.strip() for kw in key_words_value.split(';') if kw.strip()]
                else:
                    # åˆ†å·åˆ†éš”æˆ–æ™®é€šå­—ç¬¦ä¸²
                    keywords = [kw.strip() for kw in key_words_value.split(';') if kw.strip()]
                    print(f"âœ… è§£æä¸ºåˆ†å·åˆ†éš”: {keywords}")

            elif isinstance(key_words_value, (list, tuple)):
                # å·²ç»æ˜¯åˆ—è¡¨æ ¼å¼
                keywords = list(key_words_value)
                print(f"âœ… ç›´æ¥ä½¿ç”¨åˆ—è¡¨: {keywords}")

            if keywords:
                print(f"âœ… ä»æ•°æ®åº“è§£æåˆ°å…³é”®è¯: {keywords}")
                return keywords, True
            else:
                print("âš ï¸ å…³é”®è¯å­—æ®µæœ‰å€¼ä½†è§£æåä¸ºç©º")
                return [], False

    except pymysql.MySQLError as e:
        print(f"æŸ¥è¯¢å…³é”®è¯å¤±è´¥ï¼š{e}")
        return [], False
    except Exception as e:
        print(f"è§£æå…³é”®è¯å¤±è´¥ï¼š{e}")
        return [], False

def check_table_structure(db, table_name="xiaoqi_new"):
    """
    æ£€æŸ¥è¡¨ç»“æ„
    """
    try:
        with db.connection.cursor() as cursor:
            cursor.execute(f"DESCRIBE {table_name}")
            result = cursor.fetchall()
            print(f"ğŸ” [DEBUG] è¡¨ç»“æ„ {table_name}:")
            for row in result:
                print(f"  {row}")
    except Exception as e:
        print(f"âŒ æ£€æŸ¥è¡¨ç»“æ„å¤±è´¥: {e}")



def update_keywords_to_db(db, entity_name, user_id, keywords):
    """
    æ›´æ–°å…³é”®è¯åˆ°æ•°æ®åº“ - æ·»åŠ è¯¦ç»†è°ƒè¯•
    """
    print(f"ğŸ” [DEBUG] update_keywords_to_db å¼€å§‹æ‰§è¡Œ")
    print(f"ğŸ” [DEBUG] å‚æ•°: entity_name={entity_name}, user_id={user_id}")
    print(f"ğŸ” [DEBUG] keywords={keywords}")
    print(f"ğŸ” [DEBUG] keywordsç±»å‹: {type(keywords)}")

    check_query = """
    SELECT COUNT(*) 
    FROM xiaoqi_new 
    WHERE xiaoqi_name = %s
    """

    update_query = """
    UPDATE xiaoqi_new 
    SET key_words = %s 
    WHERE xiaoqi_name = %s
    """

    insert_query = """
    INSERT INTO xiaoqi_new (xiaoqi_name, key_words) 
    VALUES (%s, %s)
    """

    try:
        with db.connection.cursor() as cursor:
            # æ£€æŸ¥è®°å½•æ˜¯å¦å­˜åœ¨
            print(f"ğŸ” [DEBUG] æ‰§è¡Œæ£€æŸ¥æŸ¥è¯¢: {check_query} with {entity_name}")
            cursor.execute(check_query, (entity_name,))
            exists = cursor.fetchone()[0] > 0
            print(f"ğŸ” [DEBUG] è®°å½•æ˜¯å¦å­˜åœ¨: {exists}")

            # å‡†å¤‡å…³é”®è¯æ•°æ®
            if isinstance(keywords, list):
                keywords_json = json.dumps(keywords, ensure_ascii=False)
                print(f"ğŸ” [DEBUG] åˆ—è¡¨è½¬æ¢ä¸ºJSON: {keywords_json}")
            else:
                keywords_json = str(keywords)
                print(f"ğŸ” [DEBUG] éåˆ—è¡¨ç›´æ¥è½¬ä¸ºå­—ç¬¦ä¸²: {keywords_json}")

            print(f"ğŸ” [DEBUG] æœ€ç»ˆè¦ä¿å­˜çš„æ•°æ®: {keywords_json}")
            print(f"ğŸ” [DEBUG] æ•°æ®ç±»å‹: {type(keywords_json)}")

            if exists:
                print(f"ğŸ” [DEBUG] æ‰§è¡Œæ›´æ–°: {update_query} with ({keywords_json}, {entity_name})")
                cursor.execute(update_query, (keywords_json, entity_name))
                print(f"âœ… æ›´æ–°ç°æœ‰è®°å½•: {entity_name}")
            else:
                print(f"ğŸ” [DEBUG] æ‰§è¡Œæ’å…¥: {insert_query} with ({entity_name}, {keywords_json})")
                cursor.execute(insert_query, (entity_name, keywords_json))
                print(f"âœ… æ’å…¥æ–°è®°å½•: {entity_name}")

            print(f"ğŸ” [DEBUG] æäº¤äº‹åŠ¡")
            db.connection.commit()
            print(f"âœ… å…³é”®è¯å·²æˆåŠŸ{'æ›´æ–°' if exists else 'æ’å…¥'}åˆ°æ•°æ®åº“")
            return True

    except Exception as e:
        print(f"âŒ æ›´æ–°å…³é”®è¯å¤±è´¥ï¼š{e}")
        print(f"ğŸ” [DEBUG] é”™è¯¯ç±»å‹: {type(e)}")
        print(f"ğŸ” [DEBUG] å®Œæ•´é”™è¯¯ä¿¡æ¯:")
        import traceback
        traceback.print_exc()
        db.connection.rollback()
        return False

def auto_recommendtion(request: HttpRequest) -> dict:
    """
    ä¸»æ¨èå‡½æ•° - ä¼˜åŒ–ç‰ˆï¼šä¼˜å…ˆä»æ•°æ®åº“è¯»å–å…³é”®è¯
    """
    name = request.GET.get("name", "").strip()
    user_id = request.GET.get("user_id", "").strip()
    api_key = request.GET.get("api_key", "sk-uwokmhxknecolbmvcrnfstfrcqzjeuekvnxfoghzrakeqybw")
    max_keywords = int(request.GET.get("max_keywords", 5))
    num_pages_to_crawl = int(request.GET.get("num_pages_to_crawl", 40))

    print(f"ğŸ” å¼€å§‹å¤„ç†: name={name}, user_id={user_id}")


    if not name or not user_id:
        return {"status": "error", "message": "Missing required parameters: name or user_id"}

    # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
    db = None
    try:
        db = MySQLDatabase(
            host="114.213.234.179",
            user="koroot",
            password="DMiC-4092",
            database="db_hp"
        )
        db.connect()

        print("ğŸ” [DEBUG] æ£€æŸ¥æ•°æ®åº“è¡¨ç»“æ„...")
        check_table_structure(db)

        # 1. é¦–å…ˆå°è¯•ä»æ•°æ®åº“è·å–å…³é”®è¯
        clean_name = clean_entity_name(name)
        print("ğŸ“‹ æ­¥éª¤1: ä»æ•°æ®åº“æŸ¥è¯¢å…³é”®è¯...")
        db_keywords, has_keywords = get_keywords_from_db(db, name, user_id)

        if has_keywords and db_keywords:
            print(f"âœ… ä»æ•°æ®åº“è·å–åˆ°å…³é”®è¯: {db_keywords}")
            keywords = db_keywords
            keywords_source = "database"
            keywords_status = "from_db"

            # æ„å»ºæœç´¢æŸ¥è¯¢
            if len(keywords) >= 2:
                combined_query = f"{clean_name} {keywords[0]}"  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå…³é”®è¯
            elif len(keywords) == 1:
                combined_query = f"{clean_name} {keywords[0]}"
            else:
                combined_query = clean_name

        else:
            print("âŒ æ•°æ®åº“ä¸­æ²¡æœ‰å…³é”®è¯ï¼Œå¼€å§‹æ–‡ä»¶å¤„ç†æµç¨‹...")
            keywords_source = "file_analysis"

            # 2. ä¸‹è½½æ–‡ä»¶
            print("ğŸ“¥ æ­¥éª¤2: ä¸‹è½½ç›¸å…³æ–‡ä»¶...")
            folder = test(name, int(user_id))
            base_dir = os.path.join('D:\\data\\Auto_recommendtion', name)
            print(f"âœ… ä¸‹è½½å®Œæˆï¼Œæ–‡ä»¶ä¿å­˜åœ¨: {base_dir}")

            # 3. æ™ºèƒ½é€‰æ‹©æ–‡ä»¶
            print("ğŸ“ æ­¥éª¤3: ä»æ¯ä¸ªç±»åˆ«ä¸­éšæœºé€‰æ‹©æ–‡ä»¶...")
            selected_files = select_files_by_category(base_dir, clean_name)

            if not selected_files:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ–‡ä»¶ï¼Œä½¿ç”¨å®ä½“åä½œä¸ºé»˜è®¤æŸ¥è¯¢")
                combined_query = clean_name
                keywords = []
                keywords_status = "no_files"
            else:
                print(f"âœ… ä» {len(selected_files)} ä¸ªç±»åˆ«ä¸­é€‰æ‹©çš„æ–‡ä»¶")

                # 4. æ„å»ºæ™ºèƒ½æç¤ºæ–‡æœ¬
                print("ğŸ“ æ­¥éª¤4: æ„å»ºæ™ºèƒ½æç¤ºæ–‡æœ¬...")
                intelligent_text = build_intelligent_prompt(selected_files, clean_name, max_keywords)

                if not intelligent_text.strip():
                    print("âš ï¸ æœªèƒ½æå–åˆ°æœ‰æ•ˆæ–‡æœ¬å†…å®¹")
                    combined_query = clean_name
                    keywords = []
                    keywords_status = "no_content"
                else:
                    # 5. æå–å…³é”®è¯
                    print("ğŸ¤– æ­¥éª¤5: è°ƒç”¨å¤§æ¨¡å‹æå–å…³é”®è¯...")
                    print(f"ğŸ“‹ å‘é€ç»™æ¨¡å‹çš„æ–‡æœ¬é•¿åº¦: {len(intelligent_text)} å­—ç¬¦")

                    start_time = time.time()
                    result = call_big_model(
                        text=intelligent_text,
                        person_name=clean_name,
                        max_keywords=max_keywords,
                        api_key=api_key
                    )
                    end_time = time.time()
                    print(f"â±ï¸ å…³é”®è¯æå–è€—æ—¶: {end_time - start_time:.2f}ç§’")

                    if result:
                        print(f"ğŸ”‘ æ¨¡å‹è¿”å›ç»“æœ: {result}")

                        # éªŒè¯å’Œæå–å…³é”®è¯
                        keywords, is_valid = validate_and_extract_keywords(result, clean_name)

                        if is_valid and keywords:
                            print(f"âœ… æå–åˆ°çš„æœ‰æ•ˆå…³é”®è¯: {keywords}")

                            # åœ¨è°ƒç”¨ update_keywords_to_db ä¹‹å‰æ·»åŠ 
                            print(f"ğŸ” [DEBUG] å‡†å¤‡è°ƒç”¨ update_keywords_to_db")
                            print(f"ğŸ” [DEBUG] å½“å‰å…³é”®è¯: {keywords}")
                            print(f"ğŸ” [DEBUG] å…³é”®è¯ç±»å‹: {type(keywords)}")
                            print(f"ğŸ” [DEBUG] æ˜¯å¦æ˜¯åˆ—è¡¨: {isinstance(keywords, list)}")
                            if isinstance(keywords, list):
                                print(f"ğŸ” [DEBUG] åˆ—è¡¨é•¿åº¦: {len(keywords)}")
                                print(f"ğŸ” [DEBUG] åˆ—è¡¨å†…å®¹: {keywords}")

                            # 6. å°†æ–°å…³é”®è¯ä¿å­˜åˆ°æ•°æ®åº“
                            print("ğŸ’¾ æ­¥éª¤6: å°†å…³é”®è¯ä¿å­˜åˆ°æ•°æ®åº“...")
                            update_success = update_keywords_to_db(db, name, int(user_id), keywords)
                            if update_success:
                                print("âœ… å…³é”®è¯å·²æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“")
                            else:
                                print("âš ï¸ å…³é”®è¯ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥")

                            if len(keywords) >= 2:
                                combined_query = f"{clean_name} {keywords[0]}"
                                keywords_status = "valid_multiple"
                            elif len(keywords) == 1:
                                combined_query = f"{clean_name} {keywords[0]}"
                                keywords_status = "valid_single"
                            else:
                                combined_query = clean_name
                                keywords_status = "no_keywords_found"
                        else:
                            print("âŒ æ¨¡å‹è¿”å›æ— æ•ˆç»“æœï¼Œä½¿ç”¨å®ä½“å")
                            combined_query = clean_name
                            keywords = []
                            keywords_status = "invalid_output"
                    else:
                        print("âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å®ä½“å")
                        combined_query = clean_name
                        keywords = []
                        keywords_status = "api_failure"

        print(f"ğŸ” æœ€ç»ˆæœç´¢æŸ¥è¯¢: {combined_query} (æ¥æº: {keywords_source}, çŠ¶æ€: {keywords_status})")

        # 7. è¿›è¡Œç½‘ç»œæœç´¢
        print("ğŸŒ æ­¥éª¤7: è¿›è¡Œç½‘ç»œæœç´¢...")
        mock_request = type('MockRequest', (), {'GET': {
            "name": combined_query,
            "num_pages_to_crawl": num_pages_to_crawl,
            "userID": user_id,
            "xiaoqi_name": clean_name,
            "enable_deduplication": "true"
        }})

        search_result = search_urls(mock_request)
        result_count = len(search_result.get('data', [])) if isinstance(search_result,
                                                                        dict) and 'data' in search_result else 0
        print(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {result_count} ä¸ªç»“æœ")

        # 8. è¿›è¡Œå…³é”®è¯è¿‡æ»¤
        print("ğŸ” æ­¥éª¤8: è¿›è¡Œå…³é”®è¯è¿‡æ»¤...")
        combined_query_list = [clean_name] + keywords
        search_result = filter_search_results_by_keywords(search_result, combined_query_list)

        return {
            "status": "success",
            "data": search_result,
            "keywords": keywords,
            "combined_query": combined_query,
            "keywords_source": keywords_source,
            "keywords_status": keywords_status,
            "from_database": has_keywords if keywords_source == "database" else False
        }

    except Exception as e:
        print(f"âŒ å‘ç”Ÿå¼‚å¸¸: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"status": "error", "message": str(e)}
    finally:
        # å…³é—­æ•°æ®åº“è¿æ¥
        if db and db.connection:
            db.close()

#
# def debug_full_process():
#     """é€æ­¥è°ƒè¯•å®Œæ•´æµç¨‹"""
#     print("=== å®Œæ•´æµç¨‹è°ƒè¯• ===")
#
#     # æ¨¡æ‹Ÿè¯·æ±‚
#     class MockRequest:
#         def __init__(self):
#             self.GET = {
#                 "name": "å± å‘¦å‘¦1",
#                 "user_id": "6000622",
#                 "max_keywords": "5",
#                 "num_pages_to_crawl": "20"
#             }
#
#     request = MockRequest()
#     result = auto_recommendtion(request)
#
#     print(f"\n=== æœ€ç»ˆç»“æœ ===")
#     print(f"çŠ¶æ€: {result.get('status')}")
#     print(f"å…³é”®è¯: {result.get('keywords', [])}")
#     print(f"æœç´¢æŸ¥è¯¢: {result.get('combined_query', '')}")
#     print(f"æ–‡ä»¶æ•°é‡: {result.get('file_count', 0)}")
#     print(f"å®Œæ•´è¿”å›: {result}")
#
#
#
# if __name__ == "__main__":
#     # print("=== æµ‹è¯•æ–‡ä»¶è¯»å– ===")
#     # test_file_reading()
#     #
#     # print("\n=== æµ‹è¯•æ¨¡å‹API ===")
#     # test_model_api()
#     #
#     # print("\n=== è°ƒè¯•å®Œæ•´æµç¨‹ ===")
#     T0= time.time()
#     debug_full_process()
#     T1 = time.time()
#     print("å…¨éƒ¨è¿è¡Œæ—¶é—´",T1-T0)
#
