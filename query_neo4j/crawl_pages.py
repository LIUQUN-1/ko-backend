from bs4 import BeautifulSoup
import os
import time
import random
import threading
import json
import logging
from query_neo4j.disambiguation import process_file
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
import asyncio
from crawl4ai import AsyncWebCrawler
from urllib.parse import urlparse, quote, urlunparse
import redis
import hashlib

# è·å–Djangoé…ç½®çš„æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger('query_neo4j')

# ä¿å­˜ç›®å½•
SAVE_DIR = "D:/upload/"


class CacheManager:
    """Redisç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, host='114.213.232.140', port=26379, db=0, expire_time=3600):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨

        å‚æ•°:
        - host: RedisæœåŠ¡å™¨åœ°å€
        - port: Redisç«¯å£
        - db: Redisæ•°æ®åº“ç¼–å·
        - expire_time: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤1å°æ—¶
        """
        self.expire_time = expire_time
        self.redis_client = None
        self.is_connected = False

        try:
            # å°è¯•è¿æ¥Redis
            self.redis_client = redis.Redis(
                host=host,
                port=port,
                db=db,
                decode_responses=True,
                socket_timeout=5,
                socket_connect_timeout=5
            )
            # æµ‹è¯•è¿æ¥
            self.redis_client.ping()
            self.is_connected = True
            logger.info(f"âœ… Redisè¿æ¥æˆåŠŸ {host}:{port}")
        except Exception as e:
            logger.warning(f"âš ï¸ Redisè¿æ¥å¤±è´¥ï¼Œå°†ä¸ä½¿ç”¨ç¼“å­˜åŠŸèƒ½: {str(e)}")
            self.redis_client = None
            self.is_connected = False

    def _generate_cache_key(self, url):
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_string = f"crawl_cache:{url}"
        # ä½¿ç”¨MD5å“ˆå¸Œé¿å…é”®åè¿‡é•¿
        return hashlib.md5(key_string.encode()).hexdigest()

    def check_cache_and_file(self, url):
        """æ£€æŸ¥Redisç¼“å­˜å’Œæœ¬åœ°æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        if not self.is_connected:
            return None

        try:
            cache_key = self._generate_cache_key(url)
            cached_data = self.redis_client.get(cache_key)

            if not cached_data:
                logger.info(f"URLæœªåœ¨ç¼“å­˜ä¸­æ‰¾åˆ°: {url}")
                return None

            # è§£æç¼“å­˜æ•°æ®
            cache_info = json.loads(cached_data)
            file_path = cache_info.get('file_path')

            # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if file_path and os.path.exists(file_path):
                logger.info(f"âœ… ä»ç¼“å­˜ä¸­æ‰¾åˆ°URL: {url}, æœ¬åœ°æ–‡ä»¶å­˜åœ¨: {file_path}")
                return {
                    'url': cache_info.get('url'),
                    'file_path': cache_info.get('file_path'),
                    'file_name': cache_info.get('file_name'),
                    'file_type': cache_info.get('file_type'),
                    'cached_time': cache_info.get('cached_time'),
                    'from_cache': True
                }
            else:
                logger.warning(f"âš ï¸ ç¼“å­˜ä¸­æ‰¾åˆ°URLä½†æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ é™¤ç¼“å­˜: {url}")
                # æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ é™¤ç¼“å­˜
                self.redis_client.delete(cache_key)
                return None

        except Exception as e:
            logger.error(f"æ£€æŸ¥ç¼“å­˜æ—¶å‡ºé”™: {str(e)}")
            return None

    def save_to_cache(self, url, file_path, file_name, file_type):
        """ä¿å­˜åˆ°Redisç¼“å­˜"""
        if not self.is_connected:
            return False

        try:
            cache_key = self._generate_cache_key(url)
            cache_data = {
                'url': url,
                'file_path': file_path,
                'file_name': file_name,
                'file_type': file_type,
                'cached_time': int(time.time())
            }

            # ä¿å­˜åˆ°Redisï¼Œè®¾ç½®è¿‡æœŸæ—¶é—´
            self.redis_client.setex(cache_key, self.expire_time, json.dumps(cache_data))
            logger.info(f"âœ… å·²ç¼“å­˜URL: {url}, æ–‡ä»¶: {file_name}")
            return True

        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜æ—¶å‡ºé”™: {str(e)}")
            return False

    def clear_cache(self, url=None):
        """æ¸…ç†ç¼“å­˜"""
        if not self.is_connected:
            return False

        try:
            if url:
                # åˆ é™¤ç‰¹å®šURLçš„ç¼“å­˜
                cache_key = self._generate_cache_key(url)
                self.redis_client.delete(cache_key)
                logger.info(f"å·²æ¸…ç†URLç¼“å­˜: {url}")
            else:
                # æ¸…ç†æ‰€æœ‰çˆ¬è™«ç¼“å­˜
                keys = self.redis_client.keys("crawl_cache:*")
                if keys:
                    self.redis_client.delete(*keys)
                    logger.info(f"å·²æ¸…ç†æ‰€æœ‰çˆ¬è™«ç¼“å­˜ï¼Œå…± {len(keys)} ä¸ª")
            return True
        except Exception as e:
            logger.error(f"æ¸…ç†ç¼“å­˜æ—¶å‡ºé”™: {str(e)}")
            return False

    def get_cache_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if not self.is_connected:
            return {
                'connected': False,
                'total_keys': 0,
                'crawl_cache_keys': 0
            }

        try:
            crawl_keys = self.redis_client.keys("crawl_cache:*")
            return {
                'connected': True,
                'total_keys': self.redis_client.dbsize(),
                'crawl_cache_keys': len(crawl_keys),
                'expire_time': self.expire_time
            }
        except Exception as e:
            logger.error(f"è·å–ç¼“å­˜ç»Ÿè®¡æ—¶å‡ºé”™: {str(e)}")
            return {'connected': False, 'error': str(e)}


# åˆå§‹åŒ–å…¨å±€ç¼“å­˜ç®¡ç†å™¨
cache_manager = CacheManager()


# PROXIES = {
#     "http": "http://127.0.0.1:7897",
#     "https": "http://127.0.0.1:7897",
# }

# # åŒæ—¶è®¾ç½®ç¯å¢ƒå˜é‡
# os.environ["HTTP_PROXY"] = "http://127.0.0.1:7897"
# os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7897"
def get_file_type_from_url(url):
    """
    ä»URLä¸­æ£€æµ‹æ–‡ä»¶ç±»å‹

    å‚æ•°:
    - url: è¦æ£€æµ‹çš„URL

    è¿”å›:
    - str: æ–‡ä»¶ç±»å‹ ('pdf', 'docx', 'doc', 'ppt', 'pptx', 'txt', 'html' ç­‰)
    """
    url_lower = url.lower()
    if url_lower.endswith('.pdf'):
        return 'pdf'
    elif url_lower.endswith('.docx'):
        return 'docx'
    elif url_lower.endswith('.doc'):
        return 'doc'
    elif url_lower.endswith('.pptx'):
        return 'pptx'
    elif url_lower.endswith('.ppt'):
        return 'ppt'
    elif url_lower.endswith('.txt'):
        return 'txt'
    else:
        return 'html'  # é»˜è®¤ä¸ºHTMLé¡µé¢


def download_file(url, file_path, file_type='pdf'):
    """
    ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°

    å‚æ•°:
    - url: æ–‡ä»¶çš„URL
    - file_path: æœ¬åœ°ä¿å­˜è·¯å¾„
    - file_type: æ–‡ä»¶ç±»å‹

    è¿”å›:
    - bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
    """
    try:
        # æ ¹æ®æ–‡ä»¶ç±»å‹è®¾ç½®ä¸åŒçš„è¯·æ±‚å¤´
        if file_type == 'pdf':
            accept_header = 'application/pdf,application/octet-stream,*/*'
        elif file_type in ['doc', 'docx']:
            accept_header = 'application/msword,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/octet-stream,*/*'
        elif file_type in ['ppt', 'pptx']:
            accept_header = 'application/vnd.ms-powerpoint,application/vnd.openxmlformats-officedocument.presentationml.presentation,application/octet-stream,*/*'
        elif file_type == 'txt':
            accept_header = 'text/plain,text/*,application/octet-stream,*/*'
        else:
            accept_header = '*/*'

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Accept': accept_header,
            'Accept-Language': 'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }

        logger.info(f"å¼€å§‹ä¸‹è½½{file_type.upper()}æ–‡ä»¶: {url}")

        # response = requests.get(url, headers=headers, proxies=PROXIES, verify=False, stream=True)
        response = requests.get(url, headers=headers, verify=False, stream=True)
        if response.status_code == 200:
            # æ£€æŸ¥Content-Type
            content_type = response.headers.get('Content-Type', '').lower()
            logger.info(f"æ–‡ä»¶Content-Type: {content_type}")

            # éªŒè¯æ–‡ä»¶ç±»å‹
            expected_types = {
                'pdf': ['application/pdf'],
                'docx': ['application/vnd.openxmlformats-officedocument.wordprocessingml.document'],
                'doc': ['application/msword'],
                'pptx': ['application/vnd.openxmlformats-officedocument.presentationml.presentation'],
                'ppt': ['application/vnd.ms-powerpoint'],
                'txt': ['text/plain', 'text/html', 'text/']
            }

            if file_type in expected_types:
                valid_type = any(expected in content_type for expected in expected_types[file_type])
                if not valid_type and not url.lower().endswith(f'.{file_type}'):
                    logger.warning(f"URLå¯èƒ½ä¸æ˜¯{file_type.upper()}æ–‡ä»¶: {url}, Content-Type: {content_type}")

            # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(file_path), exist_ok=True)

            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)

            # éªŒè¯æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logger.error(f"ä¸‹è½½çš„{file_type.upper()}æ–‡ä»¶ä¸ºç©º: {file_path}")
                return False

            logger.info(f"{file_type.upper()}æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {file_path}, å¤§å°: {file_size} bytes")
            return True
        else:
            logger.error(f"{file_type.upper()}ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, URL: {url}")
            return False

    except requests.exceptions.ProxyError as e:
        logger.error(f"ä»£ç†è¿æ¥å¤±è´¥ï¼Œä¸‹è½½{file_type.upper()}æ–‡ä»¶: {url}, é”™è¯¯: {str(e)}")
        logger.warning("å»ºè®®æ£€æŸ¥ä»£ç†æœåŠ¡å™¨æ˜¯å¦æ­£å¸¸è¿è¡Œ")
        return False
    except requests.exceptions.ConnectionError as e:
        logger.error(f"ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œä¸‹è½½{file_type.upper()}æ–‡ä»¶: {url}, é”™è¯¯: {str(e)}")
        return False
    except Exception as e:
        logger.error(f"ä¸‹è½½{file_type.upper()}æ–‡ä»¶æ—¶å‡ºé”™: {url}, é”™è¯¯: {str(e)}")
        return False


# process_document_file å‡½æ•°å·²ç§»é™¤ï¼Œæ”¹ä¸ºæ‰¹é‡å¤„ç†æ¨¡å¼
def ensure_save_dir():
    """ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨"""
    if not os.path.exists(SAVE_DIR):
        os.makedirs(SAVE_DIR)


def clean_filename(title):
    """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
    return "".join(c if c.isalnum() or c in (" ", "_", "-") else "_" for c in title)


def extract_title_from_html(html_content):
    """ä»HTMLå†…å®¹ä¸­æå–titleä½œä¸ºæ–‡ä»¶å"""
    try:
        soup = BeautifulSoup(html_content, "html.parser")

        # ä¼˜å…ˆä½¿ç”¨titleæ ‡ç­¾
        title_tag = soup.find('title')
        if title_tag and title_tag.string:
            title = title_tag.string.strip()
            if title and len(title) > 0:
                # æ¸…ç†æ ‡é¢˜ï¼Œé™åˆ¶é•¿åº¦
                title = title[:100]  # é™åˆ¶æ ‡é¢˜é•¿åº¦
                filename = clean_filename(title)
                if filename and len(filename.strip()) > 0:
                    logger.info(f"ä»HTMLä¸­æå–åˆ°é¡µé¢æ ‡é¢˜: {filename}")
                    return f"{filename}.html"

        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„titleï¼Œä½¿ç”¨æ—¶é—´æˆ³
        timestamp = int(time.time())
        fallback_name = f"webpage_{timestamp}.html"
        logger.warning(f"æœªæ‰¾åˆ°æœ‰æ•ˆçš„é¡µé¢æ ‡é¢˜ï¼Œä½¿ç”¨fallbackæ–‡ä»¶å: {fallback_name}")
        return fallback_name

    except Exception as e:
        logger.error(f"ä»HTMLæå–æ ‡é¢˜æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
        # æœ€åçš„fallbackï¼šä½¿ç”¨æ—¶é—´æˆ³
        timestamp = int(time.time())
        return f"webpage_{timestamp}.html"


async def fetch_html_with_crawl4ai(url):
    """ä½¿ç”¨Crawl4AIè·å–é¡µé¢å†…å®¹"""
    try:
        from crawl4ai import AsyncWebCrawler

        logger.info(f"ğŸ•·ï¸ ä½¿ç”¨Crawl4AIçˆ¬å–é¡µé¢: {url}")

        async with AsyncWebCrawler(verbose=False) as crawler:
            # é…ç½®çˆ¬å–å‚æ•°
            result = await crawler.arun(
                url=url,
                # ç§»é™¤ä¸å¿…è¦çš„å…ƒç´ 
                exclude_tags=['script', 'style', 'nav', 'footer', 'header'],
                # ç­‰å¾…é¡µé¢åŠ è½½
                wait_for="body",
                # æå–æ–‡æœ¬å†…å®¹
                word_count_threshold=10
            )

            if result.success:
                # ä¼˜å…ˆä½¿ç”¨Crawl4AIæå–çš„Markdownå†…å®¹
                content = result.markdown
                title = result.metadata.get('title', '')

                # å¦‚æœMarkdownå†…å®¹ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹HTML
                if not content or len(content.strip()) < 50:
                    content = result.html
                    logger.warning(f"Markdownå†…å®¹è¿‡å°‘ï¼Œä½¿ç”¨åŸå§‹HTML: {url}")

                logger.info(f"âœ… Crawl4AIçˆ¬å–æˆåŠŸ: {url}")
                logger.info(f"   ğŸ“„ æ ‡é¢˜: {title}")
                logger.info(f"   ğŸ“Š å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                logger.info(
                    f"   ğŸ”— é“¾æ¥æ•°é‡: {len(result.links.get('internal', []) + result.links.get('external', []))}")

                return {
                    'success': True,
                    'html': result.html,
                    'title': title,
                    'metadata': result.metadata,
                    'links': result.links,
                    'media': result.media
                }
            else:
                logger.error(f"âŒ Crawl4AIçˆ¬å–å¤±è´¥: {url}, é”™è¯¯: {result.error_message}")
                return {
                    'success': False,
                    'error': result.error_message
                }

    except ImportError:
        logger.warning("âš ï¸ Crawl4AIæœªå®‰è£…ï¼Œå°†ä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ")
        return {
            'success': False,
            'error': 'Crawl4AI not installed'
        }
    except Exception as e:
        logger.error(f"âŒ Crawl4AIçˆ¬å–å¼‚å¸¸: {url}, é”™è¯¯: {str(e)}")
        return {
            'success': False,
            'error': str(e)
        }


def run_async_function_in_thread(async_func, *args):
    """åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°"""
    try:
        # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(async_func(*args))
        except Exception as e:
            return None
        finally:
            loop.close()
    except Exception as e:
        logger.error(f"å¼‚æ­¥å‡½æ•°æ‰§è¡Œå¤±è´¥: {str(e)}")
        return None


def save_file(filename, content):
    """ä¿å­˜æ–‡ä»¶å†…å®¹çš„è¾…åŠ©å‡½æ•°"""
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)


# process_urls_batch å‡½æ•°å·²ç§»é™¤ï¼Œç°åœ¨ç›´æ¥åœ¨å¤šçº¿ç¨‹ä¸­è°ƒç”¨ process_single_url

def process_single_url(url, file_name, index, name):
    """å¤„ç†å•ä¸ªURLï¼Œæ”¯æŒHTMLé¡µé¢ã€PDFæ–‡ä»¶ã€DOC/DOCXæ–‡ä»¶ã€PPT/PPTXæ–‡ä»¶ã€TXTæ–‡ä»¶çš„çˆ¬å–å’Œå¤„ç†"""
    try:
        logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {index + 1} ä¸ªURL: {url}")

        # æ£€æŸ¥ç¼“å­˜
        cached_result = cache_manager.check_cache_and_file(url)
        if cached_result:
            return cached_result

        # æ£€æµ‹æ–‡ä»¶ç±»å‹
        file_type = get_file_type_from_url(url)
        logger.info(f"æ£€æµ‹åˆ°æ–‡ä»¶ç±»å‹: {file_type.upper()}")

        # æ ¹æ®æ–‡ä»¶ç±»å‹è¿›è¡Œä¸åŒçš„å¤„ç†
        if file_type in ['pdf', 'doc', 'docx', 'ppt', 'pptx', 'txt']:
            # å¤„ç†æ–‡æ¡£æ–‡ä»¶
            logger.info(f"å¼€å§‹å¤„ç†{file_type.upper()}æ–‡ä»¶: {url}")

            # åªæœ‰åœ¨æ²¡æœ‰æä¾›file_nameçš„æƒ…å†µä¸‹æ‰è¿›è¡Œæ–‡ä»¶åè§£æ
            if not file_name or file_name.strip() == "":
                logger.info(f"ğŸ“ æœªæä¾›æ–‡ä»¶åï¼Œä¸º{file_type.upper()}æ–‡ä»¶ç”Ÿæˆæ–‡ä»¶å: {url}")
                try:
                    from urllib.parse import urlparse
                    parsed_url = urlparse(url)
                    path = parsed_url.path

                    # å°è¯•ä»URLè·¯å¾„ä¸­æå–æ–‡ä»¶å
                    if path and path != '/':
                        url_filename = path.split('/')[-1]
                        if url_filename and '.' in url_filename:
                            # å¦‚æœURLä¸­æœ‰æ–‡ä»¶åï¼Œä½¿ç”¨å®ƒ
                            base_name = url_filename.rsplit('.', 1)[0]
                            file_name = f"{clean_filename(base_name)}.{file_type}"
                        else:
                            # å¦‚æœURLä¸­æ²¡æœ‰æ˜ç¡®çš„æ–‡ä»¶åï¼Œä½¿ç”¨URLçš„æœ€åä¸€æ®µ
                            base_name = clean_filename(url_filename) if url_filename else "document"
                            file_name = f"{base_name}.{file_type}"
                    else:
                        # å¦‚æœæ— æ³•ä»URLæå–ï¼Œä½¿ç”¨åŸŸåå’Œæ—¶é—´æˆ³
                        domain = parsed_url.netloc.replace('www.', '').replace('.', '_')
                        timestamp = int(time.time())
                        file_name = f"{clean_filename(domain)}_{timestamp}.{file_type}"

                    logger.info(f"âœ… ä¸º{file_type.upper()}æ–‡ä»¶ç”Ÿæˆçš„æ–‡ä»¶å: {file_name}")

                except Exception as e:
                    logger.error(f"ç”Ÿæˆ{file_type.upper()}æ–‡ä»¶åå¤±è´¥: {str(e)}")
                    # fallbackæ–‡ä»¶å
                    timestamp = int(time.time())
                    file_name = f"document_{timestamp}.{file_type}"
            else:
                logger.info(f"ğŸ“„ ä½¿ç”¨æä¾›çš„æ–‡ä»¶å: {file_name}")

            # è°ƒæ•´æ–‡ä»¶åæ‰©å±•åï¼ˆç¡®ä¿file_nameä¸ä¸ºNoneï¼‰
            if file_name and not file_name.endswith(f'.{file_type}'):
                file_name = file_name.replace('.html', f'.{file_type}')
                if not file_name.endswith(f'.{file_type}'):
                    file_name = f"{file_name}.{file_type}"

            document_file_path = os.path.join(SAVE_DIR, file_name)

            # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
            ensure_save_dir()

            # ä¸‹è½½æ–‡æ¡£æ–‡ä»¶
            download_success = download_file(url, document_file_path, file_type)
            if not download_success:
                return {
                    "status": "error",
                    "message": f"{file_type.upper()}æ–‡ä»¶ä¸‹è½½å¤±è´¥",
                    "url": url,
                    "file_type": file_type
                }
            else:
                # ä¿å­˜åˆ°ç¼“å­˜
                cache_manager.save_to_cache(url, document_file_path, file_name, file_type)
                return {
                    "status": "success",
                    "message": f"{file_type.upper()}æ–‡ä»¶ä¸‹è½½æˆåŠŸ",
                    "url": url,
                    "file_path": document_file_path,
                    "file_name": file_name,
                    "file_type": file_type
                }

        else:
            # å¤„ç†HTMLé¡µé¢ï¼ˆä½¿ç”¨Crawl4AIï¼‰
            logger.info(f"å¤„ç†HTMLé¡µé¢: {url}")

            # ä½¿ç”¨Crawl4AIçˆ¬å–é¡µé¢
            crawl4ai_result = run_async_function_in_thread(fetch_html_with_crawl4ai, url)

            if not crawl4ai_result or not crawl4ai_result.get('success'):
                error_msg = crawl4ai_result.get('error', 'æœªçŸ¥é”™è¯¯') if crawl4ai_result else 'çˆ¬å–å¤±è´¥'
                logger.error(f"Crawl4AIè·å–HTMLå†…å®¹å¤±è´¥: {url}, é”™è¯¯: {error_msg}")

                # ç›´æ¥ç”¨ requests å…œåº•
                import requests
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7'
                }
                try:
                    response = requests.get(url, headers=headers, timeout=15, verify=False)
                    html_content = response.text if response.status_code == 200 else None
                except Exception as e:
                    logger.error(f"requestså…œåº•å¤±è´¥: {url}, é”™è¯¯: {str(e)}")
                    html_content = None
                if html_content and len(html_content) > 100:
                    # è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
                    if not file_name or file_name.strip() == "":
                        file_name = extract_title_from_html(html_content)
                    file_path = os.path.join(SAVE_DIR, file_name)
                    ensure_save_dir()
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(html_content)
                    logger.info(f"âœ… å·²ç”¨requestså…œåº•ä¿å­˜HTMLæ–‡ä»¶: {file_path}")
                    # ä¿å­˜åˆ°ç¼“å­˜
                    cache_manager.save_to_cache(url, file_path, file_name, "html")
                    return {
                        "status": "success",
                        "message": "requestså…œåº•çˆ¬å–æˆåŠŸ",
                        "url": url,
                        "file_path": file_path,
                        "file_name": file_name,
                        "file_type": "html"
                    }
                else:
                    return {"status": "error", "message": f"è·å–é¡µé¢å¤±è´¥: {error_msg}", "url": url, "file_type": "html"}

            try:
                # ä»Crawl4AIç»“æœä¸­è·å–å†…å®¹
                html_content = crawl4ai_result.get('html', '')
                page_title = crawl4ai_result.get('title', '')
                metadata = crawl4ai_result.get('metadata', {})

                # åªæœ‰åœ¨æ²¡æœ‰æä¾›file_nameçš„æƒ…å†µä¸‹æ‰è¿›è¡Œæ–‡ä»¶è§£æ
                if not file_name or file_name.strip() == "":
                    logger.info(f"ğŸ“ æœªæä¾›æ–‡ä»¶åï¼Œå¼€å§‹è‡ªåŠ¨è§£æ: {url}")
                    if page_title and page_title.strip():
                        # ä½¿ç”¨Crawl4AIæå–çš„æ ‡é¢˜
                        title = clean_filename(page_title.strip()[:100])
                        file_name = f"{title}.html"
                        logger.info(f"âœ… ä½¿ç”¨Crawl4AIæå–çš„æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å: {file_name}")
                    else:
                        # ä»HTMLå†…å®¹ä¸­æå–title
                        logger.info(f"ğŸ” ä»é¡µé¢å†…å®¹ä¸­æå–æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å: {url}")
                        file_name = extract_title_from_html(html_content)
                        logger.info(f"âœ… æå–åˆ°çš„æ–‡ä»¶å: {file_name}")
                else:
                    logger.info(f"ğŸ“„ ä½¿ç”¨æä¾›çš„æ–‡ä»¶å: {file_name}")

                content_to_save = html_content
                logger.warning(f"ä½¿ç”¨åŸå§‹HTMLå†…å®¹ï¼Œé•¿åº¦: {len(content_to_save)} å­—ç¬¦")
                filename = os.path.join(SAVE_DIR, file_name)

                # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
                ensure_save_dir()

                # ä¿å­˜æ–‡ä»¶ï¼ˆç›´æ¥ä¿å­˜å¤„ç†åçš„å†…å®¹ï¼‰
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(content_to_save)

                logger.info(f"âœ… å·²ä¿å­˜HTMLæ–‡ä»¶: {filename}")
                logger.info(f"   ğŸ“„ é¡µé¢æ ‡é¢˜: {page_title}")
                logger.info(f"   ğŸ“Š å†…å®¹é•¿åº¦: {len(content_to_save)} å­—ç¬¦")
                logger.info(
                    f"   ğŸ”— æå–é“¾æ¥æ•°: {len(crawl4ai_result.get('links', {}).get('internal', []) + crawl4ai_result.get('links', {}).get('external', []))}")

                # ä¿å­˜åˆ°ç¼“å­˜
                cache_manager.save_to_cache(url, filename, file_name, "html")

                # åªä¿å­˜HTMLæ–‡ä»¶ï¼Œä¸ç«‹å³å¤„ç†
                return {
                    "status": "success",
                    "message": "Crawl4AIçˆ¬å–æˆåŠŸ",
                    "url": url,
                    "file_path": filename,
                    "file_name": file_name,
                    "file_type": "html",
                    "crawl4ai_metadata": {
                        "title": page_title,
                        "links_count": len(
                            crawl4ai_result.get('links', {}).get('internal', []) + crawl4ai_result.get('links', {}).get(
                                'external', [])),
                        "content_length": len(content_to_save),
                        "metadata": metadata
                    }
                }
            except Exception as e:
                logger.error(f"å¤„ç†Crawl4AIç»“æœæ—¶å‡ºé”™: {str(e)}")
                return {"status": "error", "message": f"å¤„ç†å†…å®¹å‡ºé”™: {str(e)}", "url": url, "file_type": "html"}

    except Exception as e:
        logger.error(f"å¤„ç†URLæ—¶å‘ç”Ÿå¼‚å¸¸: {url}, é”™è¯¯: {str(e)}")
        return {"status": "error", "message": f"å¤„ç†å¼‚å¸¸: {str(e)}", "url": url}


def process_urls_multithreaded(url_list, name, max_workers=3):
    """
    å¤šçº¿ç¨‹å¤„ç†URLåˆ—è¡¨

    å‚æ•°:
    - url_list: URLåˆ—è¡¨
    - name: æœç´¢å…³é”®è¯
    - max_workers: æœ€å¤§çº¿ç¨‹æ•°ï¼Œé»˜è®¤3ä¸ª

    è¿”å›:
    - å¤„ç†ç»“æœåˆ—è¡¨
    """
    if not url_list:
        return []

    # æ ¹æ®URLæ•°é‡è°ƒæ•´çº¿ç¨‹æ•°
    optimal_workers = min(max_workers, len(url_list), 10)  # æœ€å¤š10ä¸ªçº¿ç¨‹

    logger.info(f"å¼€å§‹å¤šçº¿ç¨‹å¤„ç†ï¼ŒURLæ€»æ•°: {len(url_list)}, çº¿ç¨‹æ•°: {optimal_workers}")

    all_results = []

    # ä½¿ç”¨ThreadPoolExecutorè¿›è¡Œå¤šçº¿ç¨‹å¤„ç†
    with ThreadPoolExecutor(max_workers=optimal_workers, thread_name_prefix='URLCrawler') as executor:
        # ä¸ºæ¯ä¸ªURLæäº¤ä¸€ä¸ªä»»åŠ¡
        future_to_url = {}
        for item in url_list:
            url = item.get('url')
            file_name = item.get('file_name', None)
            index = item.get('index', 0)

            # ç›´æ¥æäº¤process_single_urlä»»åŠ¡
            future = executor.submit(process_single_url, url, file_name, index, name)
            future_to_url[future] = {
                'url': url,
                'index': index,
                'file_name': file_name
            }

        # æ”¶é›†ç»“æœ
        completed_count = 0
        total_count = len(url_list)
        for future in as_completed(future_to_url):
            url_info = future_to_url[future]
            try:
                result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                all_results.append(result)
                completed_count += 1

                # è®°å½•è¿›åº¦
                if completed_count % 5 == 0 or completed_count == total_count:
                    logger.info(f"å¤šçº¿ç¨‹å¤„ç†è¿›åº¦: {completed_count}/{total_count}")

            except Exception as e:
                logger.error(f"å¤„ç†URLå¤±è´¥: {url_info['url']}, é”™è¯¯: {str(e)}")
                all_results.append({
                    "status": "error",
                    "message": f"å¤„ç†è¶…æ—¶æˆ–å¤±è´¥: {str(e)}",
                    "url": url_info['url'],
                    "index": url_info['index']
                })
                completed_count += 1

    logger.info(f"å¤šçº¿ç¨‹å¤„ç†å®Œæˆï¼Œæ€»ç»“æœæ•°: {len(all_results)}")
    return all_results


def crawl_pages(request):
    """
    æ¥å£å‡½æ•°ï¼šæ¥æ”¶URLåˆ—è¡¨çˆ¬å–ç½‘é¡µå¹¶è°ƒç”¨disambiguationæ¥å£

    å‚æ•°:
    - url_list: URLåˆ—è¡¨åŠç›¸å…³ä¿¡æ¯ï¼ˆJSONæ ¼å¼çš„å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
    - name: æœç´¢å…³é”®è¯
    - userid: ç”¨æˆ·ID
    - only_name: æ˜¯å¦åªä½¿ç”¨åç§°è¿›è¡Œåˆ†ç±»ï¼ˆå¯é€‰ï¼Œé»˜è®¤Falseï¼‰
    - use_multithreading: æ˜¯å¦ä½¿ç”¨å¤šçº¿ç¨‹ï¼ˆå¯é€‰ï¼Œé»˜è®¤Trueï¼‰
    - max_workers: æœ€å¤§çº¿ç¨‹æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤2ï¼‰
    - private:ä¿å­˜æ–‡ä»¶ä¸ºå…¬å¼€è¿˜æ˜¯ç§æœ‰ 0?1ï¼ˆå¯é€‰ï¼Œé»˜è®¤1ç§æœ‰ï¼‰

    è¿”å›:
    - å¤„ç†ç»“æœ
    """
    try:
        # ä»POSTè¯·æ±‚ä¸­è·å–å‚æ•°
        if request.method == 'POST':
            try:
                data = json.loads(request.body)
                url_list = data.get('url_list')
                name = data.get('name')
                userid = data.get('userid')
                use_multithreading = data.get('use_multithreading', True)
                max_workers = data.get('max_workers', 2)
                only_name = data.get('only_name', False)
                private = data.get('private', 1)
            except json.JSONDecodeError:
                return {"status": "error", "message": "æ— æ•ˆçš„JSONæ ¼å¼"}
        else:
            # ä»GETè¯·æ±‚ä¸­è·å–å‚æ•°
            url_list_str = request.GET.get('url_list')
            try:
                url_list = json.loads(url_list_str) if url_list_str else None
            except json.JSONDecodeError:
                return {"status": "error", "message": "æ— æ•ˆçš„URLåˆ—è¡¨JSONæ ¼å¼"}
            name = request.GET.get('name')
            userid = request.GET.get('userid')
            use_multithreading = request.GET.get('use_multithreading', 'true').lower() == 'true'
            max_workers = int(request.GET.get('max_workers', 3))
            only_name = request.GET.get('only_name', 'false').lower() == 'true'
        if not url_list or not isinstance(url_list, list):
            return {"status": "error", "message": "ç¼ºå°‘URLåˆ—è¡¨æˆ–æ ¼å¼ä¸æ­£ç¡®"}

        if not name:
            return {"status": "error", "message": "ç¼ºå°‘nameå‚æ•°"}

        if not userid:
            return {"status": "error", "message": "ç¼ºå°‘useridå‚æ•°"}

        # éªŒè¯å’Œæ¸…ç†URLåˆ—è¡¨
        valid_urls = []
        invalid_items = []

        for i, item in enumerate(url_list):
            if not isinstance(item, dict):
                invalid_items.append({"index": i, "reason": "URLé¡¹æ ¼å¼ä¸æ­£ç¡®", "item": str(item)})
                continue

            url = item.get('url')
            file_name = item.get('file_name', None)

            # åªè¦æœ‰URLå°±å¯ä»¥ï¼Œfile_nameå¯ä»¥ä¸ºç©ºï¼ˆå°†è‡ªåŠ¨ä»ç½‘é¡µæå–ï¼‰
            if not url:
                invalid_items.append({"index": i, "reason": "URLé¡¹ç¼ºå°‘urlå­—æ®µ", "item": item})
                continue

            # è®°å½•æ˜¯å¦éœ€è¦è‡ªåŠ¨è§£ææ–‡ä»¶å
            if not file_name or file_name.strip() == "":
                logger.info(f"ğŸ“ URLé¡¹ {i} æ²¡æœ‰file_nameï¼Œå°†è‡ªåŠ¨è§£ææ–‡ä»¶å")
            else:
                logger.info(f"ğŸ“„ URLé¡¹ {i} ä½¿ç”¨æä¾›çš„file_name: {file_name}")

            # è®¾ç½®é»˜è®¤ç´¢å¼•
            if 'index' not in item:
                item['index'] = i

            valid_urls.append(item)

        logger.info(f"URLéªŒè¯å®Œæˆï¼Œæœ‰æ•ˆ: {len(valid_urls)}, æ— æ•ˆ: {len(invalid_items)}")

        if not valid_urls:
            return {"status": "error", "message": "æ²¡æœ‰æœ‰æ•ˆçš„URLé¡¹", "invalid_items": invalid_items}

        # é€‰æ‹©å¤„ç†æ–¹å¼
        if use_multithreading and len(valid_urls) > 1:
            logger.info(f"ä½¿ç”¨å¤šçº¿ç¨‹æ¨¡å¼å¤„ç† {len(valid_urls)} ä¸ªURLï¼Œæœ€å¤§çº¿ç¨‹æ•°: {max_workers}")
            results = process_urls_multithreaded(valid_urls, name, max_workers)
        else:
            logger.info(f"ä½¿ç”¨å•çº¿ç¨‹æ¨¡å¼å¤„ç† {len(valid_urls)} ä¸ªURL")
            results = []
            for item in valid_urls:
                url = item.get('url')
                file_name = item.get('file_name', None)
                index = item.get('index', 0)

                # ç›´æ¥ä¼ é€’file_nameï¼ˆå¯èƒ½ä¸ºNoneï¼‰ï¼Œè®©process_single_urlå‡½æ•°å¤„ç†æ–‡ä»¶åè§£æ
                result = process_single_url(url, file_name, index, name)
                results.append(result)

        # æ‰¹é‡å¤„ç†æ‰€æœ‰æˆåŠŸä¸‹è½½çš„æ–‡ä»¶ï¼ˆåŒ…æ‹¬ä»ç¼“å­˜è·å–çš„æ–‡ä»¶ï¼‰
        successful_downloads = [r for r in results if r.get('status') == 'success' or r.get('from_cache')]
        if successful_downloads and name and userid:
            cached_count = sum(1 for r in successful_downloads if r.get('from_cache'))
            new_count = len(successful_downloads) - cached_count
            logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(successful_downloads)} ä¸ªæ–‡ä»¶ï¼ˆæ–°çˆ¬å–: {new_count}, ç¼“å­˜: {cached_count}ï¼‰")

            # æ”¶é›†æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯
            file_name_list = []
            file_path_list = []
            url_list = []

            for download in successful_downloads:
                file_name_list.append(download.get('file_name'))
                file_path_list.append(download.get('file_path'))
                url_list.append(download.get('url'))

            try:
                # è°ƒç”¨ disambiguation çš„ process_file å‡½æ•°è¿›è¡Œæ‰¹é‡å¤„ç†
                from query_neo4j.disambiguation import process_file
                batch_result = process_file(file_name_list, file_path_list, name, userid, private, url_list=url_list,
                                            only_name=only_name)

                if batch_result and batch_result.get('status') == 'success':
                    logger.info("æ‰¹é‡å¤„ç†æˆåŠŸ")
                    # æ›´æ–°ç»“æœçŠ¶æ€
                    for i, result in enumerate(results):
                        if result.get('status') == 'success' or result.get('from_cache'):
                            if result.get('from_cache'):
                                result['message'] = f"ä»ç¼“å­˜è·å–{result.get('file_type', '').upper()}æ–‡ä»¶å¹¶å¤„ç†æˆåŠŸ"
                            else:
                                result['message'] = f"{result.get('file_type', '').upper()}æ–‡ä»¶ä¸‹è½½å¹¶å¤„ç†æˆåŠŸ"
                            result['api_processed'] = True
                else:
                    logger.warning("æ‰¹é‡å¤„ç†å¤±è´¥æˆ–è¿”å›é”™è¯¯")
                    # æ›´æ–°ç»“æœçŠ¶æ€ä¸ºéƒ¨åˆ†æˆåŠŸ
                    for i, result in enumerate(results):
                        if result.get('status') == 'success' or result.get('from_cache'):
                            result['status'] = 'partial'
                            if result.get('from_cache'):
                                result[
                                    'message'] = f"ä»ç¼“å­˜è·å–{result.get('file_type', '').upper()}æ–‡ä»¶æˆåŠŸä½†APIå¤„ç†å¤±è´¥"
                            else:
                                result['message'] = f"{result.get('file_type', '').upper()}æ–‡ä»¶ä¸‹è½½æˆåŠŸä½†APIå¤„ç†å¤±è´¥"
                            result['api_processed'] = False

            except Exception as e:
                logger.error(f"æ‰¹é‡å¤„ç†å¼‚å¸¸: {str(e)}")
                # æ›´æ–°ç»“æœçŠ¶æ€ä¸ºéƒ¨åˆ†æˆåŠŸ
                for i, result in enumerate(results):
                    if result.get('status') == 'success' or result.get('from_cache'):
                        result['status'] = 'partial'
                        if result.get('from_cache'):
                            result[
                                'message'] = f"ä»ç¼“å­˜è·å–{result.get('file_type', '').upper()}æ–‡ä»¶æˆåŠŸä½†APIå¤„ç†å¼‚å¸¸: {str(e)}"
                        else:
                            result[
                                'message'] = f"{result.get('file_type', '').upper()}æ–‡ä»¶ä¸‹è½½æˆåŠŸä½†APIå¤„ç†å¼‚å¸¸: {str(e)}"
                        result['api_processed'] = False

        # æ·»åŠ æ— æ•ˆé¡¹åˆ°ç»“æœä¸­
        for invalid_item in invalid_items:
            results.append({
                "status": "error",
                "message": invalid_item["reason"],
                "item": invalid_item["item"]
            })

        # æ±‡æ€»ç»“æœ
        success_count = sum(1 for r in results if r.get('status') == 'success')
        partial_count = sum(1 for r in results if r.get('status') == 'partial')
        error_count = sum(1 for r in results if r.get('status') == 'error')
        cached_count = sum(1 for r in results if r.get('from_cache'))
        new_crawled_count = success_count + partial_count - cached_count

        return {
            "status": "completed",
            "message": f"çˆ¬å–å®Œæˆã€‚æˆåŠŸ: {success_count}, éƒ¨åˆ†æˆåŠŸ: {partial_count}, å¤±è´¥: {error_count}",
            "processing_mode": "å¤šçº¿ç¨‹" if use_multithreading and len(valid_urls) > 1 else "å•çº¿ç¨‹",
            "total_urls": len(url_list),
            "valid_urls": len(valid_urls),
            "invalid_urls": len(invalid_items),
            "cache_stats": {
                "cache_hits": cached_count,
                "new_crawled": new_crawled_count,
                "cache_enabled": cache_manager.is_connected
            },
            "details": results
        }

    except Exception as e:
        logger.error(f"æ‰§è¡Œçˆ¬å–ä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        # ç¡®ä¿å‡ºé”™æ—¶ä¹Ÿèƒ½å…³é—­æµè§ˆå™¨
        return {"status": "error", "message": f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"}