import os
import time
import chardet
from django.http import JsonResponse
from minio import Minio, InvalidResponseError, S3Error
import hashlib
from neo4j import GraphDatabase
# from bs4 import BeautifulSoup  # å·²æ›¿æ¢ä¸ºhtml.parser
from html.parser import HTMLParser
import html
import errno
import json
import datetime
import neo4j
import re
from random import uniform
from pdfminer.high_level import extract_text
import jieba
import jieba.analyse
from collections import Counter
import pymysql
import requests
from docx import Document
import redis
from query_neo4j.WSD import xiaoqi_instance, jiekou_3
import fitz  # PyMuPDF
import threading
import logging
from markitdown import MarkItDown
from concurrent.futures import ThreadPoolExecutor, as_completed

# è·å–Djangoé…ç½®çš„æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger('query_neo4j')

# Redisåˆ†å¸ƒå¼é”é…ç½® - å‚è€ƒupload.pyçš„é…ç½®
REDIS_HOST = '114.213.232.140'  # RedisæœåŠ¡å™¨åœ°å€
REDIS_PORT = 26379  # Redisç«¯å£
REDIS_DB = 0  # Redisæ•°æ®åº“ç¼–å·
REDIS_PASSWORD = None  # Rediså¯†ç 


class RedisDistributedLock:
    """Redisåˆ†å¸ƒå¼é”å®ç°"""

    def __init__(self, redis_client, key, timeout=30, retry_times=50, retry_delay=0.1):
        """
        åˆå§‹åŒ–åˆ†å¸ƒå¼é”
        :param redis_client: Rediså®¢æˆ·ç«¯
        :param key: é”çš„é”®å
        :param timeout: é”çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        :param retry_times: é‡è¯•æ¬¡æ•°
        :param retry_delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        """
        self.redis_client = redis_client
        self.key = f"distributed_lock:{key}"
        self.timeout = timeout
        self.retry_times = retry_times
        self.retry_delay = retry_delay
        self.identifier = None

    def acquire(self, blocking=True):
        """è·å–é”
        :param blocking: æ˜¯å¦é˜»å¡ç­‰å¾…ï¼ŒTrueä¸ºé˜»å¡ç›´åˆ°è·å–åˆ°é”ï¼ŒFalseä¸ºéé˜»å¡æ¨¡å¼
        """
        import uuid
        import time

        identifier = str(uuid.uuid4())

        if blocking:
            # é˜»å¡æ¨¡å¼ï¼šä½¿ç”¨RedisåŸç”Ÿçš„é˜»å¡æœºåˆ¶
            while True:
                # å°è¯•è·å–é”
                if self.redis_client.set(self.key, identifier, nx=True, ex=self.timeout):
                    self.identifier = identifier
                    return True

                # ä½¿ç”¨Redisçš„BLPOPå®ç°é˜»å¡ç­‰å¾…
                # åˆ›å»ºä¸€ä¸ªç­‰å¾…é˜Ÿåˆ—ï¼Œå½“é”é‡Šæ”¾æ—¶ä¼šæœ‰é€šçŸ¥
                wait_key = f"{self.key}:wait"
                try:
                    # BLPOPä¼šé˜»å¡ç­‰å¾…ï¼Œç›´åˆ°æœ‰å…ƒç´ æˆ–è¶…æ—¶
                    # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶(1ç§’)ä»¥ä¾¿å®šæœŸæ£€æŸ¥é”çŠ¶æ€
                    result = self.redis_client.blpop(wait_key, timeout=2)
                    # æ— è®ºæ˜¯å¦æœ‰é€šçŸ¥ï¼Œéƒ½å†æ¬¡å°è¯•è·å–é”
                    continue
                except Exception:
                    # å¦‚æœBLPOPå‡ºç°å¼‚å¸¸ï¼Œé™çº§ä¸ºçŸ­æš‚sleep
                    time.sleep(0.1)
                    continue
        else:
            # éé˜»å¡æ¨¡å¼ï¼šåŸæœ‰é€»è¾‘
            end_time = time.time() + self.timeout

            for _ in range(self.retry_times):
                if time.time() > end_time:
                    return False

                # å°è¯•è·å–é”
                if self.redis_client.set(self.key, identifier, nx=True, ex=self.timeout):
                    self.identifier = identifier
                    return True

                time.sleep(self.retry_delay)

            return False

    def release(self):
        """é‡Šæ”¾é”"""
        if not self.identifier:
            return False

        # ä½¿ç”¨Luaè„šæœ¬ç¡®ä¿åŸå­æ€§ï¼Œå¹¶åœ¨é‡Šæ”¾é”åé€šçŸ¥ç­‰å¾…çš„çº¿ç¨‹
        lua_script = """
        if redis.call('GET', KEYS[1]) == ARGV[1] then
            local result = redis.call('DEL', KEYS[1])
            if result == 1 then
                -- é”é‡Šæ”¾æˆåŠŸï¼Œé€šçŸ¥ç­‰å¾…çš„çº¿ç¨‹
                -- å‘ç­‰å¾…é˜Ÿåˆ—æ¨é€ä¸€ä¸ªé€šçŸ¥ä¿¡å·
                redis.call('LPUSH', KEYS[2], '1')
                -- è®¾ç½®é€šçŸ¥çš„è¿‡æœŸæ—¶é—´ï¼Œé¿å…ç´¯ç§¯è¿‡å¤šé€šçŸ¥
                redis.call('EXPIRE', KEYS[2], 5)
            end
            return result
        else
            return 0
        end
        """

        try:
            wait_key = f"{self.key}:wait"
            result = self.redis_client.eval(lua_script, 2, self.key, wait_key, self.identifier)
            return result == 1
        except Exception as e:
            logger.error(f"é‡Šæ”¾é”å¤±è´¥: {e}")
            return False

    def __enter__(self):
        """æ”¯æŒwithè¯­å¥ï¼Œé»˜è®¤ä½¿ç”¨é˜»å¡æ¨¡å¼"""
        self.acquire(blocking=True)  # é˜»å¡ç­‰å¾…è·å–é”
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """æ”¯æŒwithè¯­å¥"""
        self.release()


def get_redis_client():
    """è·å–Rediså®¢æˆ·ç«¯"""
    try:
        client = redis.Redis(
            host=REDIS_HOST,
            port=REDIS_PORT,
            db=REDIS_DB,
            password=REDIS_PASSWORD,
            decode_responses=True
        )
        # æµ‹è¯•è¿æ¥
        client.ping()
        return client
    except Exception as e:
        logger.error(f"Redisè¿æ¥å¤±è´¥: {e}")
        return None


def get_distributed_lock(entity, timeout=30):
    """è·å–åˆ†å¸ƒå¼é”"""
    redis_client = get_redis_client()
    if not redis_client:
        raise Exception("Redisè¿æ¥å¤±è´¥ï¼Œæ— æ³•åˆ›å»ºåˆ†å¸ƒå¼é”")

    return RedisDistributedLock(redis_client, f"xiaoqi:{entity}", timeout)


class MySQLDatabase:
    def __init__(self, host, user, password, database, charset="utf8mb4"):
        """
        åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        """
        self.config = {
            "host": host,
            "user": user,
            "password": password,
            "database": database,
            "charset": charset
        }
        self.connection = None

    def insert_data_without_primary(self, table_name, data):
        """ä¸åŒ…å«ä¸»é”®çš„æ•°æ®æ’å…¥ï¼ˆæ— äº‹åŠ¡æ§åˆ¶ç‰ˆæœ¬ï¼‰ï¼Œä¾èµ–å¤–å±‚äº‹åŠ¡ç®¡ç†"""
        try:
            with self.connection.cursor() as cursor:
                # æ„å»º WHERE æ¡ä»¶
                conditions = " AND ".join([f"{k} = %s" for k in data.keys()])
                check_query = f"SELECT COUNT(*) FROM {table_name} WHERE {conditions} FOR UPDATE"

                # æ‰§è¡Œæ£€æŸ¥å¹¶åŠ é”
                cursor.execute(check_query, tuple(data.values()))
                result = cursor.fetchone()

                if result[0] > 0:
                    logger.info("æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡æ’å…¥")
                    return False

                # æ‰§è¡Œæ’å…¥æ“ä½œ
                columns = ", ".join(data.keys())
                placeholders = ", ".join(["%s"] * len(data))
                insert_query = f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})"
                cursor.execute(insert_query, tuple(data.values()))
                self.connection.commit()
                return True

        except pymysql.MySQLError as e:
            logger.error(f"æ’å…¥æ“ä½œå¤±è´¥ï¼š{e}")

    def search_file(self, result, file_dict, file_dict_rev):

        # ä¼˜åŒ–æ•°æ®åº“è¿æ¥ï¼šå•æ¬¡è¿æ¥å¤„ç†æ‰€æœ‰æŸ¥è¯¢
        try:
            self.connect()
            with self.connection.cursor() as cursor:
                for entity in list(result.keys()):  # éå†å‰¯æœ¬é˜²æ­¢è¿­ä»£ä¿®æ”¹
                    file_ids = result[entity][1]
                    # åˆ›å»ºæ–°åˆ—è¡¨å­˜å‚¨æœ‰æ•ˆfile_id
                    valid_file_ids = []
                    for file_id in file_ids:
                        # ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢é˜²æ­¢SQLæ³¨å…¥ [[5]]
                        query = "SELECT id, path FROM file WHERE id = %s"
                        cursor.execute(query, (file_id,))
                        row = cursor.fetchone()  # å‡è®¾idå”¯ä¸€ï¼Œä½¿ç”¨fetchone()

                        if not row:  # æœªæŸ¥è¯¢åˆ°ç»“æœæ—¶è·³è¿‡
                            continue

                        current_path = row[1]
                        # æ£€æŸ¥è·¯å¾„æ˜¯å¦ä»¥HTTPå¼€å¤´ [[7]]
                        if current_path.lower().startswith(('http://', 'https://')):
                            continue  # è·³è¿‡HTTPè·¯å¾„

                        # ä¿ç•™æœ‰æ•ˆfile_idå’Œè·¯å¾„
                        valid_file_ids.append(file_id)
                        file_dict[int(file_id)] = current_path  # æ›´æ–°è·¯å¾„
                        file_dict_rev[current_path] = int(file_id)
                    # æ›´æ–°resultä¸­çš„æœ‰æ•ˆfile_idsåˆ—è¡¨
                    if valid_file_ids:
                        result[entity][1] = valid_file_ids
        except Exception as e:
            logger.error(f"Error occurred: {e}")

        return file_dict

    def get_dir_private_list(self, entity_id, userid):
        """
        æŸ¥è¯¢dir_entityï¼Œå¦‚æœæ²¡ç»“æœåˆ™æŸ¥è¯¢dir_to_entity -> directoryï¼Œå¹¶å†™å…¥æ–°å†…å®¹åè¿”å›thirdåˆ—è¡¨ï¼ˆæœ‰äº‹åŠ¡æ§åˆ¶ç‰ˆæœ¬ï¼‰
        """
        # å¼€å§‹äº‹åŠ¡
        connection = self.connection
        try:
            # å¼€å§‹äº‹åŠ¡
            connection.begin()

            with connection.cursor() as cursor:
                # å…ˆæŸ¥ dir_entity
                sql1 = """
                       SELECT dir_private
                       FROM dir_entity
                       WHERE entity_id = %s \
                         AND userid = %s \
                       """
                cursor.execute(sql1, (entity_id, userid))
                result = cursor.fetchall()
                if result:
                    # å¦‚æœæŸ¥åˆ°ç»“æœï¼Œæäº¤äº‹åŠ¡å¹¶è¿”å›
                    connection.commit()
                    return [row[0] for row in result]

                # æŸ¥ä¸åˆ° -> æŸ¥ dir_to_entity è·å– second
                sql2 = "SELECT second FROM dir_to_entity WHERE entity_id = %s"
                cursor.execute(sql2, (entity_id,))
                second_result = cursor.fetchone()
                if not second_result:
                    connection.commit()
                    return []  # å¦‚æœè¿ second éƒ½æŸ¥ä¸åˆ°ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨

                second = second_result[0]

                # æŸ¥ directory è¡¨è·å– third
                sql3 = "SELECT third FROM directory WHERE second = %s"
                cursor.execute(sql3, (second,))
                third_results = cursor.fetchall()
                third_list = [row[0] for row in third_results]

                # æ‰¹é‡æ’å…¥åˆ° dir_entity è¡¨
                insert_sql = """
                             INSERT INTO dir_entity (entity_id, dir_private, dir_sys, userid)
                             VALUES (%s, %s, %s, %s) \
                             """
                for third in third_list:
                    cursor.execute(insert_sql, (entity_id, third, third, userid))

            # æ‰€æœ‰æ“ä½œæˆåŠŸï¼Œæäº¤äº‹åŠ¡
            connection.commit()
            return third_list

        except Exception as e:
            # å‘ç”Ÿé”™è¯¯ï¼Œå›æ»šäº‹åŠ¡
            connection.rollback()
            logger.error(f"è·å–ç›®å½•ç§æœ‰åˆ—è¡¨å¤±è´¥ï¼Œäº‹åŠ¡å·²å›æ»šï¼š{e}")
            raise

    def get_dir_private_list_simple(self, entity_id, userid):
        """
        æŸ¥è¯¢dir_entityï¼Œå¦‚æœæ²¡ç»“æœåˆ™æŸ¥è¯¢dir_to_entity -> directoryï¼Œå¹¶å†™å…¥æ–°å†…å®¹åè¿”å›thirdåˆ—è¡¨ï¼ˆæ— äº‹åŠ¡æ§åˆ¶ç‰ˆæœ¬ï¼‰
        """
        try:
            with self.connection.cursor() as cursor:
                # å…ˆæŸ¥ dir_entity
                sql1 = """
                       SELECT dir_private
                       FROM dir_entity
                       WHERE entity_id = %s \
                         AND userid = %s \
                       """
                cursor.execute(sql1, (entity_id, userid))
                result = cursor.fetchall()
                if result:
                    # å¦‚æœæŸ¥åˆ°ç»“æœï¼Œç›´æ¥è¿”å›
                    return [row[0] for row in result]

                # æŸ¥ä¸åˆ° -> æŸ¥ dir_to_entity è·å– second
                sql2 = "SELECT second FROM dir_to_entity WHERE entity_id = %s"
                cursor.execute(sql2, (entity_id,))
                second_result = cursor.fetchone()
                if not second_result:
                    return []  # å¦‚æœè¿ second éƒ½æŸ¥ä¸åˆ°ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨

                second = second_result[0]

                # æŸ¥ directory è¡¨è·å– third
                sql3 = "SELECT third FROM directory WHERE second = %s"
                cursor.execute(sql3, (second,))
                third_results = cursor.fetchall()
                third_list = [row[0] for row in third_results]

                # æ‰¹é‡æ’å…¥åˆ° dir_entity è¡¨
                insert_sql = """
                             INSERT INTO dir_entity (entity_id, dir_private, dir_sys, userid)
                             VALUES (%s, %s, %s, %s) \
                             """
                for third in third_list:
                    cursor.execute(insert_sql, (entity_id, third, third, userid))
            self.connection.commit()
            return third_list

        except Exception as e:
            logger.error(f"è·å–ç›®å½•ç§æœ‰åˆ—è¡¨å¤±è´¥ï¼š{e}")
            raise e  # æŠ›å‡ºå¼‚å¸¸è®©å¤–å±‚äº‹åŠ¡å¤„ç†

    def insert_classification_and_entity_data(self, classification_data, entity_id, file_dict_rev=None, userid=None):
        """
        åˆå¹¶çš„å‡½æ•°ï¼šæ’å…¥åˆ†ç±»ç»“æœå’Œç›®å½•åˆ°å®ä½“å…³è”
        å‚æ•°:
        - classification_data: APIè¿”å›çš„åˆ†ç±»ç»“æœæ•°æ®
        - entity_id: å®ä½“ID
        - file_dict_rev: æ–‡ä»¶å­—å…¸åå‘æ˜ å°„ï¼ˆå¯é€‰ï¼Œç”¨äºåˆ†ç±»ç»“æœæ’å…¥ï¼‰
        - userid: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼Œç”¨äºåˆ†ç±»ç»“æœæ’å…¥ï¼‰
        """
        # æ‰‹åŠ¨ç®¡ç†Redisåˆ†å¸ƒå¼é”
        lock = get_distributed_lock(f"classification_entity:{entity_id}", timeout=30)

        try:
            # è·å–åˆ†å¸ƒå¼é” - é˜»å¡æ¨¡å¼ï¼Œä½¿ç”¨RedisåŸç”ŸBLPOPç­‰å¾…
            logger.info(f"â³ çº¿ç¨‹ {threading.current_thread().name} æ­£åœ¨ç­‰å¾…è·å–åˆ†ç±»æ’å…¥åˆ†å¸ƒå¼é”: entity_id={entity_id}")
            lock.acquire(blocking=True)  # ä½¿ç”¨RedisåŸç”Ÿé˜»å¡æœºåˆ¶
            logger.info(f"ğŸ”’ çº¿ç¨‹ {threading.current_thread().name} æˆåŠŸè·å–åˆ†ç±»æ’å…¥åˆ†å¸ƒå¼é”: entity_id={entity_id}")

            try:
                data = classification_data['data']
                files = data.get('files', {})
                label_1 = data.get('label_1', '')
                label_2 = data.get('label_2', '')

                # ç¬¬ä¸€éƒ¨åˆ†ï¼šæ’å…¥ç›®å½•åˆ°å®ä½“å…³è”ï¼ˆåŸ insert_dir_toentity çš„åŠŸèƒ½ï¼‰
                dir_entity_data = {
                    'second': label_2,
                    'entity_id': entity_id,
                }
                self.insert_data_without_primary('dir_to_entity', dir_entity_data)
                logger.info(f"æˆåŠŸæ’å…¥ç›®å½•åˆ°å®ä½“å…³è”: entity_id={entity_id}, label_2={label_2}")

                # ç¬¬äºŒéƒ¨åˆ†ï¼šæ’å…¥åˆ†ç±»ç»“æœï¼ˆåŸ insert_classification_result çš„åŠŸèƒ½ï¼‰
                results = []
                if file_dict_rev is not None and userid is not None and files:
                    # ä¸ºæ¯ä¸ªæ–‡ä»¶æ’å…¥è®°å½•
                    for filename, file_category in files.items():
                        # è·å–ç›®å½•IDï¼ˆæ— äº‹åŠ¡ç‰ˆæœ¬ï¼‰
                        directory_id = self.get_directory_simple(label_2, file_category)

                        # æ’å…¥åˆ° dir_to_file è¡¨
                        file_data = {
                            'id': directory_id,
                            'fileid': file_dict_rev[f"bb/{filename}"],
                        }
                        self.insert_data_without_primary('dir_to_file', file_data)

                        # è·å–ç§æœ‰ç›®å½•åˆ—è¡¨ï¼ˆæ— äº‹åŠ¡ç‰ˆæœ¬ï¼‰
                        self.get_dir_private_list_simple(entity_id, userid)

                        # è·å–æ–°ç›®å½•ID
                        pri_dir_id = self.get_new_directory(entity_id, userid, file_category)

                        # æ’å…¥åˆ° dir_file è¡¨
                        dir_file_data = {
                            'dir_id': pri_dir_id[0],
                            'file_id': file_dict_rev[f"bb/{filename}"],
                        }
                        self.insert_data_without_primary("dir_file", dir_file_data)

                    logger.info(f"æˆåŠŸæ’å…¥åˆ†ç±»ç»“æœ: entity_id={entity_id}, æ–‡ä»¶æ•°é‡={len(files)}")

                logger.info(f"æ‰€æœ‰æ•°æ®æ’å…¥æˆåŠŸ: entity_id={entity_id}")
                logger.info(f"âœ… åˆ†ç±»æ’å…¥æ“ä½œå®Œæˆ (é”å°†é‡Šæ”¾): entity_id={entity_id}")
                return True

            except Exception as inner_e:
                error_msg = f"åˆ†ç±»æ•°æ®æ’å…¥å¤±è´¥ - entity_id: {entity_id}, é”™è¯¯: {str(inner_e)}"
                logger.error(f"âŒ {error_msg}")
                # å‘ç”Ÿå¼‚å¸¸æ—¶å°è¯•é‡Šæ”¾é”
                try:
                    lock.release()
                    logger.info(
                        f"ğŸ”“ çº¿ç¨‹ {threading.current_thread().name} å¼‚å¸¸é‡Šæ”¾åˆ†ç±»æ’å…¥åˆ†å¸ƒå¼é”: entity_id={entity_id}")
                except Exception as release_error:
                    logger.error(f"âŒ å¼‚å¸¸å¤„ç†ä¸­é‡Šæ”¾åˆ†å¸ƒå¼é”å¤±è´¥ - entity_id: {entity_id}, é”™è¯¯: {str(release_error)}")
                raise  # é‡æ–°æŠ›å‡ºåŸå§‹å¼‚å¸¸

        except Exception as outer_e:
            error_msg = str(outer_e)
            detailed_msg = f"åˆ†ç±»æ’å…¥åˆ†å¸ƒå¼é”æ“ä½œå¼‚å¸¸: {error_msg}"
            logger.error(f"âŒ {detailed_msg}")

            # å¦‚æœæ˜¯Redisè¿æ¥é—®é¢˜ï¼Œæä¾›æ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
            if "Redis" in error_msg or "Connection" in error_msg:
                detailed_msg += " (RedisæœåŠ¡å™¨å¯èƒ½æœªå¯åŠ¨æˆ–è¿æ¥é…ç½®æœ‰é—®é¢˜)"
                logger.warning(f"ğŸ’¡ RedisæœåŠ¡å™¨å¯èƒ½æœªå¯åŠ¨æˆ–è¿æ¥é…ç½®æœ‰é—®é¢˜")

            # ç¡®ä¿é”è¢«é‡Šæ”¾
            try:
                lock.release()
                logger.info(
                    f"ğŸ”“ çº¿ç¨‹ {threading.current_thread().name} å¤–å±‚å¼‚å¸¸é‡Šæ”¾åˆ†ç±»æ’å…¥åˆ†å¸ƒå¼é”: entity_id={entity_id}")
            except Exception as release_error:
                logger.error(f"âŒ å¤–å±‚å¼‚å¸¸å¤„ç†ä¸­é‡Šæ”¾åˆ†å¸ƒå¼é”å¤±è´¥ - entity_id: {entity_id}, é”™è¯¯: {str(release_error)}")

            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸

        finally:
            # æœ€ç»ˆé‡Šæ”¾é”
            try:
                lock.release()
                logger.info(f"ğŸ”“ çº¿ç¨‹ {threading.current_thread().name} æœ€ç»ˆé‡Šæ”¾åˆ†ç±»æ’å…¥åˆ†å¸ƒå¼é”: entity_id={entity_id}")
            except Exception as release_error:
                logger.error(f"âŒ æœ€ç»ˆé‡Šæ”¾åˆ†å¸ƒå¼é”å¤±è´¥ - entity_id: {entity_id}, é”™è¯¯: {str(release_error)}")
                # åœ¨finallyå—ä¸­æ— æ³•é€šè¿‡returnåœæ­¢ï¼Œä½†å¯ä»¥æŠ›å‡ºå¼‚å¸¸
                raise Exception(f"æœ€ç»ˆé‡Šæ”¾åˆ†å¸ƒå¼é”å¤±è´¥ - entity_id: {entity_id}, é”™è¯¯: {str(release_error)}")

    def upload_direct(self, xiaoqi, second_classify):
        # éªŒè¯è¾“å…¥æ•°æ®æœ‰æ•ˆæ€§
        if not second_classify.get('data', {}).get('entity_path'):
            logger.warning(f"Invalid input data: {second_classify}")
            return
        original_path = second_classify['data']['entity_path']

        # ä½¿ç”¨Redisåˆ†å¸ƒå¼é”ï¼Œç¡®ä¿è·¨è¿›ç¨‹/è·¨è¯·æ±‚çš„äº’æ–¥è®¿é—®
        try:
            with get_distributed_lock(xiaoqi, timeout=60) as lock:
                logger.info(f"ğŸ”’ è·å–åˆ°åˆ†å¸ƒå¼é”: {xiaoqi}")

                # è·¯å¾„è½¬æ¢é€»è¾‘
                parts = original_path.strip('\\').split('\\')
                processed_parts = ['KOç›®å½•'] + [p for p in parts]
                new_entity_path = ['->'.join(processed_parts)]

                # æ•°æ®åº“æ“ä½œ - æ·»åŠ äº‹åŠ¡æ”¯æŒ
                with self.connection.cursor() as cursor:
                    try:
                        # å¼€å§‹äº‹åŠ¡
                        self.connection.begin()

                        # æ£€æŸ¥ç›®å½•çŠ¶æ€ï¼ˆæ³¨æ„è¡¨åä¿æŒä¸€è‡´æ€§ï¼‰
                        query_sql = "SELECT directory FROM xiaoqi_new WHERE xiaoqi_name = %s"
                        cursor.execute(query_sql, (xiaoqi,))
                        result = cursor.fetchone()

                        if result is None or result[0] is None:
                            # ä½¿ç”¨JSONåºåˆ—åŒ–æ›¿ä»£å­—ç¬¦ä¸²å¼ºè½¬ï¼ˆæ›´è§„èŒƒçš„å­˜å‚¨æ–¹å¼ï¼‰
                            directory_json = json.dumps(new_entity_path, ensure_ascii=False)
                            update_sql = "UPDATE xiaoqi_new SET directory = %s WHERE xiaoqi_name = %s"

                            cursor.execute(update_sql, (directory_json, xiaoqi))

                            # æ£€æŸ¥æ˜¯å¦å®é™…æ›´æ–°äº†æ•°æ®
                            if cursor.rowcount > 0:
                                logger.info(f"âœ… æˆåŠŸæ›´æ–°ç›®å½•: {xiaoqi}")
                            else:
                                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å¯¹åº”çš„xiaoqiè®°å½•: {xiaoqi}")
                        else:
                            logger.info(f"âš ï¸ è·³è¿‡æ›´æ–°: {xiaoqi} å·²å­˜åœ¨ç›®å½•")

                        # æäº¤äº‹åŠ¡
                        self.connection.commit()

                    except Exception as e:
                        # å‘ç”Ÿå¼‚å¸¸æ—¶å›æ»šäº‹åŠ¡
                        self.connection.rollback()
                        logger.error(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥ï¼Œå·²å›æ»š: {str(e)}")
                        raise

                logger.info(f"ğŸ”“ é‡Šæ”¾åˆ†å¸ƒå¼é”: {xiaoqi}")

        except Exception as e:
            logger.error(f"âŒ åˆ†å¸ƒå¼é”æ“ä½œæˆ–å¤„ç†å¼‚å¸¸: {str(e)}")
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸è®©è°ƒç”¨æ–¹å¤„ç†

    def get_new_directory(self, entity_id, userid, label_2):
        """
        æ ¹æ®entity_idå’ŒuseridæŸ¥è¯¢dir_privateå­—æ®µï¼Œå¹¶ä»¥åˆ—è¡¨å½¢å¼è¿”å›
        """
        with self.connection.cursor() as cursor:
            sql = """
                  SELECT id
                  FROM dir_entity
                  WHERE entity_id = %s \
                    AND userid = %s \
                    AND dir_private = %s \
                  """
            cursor.execute(sql, (entity_id, userid, label_2))
            result = cursor.fetchall()
            return result[0]

    def get_directory(self, label_1, label_2):
        """
        æ’å…¥æˆ–è·å–ç›®å½•IDï¼ˆæœ‰äº‹åŠ¡æ§åˆ¶ç‰ˆæœ¬ï¼‰
        """
        try:
            # æ£€æŸ¥ç›®å½•æ˜¯å¦å·²å­˜åœ¨
            query = "SELECT id FROM directory WHERE second = %s AND third = %s"
            with self.connection.cursor() as cursor:
                cursor.execute(query, (label_1, label_2))
                result = cursor.fetchone()

                if result:
                    return result[0]  # è¿”å›å·²å­˜åœ¨çš„ç›®å½•ID
                else:
                    return -1
        except pymysql.MySQLError as e:
            logger.error(f"ç›®å½•æ’å…¥æˆ–æŸ¥è¯¢å¤±è´¥: {e}")
            self.connection.rollback()
            return None

    def get_directory_simple(self, label_1, label_2):
        """
        æ’å…¥æˆ–è·å–ç›®å½•IDï¼ˆæ— äº‹åŠ¡æ§åˆ¶ç‰ˆæœ¬ï¼‰
        """
        try:
            # æ£€æŸ¥ç›®å½•æ˜¯å¦å·²å­˜åœ¨
            query = "SELECT id FROM directory WHERE second = %s AND third = %s"
            with self.connection.cursor() as cursor:
                cursor.execute(query, (label_1, label_2))
                result = cursor.fetchone()

                if result:
                    return result[0]  # è¿”å›å·²å­˜åœ¨çš„ç›®å½•ID
                else:
                    return -1
        except pymysql.MySQLError as e:
            logger.error(f"ç›®å½•æ’å…¥æˆ–æŸ¥è¯¢å¤±è´¥: {e}")
            raise e  # æŠ›å‡ºå¼‚å¸¸è®©å¤–å±‚äº‹åŠ¡å¤„ç†

    def query_dir_by_name_id(self, name_id):
        """
        æ ¹æ®ç›®å½•åç§°å’ŒIDæŸ¥è¯¢ç›®å½•
        å‚æ•°:
            name_id: ç›®å½•åç§°å’ŒIDï¼Œæ ¼å¼ä¸º"åç§°:ID"
        è¿”å›:
            ç›®å½•ä¿¡æ¯ï¼Œæ ¼å¼ä¸º{'id': ID, 'name': åç§°}ï¼Œå¦‚æœæŸ¥è¯¢å¤±è´¥åˆ™è¿”å›None
        """
        try:
            with self.connection.cursor() as cursor:
                # æ‰§è¡ŒæŸ¥è¯¢
                sql = "SELECT second  FROM dir_to_entity WHERE entity_id = %s"
                cursor.execute(sql, (name_id,))
                for second in cursor.fetchall():
                    return second[0]
        except pymysql.MySQLError as e:
            logger.error(f"æŸ¥è¯¢å¤±è´¥ï¼š{e}")
            return ""

    def query_entities_by_name(self, name, exact_match=False):
        """
        æŸ¥è¯¢å®ä½“å¹¶æ ¼å¼åŒ–æˆç›®æ ‡ç»“æ„
        å‚æ•°:
            name: è¦æŸ¥è¯¢çš„åç§°
            exact_match: æ˜¯å¦ç²¾ç¡®åŒ¹é…ï¼ŒFalseä¸ºæ¨¡ç³ŠåŒ¹é…ï¼ˆé»˜è®¤ï¼‰ï¼ŒTrueä¸ºç²¾ç¡®åŒ¹é…
        è¿”å›:
            {
                "æ±ªèŒ1": ["æ±ªèŒ", "æ•™æˆ", ...],
                "æ±ªèŒ2": ["é‡åº†å¤§å­¦"],
                ...
            }
        """
        entity_list = {}
        entity_dict = {}
        try:
            self.connect()
            with self.connection.cursor() as cursor:
                # æ‰§è¡Œæ¨¡ç³ŠæŸ¥è¯¢ - æ”¯æŒå­—ç¬¦+ä»»æ„å­—ç¬¦çš„åŒ¹é…
                sql = """SELECT xiaoqi_name, key_words, xiaoqi_id
                         FROM xiaoqi_new
                         WHERE xiaoqi_name LIKE %s"""
                # æ·»åŠ è°ƒè¯•æ—¥å¿—
                search_pattern = f"{name}%"
                logger.debug(f"ğŸ” æŸ¥è¯¢SQL: {sql}")
                logger.debug(f"ğŸ” æŸ¥è¯¢å‚æ•°: {search_pattern}")
                cursor.execute(sql, (search_pattern,))  # çœŸæ­£æ·»åŠ %é€šé…ç¬¦ï¼ŒåŒ¹é…ä»¥nameå¼€å¤´çš„ä»»æ„å­—ç¬¦

                # å¤„ç†æŸ¥è¯¢ç»“æœ
                results = cursor.fetchall()
                logger.debug(f"ğŸ” æŸ¥è¯¢ç»“æœæ•°é‡: {len(results)}")
                for xiaoqi_name, keywords, xiaoqi_id in results:
                    logger.debug(f"ğŸ” æ‰¾åˆ°è®°å½•: xiaoqi_name='{xiaoqi_name}', xiaoqi_id={xiaoqi_id}")

                    # æ·»åŠ JSONè§£æçš„é”™è¯¯å¤„ç†
                    if keywords is None or keywords.strip() == "":
                        keyword_list = [xiaoqi_name]
                    else:
                        try:
                            keywords = keywords.replace("'", '"').replace("'", '"').replace("'", '"')
                            keyword_list = json.loads(keywords)
                        except json.JSONDecodeError as e:
                            logger.warning(
                                f"âš ï¸ JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: xiaoqi_name='{xiaoqi_name}', keywords='{keywords}', é”™è¯¯: {e}")
                            keyword_list = [xiaoqi_name]

                    entity_list[xiaoqi_name] = keyword_list
                    entity_dict[xiaoqi_name] = xiaoqi_id
        except pymysql.MySQLError as e:
            logger.error(f"æŸ¥è¯¢å¤±è´¥ï¼š{e}")
            return {}
        finally:
            self.close()
        return entity_list, entity_dict

    def connect(self):
        """
        å»ºç«‹æ•°æ®åº“è¿æ¥
        """
        try:
            self.connection = pymysql.connect(**self.config)
            # ç¡®ä¿å¼€å¯è‡ªåŠ¨æäº¤æ¨¡å¼
            self.connection.autocommit(True)
            logger.info(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸï¼Œå·²å¼€å¯autocommit")
        except pymysql.MySQLError as e:
            logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥ï¼š{e}")
            raise

    def insert_xiaoqi_new(self, table_name, data, primary_key='xiaoqi_name'):
        # è·å–ä¸»é”®å€¼
        primary_key_value = data.get(primary_key)

        with self.connection.cursor() as cursor:
            # æ£€æŸ¥è®°å½•æ˜¯å¦å·²å­˜åœ¨
            check_query = f"SELECT {primary_key}, xiaoqi_id, key_words FROM {table_name} WHERE {primary_key} = %s FOR UPDATE"
            cursor.execute(check_query, (primary_key_value,))
            existing_record = cursor.fetchone()

            if existing_record:
                # è®°å½•å­˜åœ¨
                existing_name, existing_id, existing_keywords = existing_record

                if existing_keywords is not None and str(existing_keywords).strip() != "":
                    # è®°å½•å­˜åœ¨ä¸” key_words ä¸ä¸ºç©ºï¼Œè¿”å›ç°æœ‰ID
                    logger.info(f"ä¸»é”® {primary_key_value} å·²å­˜åœ¨ä¸”key_wordsä¸ä¸ºç©ºï¼Œè¿”å›ç°æœ‰ID: {existing_id}")
                    return existing_id
                else:
                    # è®°å½•å­˜åœ¨ä½† key_words ä¸ºç©ºï¼Œæ‰§è¡Œæ›´æ–°æ“ä½œ
                    logger.info(f"ä¸»é”® {primary_key_value} å·²å­˜åœ¨ä½†key_wordsä¸ºç©ºï¼Œæ‰§è¡Œæ›´æ–°æ“ä½œ")

                    # æ„å»ºæ›´æ–°è¯­å¥ï¼Œåªæ›´æ–°éä¸»é”®å­—æ®µ
                    update_fields = []
                    update_values = []
                    for key, value in data.items():
                        if key != primary_key:  # æ’é™¤ä¸»é”®å­—æ®µ
                            update_fields.append(f"{key} = %s")
                            update_values.append(value)

                    if update_fields:
                        update_query = f"UPDATE {table_name} SET {', '.join(update_fields)} WHERE {primary_key} = %s"
                        update_values.append(primary_key_value)
                        cursor.execute(update_query, tuple(update_values))
                        self.connection.commit()
                        logger.info(f"âœ… æˆåŠŸæ›´æ–°xiaoqi: {primary_key_value}")
                        logger.info(f"ğŸ”¥ insert_xiaoqi_new æ›´æ–°æäº¤æˆåŠŸ: {primary_key_value}")

                    return existing_id
            else:
                # è®°å½•ä¸å­˜åœ¨ï¼Œæ’å…¥æ–°æ•°æ®
                logger.info(f"ä¸»é”® {primary_key_value} ä¸å­˜åœ¨ï¼Œæ‰§è¡Œæ’å…¥æ“ä½œ")
                columns = ", ".join(data.keys())
                placeholders = ", ".join(["%s"] * len(data))
                insert_query = f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})"
                cursor.execute(insert_query, tuple(data.values()))
                self.connection.commit()
                logger.info(f"âœ… æˆåŠŸæ’å…¥xiaoqi: {primary_key_value}")
                logger.info(f"ğŸ”¥ insert_xiaoqi_new æ’å…¥æäº¤æˆåŠŸ: {primary_key_value}")
                return cursor.lastrowid

    def insert_data(self, table_name, data):
        try:
            # å…ˆæ£€æŸ¥ä¸»é”®æ˜¯å¦å­˜åœ¨
            primary_key = list(data.keys())[0]  # å‡è®¾ä¸»é”®åœ¨ç¬¬ä¸€ä¸ªä½ç½®
            primary_key_value = data[primary_key]

            # ç”Ÿæˆæ£€æŸ¥ä¸»é”®æ˜¯å¦å­˜åœ¨çš„ SQL æŸ¥è¯¢
            check_query = f"SELECT COUNT(*) FROM {table_name} WHERE {primary_key} = %s"
            with self.connection.cursor() as cursor:
                cursor.execute(check_query, (primary_key_value,))
                result = cursor.fetchone()

                if result[0] > 0:
                    return  # ä¸»é”®å·²å­˜åœ¨ï¼Œè·³è¿‡æ’å…¥æ“ä½œ

            # ç”Ÿæˆæ’å…¥ SQL è¯­å¥
            columns = ", ".join(data.keys())
            placeholders = ", ".join(["%s"] * len(data))
            insert_query = f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})"

            # æ‰§è¡Œæ’å…¥æ“ä½œ
            with self.connection.cursor() as cursor:
                cursor.execute(insert_query, tuple(data.values()))
        except pymysql.MySQLError as e:
            logger.error(f"æ’å…¥æ•°æ®å¤±è´¥ï¼š{e}")

    def insert_relation(self, table_name, data):
        try:
            # å…ˆæ£€æŸ¥ä¸»é”®æ˜¯å¦å­˜åœ¨
            primary_key = list(data.keys())[0]  # å‡è®¾ä¸»é”®åœ¨ç¬¬ä¸€ä¸ªä½ç½®
            last_key = list(data.keys())[-1]
            primary_key_value = data[primary_key]
            last_key_value = data[last_key]

            # ç”Ÿæˆæ£€æŸ¥ä¸»é”®æ˜¯å¦å­˜åœ¨çš„ SQL æŸ¥è¯¢
            check_query = f"SELECT COUNT(*) FROM {table_name} WHERE {primary_key} = %s AND {last_key} = %s "
            with self.connection.cursor() as cursor:
                cursor.execute(check_query, (primary_key_value, last_key_value,))
                result = cursor.fetchone()

                if result[0] > 0:
                    return  # ä¸»é”®å·²å­˜åœ¨ï¼Œè·³è¿‡æ’å…¥æ“ä½œ

            # ç”Ÿæˆæ’å…¥ SQL è¯­å¥
            columns = ", ".join(data.keys())
            placeholders = ", ".join(["%s"] * len(data))
            insert_query = f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})"

            # æ‰§è¡Œæ’å…¥æ“ä½œ
            with self.connection.cursor() as cursor:
                cursor.execute(insert_query, tuple(data.values()))
        except pymysql.MySQLError as e:
            logger.error(f"æ’å…¥æ•°æ®å¤±è´¥ï¼š{e}")

    def close(self):
        """
        å…³é—­æ•°æ®åº“è¿æ¥
        """
        if self.connection:
            self.connection.close()
            logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­ï¼")


class Bucket:

    def __init__(self, minio_address, minio_admin, minio_password):
        # é€šè¿‡ip è´¦å· å¯†ç  è¿æ¥minio server
        # Httpè¿æ¥ å°†secureè®¾ç½®ä¸ºFalse
        self.minioClient = Minio(endpoint=minio_address,
                                 access_key=minio_admin,
                                 secret_key=minio_password,
                                 secure=False)

    def create_one_bucket(self, bucket_name):
        # åˆ›å»ºæ¡¶(è°ƒç”¨make_bucket apiæ¥åˆ›å»ºä¸€ä¸ªæ¡¶)
        """
        æ¡¶å‘½åè§„åˆ™ï¼šå°å†™å­—æ¯ï¼Œå¥ç‚¹ï¼Œè¿å­—ç¬¦å’Œæ•°å­— å…è®¸ä½¿ç”¨ é•¿åº¦è‡³å°‘3ä¸ªå­—ç¬¦
        ä½¿ç”¨å¤§å†™å­—æ¯ã€ä¸‹åˆ’çº¿ç­‰ä¼šæŠ¥é”™
        """
        try:
            # bucket_existsï¼šæ£€æŸ¥æ¡¶æ˜¯å¦å­˜åœ¨
            if not self.minioClient.bucket_exists(bucket_name=bucket_name):
                self.minioClient.make_bucket(bucket_name=bucket_name)
        except InvalidResponseError as err:
            logger.error(f"åˆ›å»ºæ¡¶å¤±è´¥: {err}")

    def remove_one_bucket(self, bucket_name):
        # åˆ é™¤æ¡¶(è°ƒç”¨remove_bucket apiæ¥åˆ›å»ºä¸€ä¸ªå­˜å‚¨æ¡¶)
        try:
            if self.minioClient.bucket_exists(bucket_name=bucket_name):
                self.minioClient.remove_bucket(bucket_name)
            else:
                logger.warning("è¯¥å­˜å‚¨æ¡¶ä¸å­˜åœ¨")
        except InvalidResponseError as err:
            logger.error(f"åˆ é™¤æ¡¶å¤±è´¥: {err}")

    def upload_stream_tobucket(self, bucket_name, upload_file):
        object_name = upload_file.name  # æˆ–è€…è‡ªå®šä¹‰å¸¦å‰ç¼€çš„è·¯å¾„
        try:
            minio_client.put_object(
                bucket_name=bucket_name,
                object_name=object_name,
                data=upload_file,  # upload_file æœ¬èº«æ”¯æŒ .read()
                length=upload_file.size,  # è¯·æ±‚å¯¹è±¡çš„æ€»å¤§å°
                content_type=upload_file.content_type  # å¯é€‰ï¼Œæ¨èæä¾›
            )
        except S3Error as err:
            return {"error": f"ä¸Šä¼ å¤±è´¥ï¼š{err}"}

    def upload_file_to_bucket(self, bucket_name, file_name, file_path):
        """
        å°†æ–‡ä»¶ä¸Šä¼ åˆ°bucket
        :param bucket_name: minioæ¡¶åç§°
        :param file_name: å­˜æ”¾åˆ°minioæ¡¶ä¸­çš„æ–‡ä»¶åå­—(ç›¸å½“äºå¯¹æ–‡ä»¶è¿›è¡Œäº†é‡å‘½åï¼Œå¯ä»¥ä¸åŸæ–‡ä»¶åä¸åŒ)
                            file_nameå¤„å¯ä»¥åˆ›å»ºæ–°çš„ç›®å½•(æ–‡ä»¶å¤¹) ä¾‹å¦‚ /example/file_name
                            ç›¸å½“äºåœ¨è¯¥æ¡¶ä¸­æ–°å»ºäº†ä¸€ä¸ªexampleæ–‡ä»¶å¤¹ å¹¶æŠŠæ–‡ä»¶æ”¾åœ¨å…¶ä¸­
        :param file_path: æœ¬åœ°æ–‡ä»¶çš„è·¯å¾„
        """
        # æ¡¶æ˜¯å¦å­˜åœ¨ ä¸å­˜åœ¨åˆ™æ–°å»º
        check_bucket = self.minioClient.bucket_exists(bucket_name)
        if not check_bucket:
            self.minioClient.make_bucket(bucket_name)

        try:
            self.minioClient.fput_object(bucket_name=bucket_name,
                                         object_name=file_name,
                                         file_path=file_path)
        except FileNotFoundError as err:
            logger.error(f'upload_failed: {str(err)}')
        except S3Error as err:
            logger.error(f"upload_failed: {err}")

    def download_file_from_bucket(self, bucket_name, minio_file_path, download_file_path):
        """
        ä»bucketä¸‹è½½æ–‡ä»¶
        :param bucket_name: minioæ¡¶åç§°
        :param minio_file_path: å­˜æ”¾åœ¨minioæ¡¶ä¸­æ–‡ä»¶åå­—
                            file_nameå¤„å¯ä»¥åŒ…å«ç›®å½•(æ–‡ä»¶å¤¹) ä¾‹å¦‚ /example/file_name
        :param download_file_path: æ–‡ä»¶è·å–åå­˜æ”¾çš„è·¯å¾„
        """
        # æ¡¶æ˜¯å¦å­˜åœ¨
        check_bucket = self.minioClient.bucket_exists(bucket_name)
        if check_bucket:
            try:
                self.minioClient.fget_object(bucket_name=bucket_name,
                                             object_name=minio_file_path,
                                             file_path=download_file_path)
                return 1
            except FileNotFoundError as err:
                logger.error(f'download_failed: {str(err)}')
                return 0
            except S3Error as err:
                logger.error(f"download_failed: {err}")

    def remove_object(self, bucket_name, object_name):
        """
        ä»bucketåˆ é™¤æ–‡ä»¶
        :param bucket_name: minioæ¡¶åç§°
        :param object_name: å­˜æ”¾åœ¨minioæ¡¶ä¸­çš„æ–‡ä»¶åå­—
                            object_nameå¤„å¯ä»¥åŒ…å«ç›®å½•(æ–‡ä»¶å¤¹) ä¾‹å¦‚ /example/file_name
        """
        # æ¡¶æ˜¯å¦å­˜åœ¨
        check_bucket = self.minioClient.bucket_exists(bucket_name)
        if check_bucket:
            try:
                self.minioClient.remove_object(bucket_name=bucket_name,
                                               object_name=object_name)
            except FileNotFoundError as err:
                logger.error(f'delete_failed: {str(err)}')
            except S3Error as err:
                logger.error(f"delete_failed: {err}")

    # è·å–æ‰€æœ‰çš„æ¡¶
    def get_all_bucket(self):
        buckets = self.minioClient.list_buckets()
        ret = []
        for _ in buckets:
            ret.append(_.name)
        return ret

    # è·å–ä¸€ä¸ªæ¡¶ä¸­çš„æ‰€æœ‰ä¸€çº§ç›®å½•å’Œæ–‡ä»¶
    def get_list_objects_from_bucket(self, bucket_name):
        # æ¡¶æ˜¯å¦å­˜åœ¨
        check_bucket = self.minioClient.bucket_exists(bucket_name)
        if check_bucket:
            # è·å–åˆ°è¯¥æ¡¶ä¸­çš„æ‰€æœ‰ç›®å½•å’Œæ–‡ä»¶
            objects = self.minioClient.list_objects(bucket_name=bucket_name)
            ret = []
            for _ in objects:
                ret.append(_.object_name)
            return ret

    # è·å–æ¡¶é‡ŒæŸä¸ªç›®å½•ä¸‹çš„æ‰€æœ‰ç›®å½•å’Œæ–‡ä»¶
    def get_list_objects_from_bucket_dir(self, bucket_name, dir_name):
        # æ¡¶æ˜¯å¦å­˜åœ¨
        check_bucket = self.minioClient.bucket_exists(bucket_name)
        if check_bucket:
            objects = self.minioClient.list_objects(bucket_name=bucket_name,
                                                    prefix=dir_name,
                                                    recursive=True)
            ret = []
            for obj in objects:
                object_name = obj.object_name
                # è·å–å¯¹è±¡çš„å†…å®¹
                content = self.minioClient.get_object(bucket_name=bucket_name,
                                                      object_name=object_name)
                ret.append(content.data.decode())
            return ret


def get_sha1_hash(file_name):
    shal_hash = hashlib.sha1(file_name.encode()).hexdigest()
    return shal_hash


def read_txt_file(file_path):
    with open(file_path, 'r', encoding="utf-8") as file:
        file_contents = file.read()
    return file_contents


class HtmlParse(HTMLParser):
    """
    åŸºäºHTMLParserçš„HTMLè§£æå™¨
    æå–æŒ‡å®šæ ‡ç­¾çš„å†…å®¹å’Œå±æ€§
    """

    def __init__(self, flag=2):
        super().__init__()
        self.flag = flag
        self.main_content = []
        self.image_content = []

        # çŠ¶æ€å˜é‡
        self.in_p_tag = False
        self.in_span_tag = False
        self.in_a_tag = False
        self.current_span_class = None
        self.current_text = ""
        self.span_inner_text = ""

        # ä¸´æ—¶å­˜å‚¨
        self.temp_meta_attrs = {}

    def handle_starttag(self, tag, attrs):
        """å¤„ç†å¼€å§‹æ ‡ç­¾"""
        attrs_dict = dict(attrs)

        if tag == 'meta':
            self.temp_meta_attrs = attrs_dict

        elif tag == 'p' and self.flag == 1:  # ç»´åŸºç™¾ç§‘æ®µè½
            self.in_p_tag = True
            self.current_text = ""

        elif tag == 'span' and self.flag == 2:  # ç™¾åº¦ç™¾ç§‘span
            if 'class' in attrs_dict and 'text_bypwF' in attrs_dict['class']:
                self.in_span_tag = True
                self.current_span_class = attrs_dict['class']
                self.span_inner_text = ""

        elif tag == 'a' and self.in_span_tag:
            self.in_a_tag = True

    def handle_endtag(self, tag):
        """å¤„ç†ç»“æŸæ ‡ç­¾"""
        if tag == 'meta':
            self._process_meta_tag()
            self.temp_meta_attrs = {}

        elif tag == 'p' and self.in_p_tag:
            self.in_p_tag = False
            if self.current_text.strip():
                self.main_content.append(self.current_text.strip())
            self.current_text = ""

        elif tag == 'span' and self.in_span_tag:
            self.in_span_tag = False
            if self.span_inner_text.strip():
                self.main_content.append(self.span_inner_text.strip())
            self.span_inner_text = ""

        elif tag == 'a' and self.in_a_tag:
            self.in_a_tag = False

    def handle_data(self, data):
        """å¤„ç†æ–‡æœ¬æ•°æ®"""
        if self.in_p_tag:
            self.current_text += data

        elif self.in_span_tag:
            self.span_inner_text += data

    def _process_meta_tag(self):
        """å¤„ç†metaæ ‡ç­¾å†…å®¹"""
        if not self.temp_meta_attrs:
            return

        # å¤„ç†æè¿°å†…å®¹
        if 'name' in self.temp_meta_attrs:
            name = self.temp_meta_attrs['name'].lower()
            if name == 'description' and 'content' in self.temp_meta_attrs:
                content = html.unescape(self.temp_meta_attrs['content'])
                self.main_content.append(content)
            elif name == 'image' and 'content' in self.temp_meta_attrs:
                content = self.temp_meta_attrs['content']
                self.image_content.append(content)

        elif 'property' in self.temp_meta_attrs:
            property_val = self.temp_meta_attrs['property'].lower()
            if property_val == 'og:description' and 'content' in self.temp_meta_attrs:
                content = html.unescape(self.temp_meta_attrs['content'])
                self.main_content.append(content)
            elif property_val == 'og:image' and 'content' in self.temp_meta_attrs:
                content = self.temp_meta_attrs['content']
                self.image_content.append(content)

    def get_results(self):
        """è·å–è§£æç»“æœ"""
        return self.main_content, self.image_content


def Dir_html_word(html_file_path, flag):
    """
    ä½¿ç”¨HtmlParseè§£æHTMLæ–‡ä»¶

    Args:
        html_file_path (str): HTMLæ–‡ä»¶è·¯å¾„
        flag (int): è§£ææ¨¡å¼ 1-ç»´åŸºç™¾ç§‘ 2-ç™¾åº¦ç™¾ç§‘

    Returns:
        dict: ç»Ÿä¸€çš„è§£æç»“æœæ ¼å¼
        {
            "success": bool,          # è§£ææ˜¯å¦æˆåŠŸ
            "code": int,              # çŠ¶æ€ç  200-æˆåŠŸï¼Œ400-å®¢æˆ·ç«¯é”™è¯¯ï¼Œ500-æœåŠ¡å™¨é”™è¯¯
            "message": str,           # çŠ¶æ€æ¶ˆæ¯
            "data": {                 # è§£æç»“æœæ•°æ®
                "content": list,      # è§£æå‡ºçš„ä¸»è¦å†…å®¹åˆ—è¡¨
                "image_content": list,# å›¾ç‰‡å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
                "content_count": int, # å†…å®¹æ¡æ•°
                "file_info": dict     # æ–‡ä»¶ä¿¡æ¯
            },
            "error": str              # é”™è¯¯è¯¦æƒ…ï¼ˆå¦‚æœæœ‰ï¼‰
        }
    """
    try:
        html_content = None
        file_basename = os.path.basename(html_file_path)

        # è¯»å–HTMLæ–‡ä»¶å¹¶æ£€æµ‹ç¼–ç 
        with open(html_file_path, 'rb') as f:
            raw_data = f.read()
            detected = chardet.detect(raw_data)
            encoding = detected['encoding'] if detected['confidence'] > 0.7 else 'utf-8'

            try:
                html_content = raw_data.decode(encoding, errors='replace')
            except (LookupError, UnicodeDecodeError):
                # å¦‚æœæ£€æµ‹çš„ç¼–ç æ— æ•ˆï¼Œå°è¯•å¸¸è§ä¸­æ–‡ç¼–ç 
                try:
                    html_content = raw_data.decode('gbk', errors='replace')
                except UnicodeDecodeError:
                    html_content = raw_data.decode('utf-8', errors='replace')

        # æ£€æµ‹åçˆ¬è™«å’Œå¼‚å¸¸å†…å®¹
        if is_anti_crawler_or_error_page(html_content):
            logger.warning(f"âš ï¸ æ£€æµ‹åˆ°åçˆ¬è™«æˆ–é”™è¯¯é¡µé¢: {html_file_path}")
            return {
                "success": False,
                "code": 400,
                "message": "æ£€æµ‹åˆ°åçˆ¬è™«æˆ–é”™è¯¯é¡µé¢",
                "data": {
                    "content": [],
                    "image_content": [],
                    "content_count": 0,
                    "file_info": {
                        "file_name": file_basename,
                        "file_path": html_file_path,
                        "encoding": encoding,
                        "parse_mode": flag
                    }
                },
                "error": f"æ–‡ä»¶ {file_basename} åŒ…å«æ— æ•ˆå†…å®¹æˆ–è¢«è®¿é—®é™åˆ¶"
            }

        # ä½¿ç”¨HtmlParseè§£æHTMLå†…å®¹
        parser = HtmlParse(flag=flag)
        parser.feed(html_content)

        main_content, image_content = parser.get_results()

        # å¦‚æœä½¿ç”¨ç™¾åº¦ç™¾ç§‘æ¨¡å¼ä½†æ²¡æœ‰æ‰¾åˆ°spanå†…å®¹ï¼Œå°è¯•æå–æ‰€æœ‰æ–‡æœ¬
        if flag == 2 and not main_content:
            logger.warning(f"æœªæ‰¾åˆ°ç™¾åº¦ç™¾ç§‘ç‰¹å®šå†…å®¹ï¼Œå°è¯•æå–é€šç”¨æ–‡æœ¬å†…å®¹: {html_file_path}")
            # åˆ›å»ºé€šç”¨è§£æå™¨æå–æ‰€æœ‰å¯è§æ–‡æœ¬
            generic_parser = GenericHtmlParser()
            generic_parser.feed(html_content)
            main_content = generic_parser.get_text_content()

        # å†æ¬¡æ£€æŸ¥è§£æåçš„å†…å®¹æ˜¯å¦æœ‰æ•ˆ
        if main_content:
            filtered_content = filter_invalid_content(main_content)
            if not filtered_content:
                logger.warning(f"âš ï¸ è§£æåå†…å®¹è¢«è¿‡æ»¤ä¸ºç©º: {html_file_path}")
                return {
                    "success": False,
                    "code": 400,
                    "message": "è§£æåå†…å®¹æ— æ•ˆ",
                    "data": {
                        "content": [],
                        "image_content": image_content or [],
                        "content_count": 0,
                        "file_info": {
                            "file_name": file_basename,
                            "file_path": html_file_path,
                            "encoding": encoding,
                            "parse_mode": flag
                        }
                    },
                    "error": f"æ–‡ä»¶ {file_basename} è§£æåå†…å®¹æ— æ•ˆ"
                }
            main_content = filtered_content

        parser.close()

        # ç¡®ä¿è¿”å›çš„å†…å®¹æ˜¯åˆ—è¡¨æ ¼å¼
        if not isinstance(main_content, list):
            main_content = [main_content] if main_content else []
        if not isinstance(image_content, list):
            image_content = [image_content] if image_content else []

        logger.info(f"âœ… HTMLè§£æå®Œæˆ: {html_file_path}, æå–å†…å®¹æ•°é‡: {len(main_content)}")

        return {
            "success": True,
            "code": 200,
            "message": "HTMLè§£ææˆåŠŸ",
            "data": {
                "content": main_content,
                "image_content": image_content,
                "content_count": len(main_content),
                "file_info": {
                    "file_name": file_basename,
                    "file_path": html_file_path,
                    "encoding": encoding,
                    "parse_mode": flag
                }
            },
            "error": None
        }

    except FileNotFoundError:
        logger.error(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {html_file_path}")
        return {
            "success": False,
            "code": 404,
            "message": "æ–‡ä»¶æœªæ‰¾åˆ°",
            "data": {
                "content": [],
                "image_content": [],
                "content_count": 0,
                "file_info": {
                    "file_name": os.path.basename(html_file_path),
                    "file_path": html_file_path,
                    "encoding": None,
                    "parse_mode": flag
                }
            },
            "error": f"æ–‡ä»¶æœªæ‰¾åˆ°: {os.path.basename(html_file_path)}"
        }
    except IOError as e:
        logger.error(f"âŒ æ–‡ä»¶è¯»å–é”™è¯¯ {html_file_path}: {e}")
        return {
            "success": False,
            "code": 500,
            "message": "æ–‡ä»¶è¯»å–é”™è¯¯",
            "data": {
                "content": [],
                "image_content": [],
                "content_count": 0,
                "file_info": {
                    "file_name": os.path.basename(html_file_path),
                    "file_path": html_file_path,
                    "encoding": None,
                    "parse_mode": flag
                }
            },
            "error": f"æ–‡ä»¶è¯»å–é”™è¯¯: {str(e)}"
        }
    except Exception as e:
        logger.error(f"âŒ HTMLè§£æé”™è¯¯ {html_file_path}: {e}")
        return {
            "success": False,
            "code": 500,
            "message": "HTMLè§£æå¤±è´¥",
            "data": {
                "content": [],
                "image_content": [],
                "content_count": 0,
                "file_info": {
                    "file_name": os.path.basename(html_file_path),
                    "file_path": html_file_path,
                    "encoding": None,
                    "parse_mode": flag
                }
            },
            "error": f"HTMLè§£æå¤±è´¥: {str(e)}"
        }


def is_anti_crawler_or_error_page(html_content):
    """
    æ£€æµ‹æ˜¯å¦ä¸ºåçˆ¬è™«é¡µé¢æˆ–é”™è¯¯é¡µé¢

    Args:
        html_content (str): HTMLå†…å®¹

    Returns:
        bool: Trueè¡¨ç¤ºæ˜¯åçˆ¬è™«æˆ–é”™è¯¯é¡µé¢
    """
    if not html_content or len(html_content.strip()) < 50:
        return True

    # å¸¸è§çš„åçˆ¬è™«å’Œé”™è¯¯é¡µé¢ç‰¹å¾
    anti_crawler_indicators = [
        # çŸ¥ä¹ç›¸å…³
        "æ‚¨å½“å‰è¯·æ±‚å­˜åœ¨å¼‚å¸¸ï¼Œæš‚æ—¶é™åˆ¶æœ¬æ¬¡è®¿é—®",
        "çŸ¥ä¹å°ç®¡å®¶åé¦ˆ",
        "æ‘‡ä¸€æ‘‡æˆ–ç™»å½•åç§ä¿¡",
        "7459a2388cb8a6eed61551b9eec0924",

        # ç™¾åº¦ç›¸å…³
        "ç™¾åº¦å®‰å…¨éªŒè¯",
        "è¯·è¾“å…¥éªŒè¯ç ",
        "è®¿é—®è¿‡äºé¢‘ç¹",
        "ç³»ç»Ÿæ£€æµ‹åˆ°æ‚¨çš„ç½‘ç»œç¯å¢ƒå­˜åœ¨å¼‚å¸¸",

        # é€šç”¨åçˆ¬è™«
        "è®¿é—®è¢«æ‹’ç»",
        "Access Denied",
        "403 Forbidden",
        "Please verify you are human",
        "robot or spider",
        "verification required",
        "captcha",
        "é¢‘ç¹è®¿é—®",
        "è®¿é—®é™åˆ¶",
        "æš‚æ—¶æ— æ³•è®¿é—®",
        "è¯·ç¨åå†è¯•",
        "Rate limit exceeded",
        "Too many requests",

        # é”™è¯¯é¡µé¢
        "404 Not Found",
        "é¡µé¢ä¸å­˜åœ¨",
        "æ‰¾ä¸åˆ°é¡µé¢",
        "æœåŠ¡å™¨é”™è¯¯",
        "Server Error",
        "Internal Server Error",
        "ç½‘ç»œé”™è¯¯",
        "è¿æ¥è¶…æ—¶",
        "è¯·æ±‚è¶…æ—¶",

        # é‡å®šå‘æˆ–åŠ è½½é¡µé¢
        "æ­£åœ¨è·³è½¬",
        "é¡µé¢è·³è½¬ä¸­",
        "Loading...",
        "è¯·ç¨å€™",
        "åŠ è½½ä¸­",

        # ç™»å½•é¡µé¢
        "è¯·ç™»å½•",
        "Please login",
        "Sign in required",
        "ç™»å½•åæŸ¥çœ‹",
        "éœ€è¦ç™»å½•"
    ]

    # æ£€æŸ¥å†…å®¹é•¿åº¦ï¼Œè¿‡çŸ­çš„å†…å®¹é€šå¸¸æ— æ•ˆ
    if len(html_content.strip()) < 100:
        return True

    # æ£€æŸ¥æ˜¯å¦åŒ…å«åçˆ¬è™«ç‰¹å¾
    html_lower = html_content.lower()
    for indicator in anti_crawler_indicators:
        if indicator.lower() in html_lower:
            logger.warning(f"ğŸš¨ æ£€æµ‹åˆ°åçˆ¬è™«æŒ‡æ ‡: {indicator}")
            return True

    # æ£€æŸ¥æ˜¯å¦ä¸»è¦ç”±è„šæœ¬æˆ–æ ·å¼ç»„æˆï¼ˆæ— å®é™…å†…å®¹ï¼‰
    try:
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')

        # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
        for script in soup(["script", "style"]):
            script.decompose()

        # è·å–çº¯æ–‡æœ¬å†…å®¹
        text_content = soup.get_text(strip=True)

        # å¦‚æœçº¯æ–‡æœ¬å†…å®¹è¿‡å°‘ï¼Œè®¤ä¸ºæ˜¯æ— æ•ˆé¡µé¢
        if len(text_content) < 50:
            logger.warning(f"ğŸš¨ é¡µé¢çº¯æ–‡æœ¬å†…å®¹è¿‡å°‘: {len(text_content)} å­—ç¬¦")
            return True

        # æ£€æŸ¥æ˜¯å¦å¤§éƒ¨åˆ†æ˜¯é‡å¤å­—ç¬¦æˆ–æ— æ„ä¹‰å†…å®¹
        if is_repetitive_or_meaningless(text_content):
            logger.warning(f"ğŸš¨ æ£€æµ‹åˆ°é‡å¤æˆ–æ— æ„ä¹‰å†…å®¹")
            return True
    except Exception as e:
        logger.warning(f"âš ï¸ å†…å®¹æ£€æµ‹è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
        return True

    return False


def is_repetitive_or_meaningless(text):
    """
    æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸ºé‡å¤æˆ–æ— æ„ä¹‰å†…å®¹

    Args:
        text (str): æ–‡æœ¬å†…å®¹

    Returns:
        bool: Trueè¡¨ç¤ºæ˜¯é‡å¤æˆ–æ— æ„ä¹‰å†…å®¹
    """
    if not text or len(text.strip()) < 20:
        return True

    # æ£€æŸ¥æ˜¯å¦å¤§éƒ¨åˆ†æ˜¯é‡å¤å­—ç¬¦
    char_counts = {}
    for char in text:
        char_counts[char] = char_counts.get(char, 0) + 1

    # å¦‚æœæŸä¸ªå­—ç¬¦å æ¯”è¶…è¿‡30%ï¼Œè®¤ä¸ºæ˜¯é‡å¤å†…å®¹
    total_chars = len(text)
    for char, count in char_counts.items():
        if count / total_chars > 0.3 and char not in [' ', '\n', '\t']:
            return True

    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤§é‡æ•°å­—å’Œç‰¹æ®Šå­—ç¬¦ï¼ˆå¯èƒ½æ˜¯åŠ å¯†æˆ–æ··æ·†å†…å®¹ï¼‰
    import re
    special_chars = len(re.findall(r'[^a-zA-Z0-9\u4e00-\u9fff\s]', text))
    if special_chars / total_chars > 0.4:
        return True

    # æ£€æŸ¥æ˜¯å¦æœ‰åŸºæœ¬çš„ä¸­æ–‡æˆ–è‹±æ–‡å†…å®¹
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    english_chars = len(re.findall(r'[a-zA-Z]', text))

    if chinese_chars + english_chars < total_chars * 0.3:
        return True

    return False


def filter_invalid_content(content_list):
    """
    è¿‡æ»¤æ— æ•ˆå†…å®¹

    Args:
        content_list (list): å†…å®¹åˆ—è¡¨

    Returns:
        list: è¿‡æ»¤åçš„å†…å®¹åˆ—è¡¨
    """
    if not content_list:
        return []

    filtered = []
    for content in content_list:
        if not content or not isinstance(content, str):
            continue

        content = content.strip()
        if len(content) < 10:  # è¿‡çŸ­çš„å†…å®¹
            continue

        # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯æ•°å­—ã€çº¯ç¬¦å·æˆ–é‡å¤å­—ç¬¦
        if content.isdigit() or len(set(content)) <= 3:
            continue

        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„æ–‡å­—
        import re
        meaningful_chars = len(re.findall(r'[\u4e00-\u9fff]', content))  # ä¸­æ–‡å­—ç¬¦
        meaningful_chars += len(re.findall(r'[a-zA-Z]', content))  # è‹±æ–‡å­—ç¬¦

        if meaningful_chars < len(content) * 0.3:  # æœ‰æ„ä¹‰å­—ç¬¦å°‘äº30%
            continue

        # æ£€æŸ¥æ˜¯å¦ä¸ºå¸¸è§çš„æ— æ•ˆå†…å®¹
        invalid_patterns = [
            r'^[\d\s\-_=+*#@$%^&()[\]{}<>|\\/:;,.?!~`\'\"]+$',  # çº¯ç¬¦å·
            r'^(.*?)\1{3,}',  # é‡å¤æ¨¡å¼
            r'^\s*$',  # ç©ºç™½
        ]

        is_invalid = False
        for pattern in invalid_patterns:
            if re.match(pattern, content):
                is_invalid = True
                break

        if not is_invalid:
            filtered.append(content)

    return filtered


def extract_text_from_docx(docx_path):
    try:
        # æ‰“å¼€docxæ–‡ä»¶
        doc = Document(docx_path)

        # æå–æ‰€æœ‰æ®µè½æ–‡æœ¬
        full_text = []
        for para in doc.paragraphs:
            full_text.append(para.text)

        # æå–è¡¨æ ¼ä¸­çš„æ–‡æœ¬
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    full_text.append(cell.text)

        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬ï¼Œç”¨æ¢è¡Œç¬¦åˆ†éš”
        return '\n'.join(full_text)

    except Exception as e:
        logger.error(f"Error processing {docx_path}: {str(e)}")
        return None


def extract_text_with_markitdown(file_path, file_name_original):
    """
    ä½¿ç”¨Microsoft MarkItDownåº“ç»Ÿä¸€è§£æå„ç§æ–‡æ¡£æ ¼å¼

    Args:
        file_path (str): æ–‡ä»¶è·¯å¾„
        file_name_original (str): åŸå§‹æ–‡ä»¶åï¼ˆç”¨ä½œå¤‡ç”¨å†…å®¹ï¼‰

    Returns:
        str: è§£æåçš„æ–‡æœ¬å†…å®¹ï¼Œå¦‚æœè§£æå¤±è´¥åˆ™è¿”å›æ–‡ä»¶å
    """
    try:
        # åˆå§‹åŒ–MarkItDown
        md = MarkItDown()

        logger.info(f"ğŸ” å¼€å§‹ä½¿ç”¨MarkItDownè§£ææ–‡ä»¶: {file_path}")

        # ä½¿ç”¨MarkItDownè§£ææ–‡ä»¶
        result = md.convert(file_path)

        if result and hasattr(result, 'text_content') and result.text_content:
            # è·å–è§£æåçš„æ–‡æœ¬å†…å®¹
            text_content = result.text_content.strip()[:1000]

            if text_content:
                logger.info(f"âœ… MarkItDownè§£ææˆåŠŸ: {file_path}, å†…å®¹é•¿åº¦: {len(text_content)} å­—ç¬¦")
                return text_content
            else:
                logger.warning(f"âš ï¸ MarkItDownè§£æåå†…å®¹ä¸ºç©º: {file_path}")
                return file_name_original

        # å¦‚æœresultæ²¡æœ‰text_contentå±æ€§ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨result
        elif result and hasattr(result, 'content'):
            text_content = result.content.strip()[:1000]
            if text_content:
                logger.info(f"âœ… MarkItDownè§£ææˆåŠŸ(ä½¿ç”¨content): {file_path}, å†…å®¹é•¿åº¦: {len(text_content)} å­—ç¬¦")
                return text_content
            else:
                logger.warning(f"âš ï¸ MarkItDownè§£æåcontentä¸ºç©º: {file_path}")
                return file_name_original

        # å¦‚æœresultæ˜¯å­—ç¬¦ä¸²
        elif isinstance(result, str) and result.strip():
            logger.info(f"âœ… MarkItDownè§£ææˆåŠŸ(å­—ç¬¦ä¸²ç»“æœ): {file_path}, å†…å®¹é•¿åº¦: {len(result)} å­—ç¬¦")
            return result.strip()

        # å¤„ç†DocumentConverterRå¯¹è±¡
        elif result and hasattr(result, '__class__') and 'DocumentConverterR' in str(type(result)):
            logger.info(f"ğŸ” æ£€æµ‹åˆ°DocumentConverterRå¯¹è±¡ï¼Œå°è¯•æå–å†…å®¹: {file_path}")
            try:
                # å°è¯•è·å–æ–‡æœ¬å†…å®¹
                if hasattr(result, 'text_content'):
                    text_content = result.text_content.strip()[:1000] if result.text_content else ""
                elif hasattr(result, 'content'):
                    text_content = result.content.strip()[:1000] if result.content else ""
                elif hasattr(result, 'text'):
                    text_content = result.text.strip()[:1000] if result.text else ""
                elif hasattr(result, 'html'):
                    text_content = result.html.strip()[:1000] if result.html else ""
                else:
                    # å°è¯•å°†å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    text_content = str(result).strip()[:1000]

                if text_content and len(text_content) > 10:
                    logger.info(f"âœ… DocumentConverterRå¯¹è±¡è§£ææˆåŠŸ: {file_path}, å†…å®¹é•¿åº¦: {len(text_content)} å­—ç¬¦")
                    return text_content
                else:
                    logger.warning(f"âš ï¸ DocumentConverterRå¯¹è±¡å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­: {file_path}")
                    return file_name_original

            except Exception as e:
                logger.error(f"âŒ å¤„ç†DocumentConverterRå¯¹è±¡æ—¶å‡ºé”™: {file_path}, é”™è¯¯: {str(e)}")
                return file_name_original

        else:
            logger.warning(f"âš ï¸ MarkItDownè¿”å›äº†æœªé¢„æœŸçš„ç»“æœæ ¼å¼: {file_path}, ç»“æœç±»å‹: {type(result)}")
            # å°è¯•å°†ç»“æœè½¬æ¢ä¸ºå­—ç¬¦ä¸²ä½œä¸ºæœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
            try:
                if result:
                    text_content = str(result).strip()[:1000]
                    if text_content and len(text_content) > 10:
                        logger.info(f"âœ… å°†ç»“æœè½¬æ¢ä¸ºå­—ç¬¦ä¸²æˆåŠŸ: {file_path}, å†…å®¹é•¿åº¦: {len(text_content)} å­—ç¬¦")
                        return text_content
            except Exception as e:
                logger.error(f"âŒ è½¬æ¢ç»“æœä¸ºå­—ç¬¦ä¸²æ—¶å‡ºé”™: {file_path}, é”™è¯¯: {str(e)}")

            return file_name_original

    except FileNotFoundError:
        logger.error(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return file_name_original
    except ImportError as import_e:
        logger.error(f"âŒ MarkItDownåº“å¯¼å…¥å¤±è´¥: {str(import_e)}")
        logger.error("ğŸ’¡ è¯·å®‰è£…markitdownåº“: pip install markitdown")
        return file_name_original
    except Exception as e:
        error_msg = str(e)
        logger.warning(f"âš ï¸ MarkItDownè§£æå¤±è´¥: {file_path}, é”™è¯¯: {error_msg}")

        # å¯¹äºæŸäº›å·²çŸ¥çš„é”™è¯¯ç±»å‹ï¼Œæä¾›æ›´å…·ä½“çš„ä¿¡æ¯
        if "unsupported" in error_msg.lower():
            logger.info(f"ğŸ“„ æ–‡ä»¶æ ¼å¼ä¸å—æ”¯æŒï¼Œä½¿ç”¨æ–‡ä»¶åä½œä¸ºå†…å®¹: {file_path}")
        elif "corrupted" in error_msg.lower() or "damaged" in error_msg.lower():
            logger.warning(f"ğŸ“„ æ–‡ä»¶å¯èƒ½å·²æŸå: {file_path}")
        elif "permission" in error_msg.lower():
            logger.error(f"ğŸ“„ æ–‡ä»¶æƒé™é—®é¢˜: {file_path}")

        # è¿”å›æ–‡ä»¶åä½œä¸ºå¤‡ç”¨å†…å®¹
        return file_name_original


def extract_text_with_markitdown_safe(file_path, file_name_original, doc_type):
    """
    å®‰å…¨çš„MarkItDownæ–‡æ¡£è§£æï¼Œå¸¦æœ‰å¤‡ç”¨æ–¹æ¡ˆ

    Args:
        file_path (str): æ–‡ä»¶è·¯å¾„
        file_name_original (str): åŸå§‹æ–‡ä»¶å
        doc_type (str): æ–‡æ¡£ç±»å‹

    Returns:
        str: è§£æåçš„æ–‡æœ¬å†…å®¹
    """
    # é¦–å…ˆå°è¯•ä½¿ç”¨MarkItDown
    result = extract_text_with_markitdown(file_path, file_name_original)

    # # å¦‚æœMarkItDownè§£æå¤±è´¥ï¼ˆè¿”å›äº†æ–‡ä»¶åï¼‰ï¼Œå¹¶ä¸”æ˜¯ç‰¹å®šæ ¼å¼ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ
    # if result == file_name_original and doc_type:
    #     logger.info(f"ğŸ”„ MarkItDownè§£æå¤±è´¥ï¼Œå°è¯•å¤‡ç”¨è§£ææ–¹æ¡ˆ: {doc_type}")

    # try:
    #     if doc_type == "pdf":
    #         # PDFå¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨PyMuPDF
    #         backup_result = extract_text_with_pymupdf(file_path)
    #         if backup_result and backup_result.strip():
    #             logger.info(f"âœ… PDFå¤‡ç”¨æ–¹æ¡ˆæˆåŠŸ: {file_path}")
    #             return backup_result

    #     elif doc_type == "docx":
    #         # DOCXå¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨python-docx
    #         backup_result = extract_text_from_docx(file_path)
    #         if backup_result and backup_result.strip():
    #             logger.info(f"âœ… DOCXå¤‡ç”¨æ–¹æ¡ˆæˆåŠŸ: {file_path}")
    #             return backup_result

    #     elif doc_type == "html":
    #         # HTMLå¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ç°æœ‰çš„HTMLè§£æå™¨
    #         html_result = Dir_html_word(file_path, 2)
    #         if html_result["success"] and html_result["data"]["content"]:
    #             html_content_list = html_result["data"]["content"]
    #             backup_result = '\n'.join(html_content_list)
    #             logger.info(f"âœ… HTMLå¤‡ç”¨æ–¹æ¡ˆæˆåŠŸ: {file_path}")
    #             return backup_result

    #         elif doc_type == "txt":
    #             # TXTå¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥è¯»å–æ–‡ä»¶
    #             backup_result = read_txt_file(file_path)
    #             if backup_result and backup_result.strip():
    #                 logger.info(f"âœ… TXTå¤‡ç”¨æ–¹æ¡ˆæˆåŠŸ: {file_path}")
    #                 return backup_result

    #     except Exception as backup_e:
    #         logger.warning(f"âš ï¸ å¤‡ç”¨è§£ææ–¹æ¡ˆä¹Ÿå¤±è´¥: {file_path}, é”™è¯¯: {str(backup_e)}")

    # å¦‚æœæ‰€æœ‰æ–¹æ¡ˆéƒ½å¤±è´¥ï¼Œè¿”å›åŸå§‹ç»“æœï¼ˆé€šå¸¸æ˜¯æ–‡ä»¶åï¼‰
    return result


def generate_entity_json(result, file_text_list, file_dict, info_list, customize_content_list, only_name,
                         file_id_list=None):
    """
    ç”Ÿæˆå®ä½“JSONæ•°æ®ï¼Œæ–°æ ¼å¼åŒ…å«æ–‡ä»¶å†…å®¹å­—å…¸

    Args:
        result: å®ä½“ç»“æœå­—å…¸
        file_text_list: æ–‡ä»¶å†…å®¹åˆ—è¡¨ï¼ˆæŒ‰file_id_listé¡ºåºæ’åˆ—ï¼‰
        file_dict: æ–‡ä»¶IDåˆ°è·¯å¾„çš„æ˜ å°„
        info_list: å®ä½“ä¿¡æ¯åˆ—è¡¨
        customize_content_list: è‡ªå®šä¹‰å†…å®¹åˆ—è¡¨
        only_name: æ˜¯å¦åªä½¿ç”¨å®ä½“åï¼ˆä¿æŒå…¼å®¹æ€§ï¼Œå®é™…å·²ä¸ä½¿ç”¨ï¼‰
        file_id_list: æ–‡ä»¶IDåˆ—è¡¨ï¼ˆä¸file_text_listå¯¹åº”ï¼‰

    Returns:
        list: æ ¼å¼åŒ–çš„å®ä½“æ•°æ®åˆ—è¡¨ï¼Œæ–°æ ¼å¼åŒ…å«filetext_dict
    """
    output_list = []
    # ç¡®ä¿ info_list å’Œ customize_content_list çš„é•¿åº¦ä¸ result åŒ¹é…
    result_len = len(result)
    if len(info_list) < result_len:
        info_list = info_list + [''] * (result_len - len(info_list))

    if len(customize_content_list) < result_len:
        customize_content_list = customize_content_list + [''] * (result_len - len(customize_content_list))

    # åˆ›å»ºfile_idåˆ°contentçš„æ˜ å°„ï¼Œç¡®ä¿æ­£ç¡®çš„å¯¹åº”å…³ç³»
    file_id_to_content = {}
    if file_id_list and len(file_text_list) == len(file_id_list):
        # ä½¿ç”¨ä¼ å…¥çš„file_id_listå»ºç«‹æ­£ç¡®çš„æ˜ å°„
        for fid, content in zip(file_id_list, file_text_list):
            file_id_to_content[fid] = content
        logger.info(f"âœ… å»ºç«‹file_idåˆ°contentæ˜ å°„: {len(file_id_to_content)}ä¸ªæ–‡ä»¶")
        # è°ƒè¯•è¾“å‡ºæ˜ å°„å…³ç³»ï¼ˆç®€åŒ–ç‰ˆï¼‰
        for idx, (fid, content) in enumerate(zip(file_id_list[:3], file_text_list[:3])):  # åªè¾“å‡ºå‰3ä¸ª
            content_preview = str(content)[:50] + "..." if len(str(content)) > 50 else str(content)
            logger.debug(f"æ˜ å°„ [{idx}]: file_id={fid} -> content='{content_preview}'")
    else:
        logger.warning(
            f"âš ï¸ file_id_listå‚æ•°ç¼ºå¤±æˆ–é•¿åº¦ä¸åŒ¹é…ï¼Œfile_text_listé•¿åº¦: {len(file_text_list)}, file_id_listé•¿åº¦: {len(file_id_list) if file_id_list else 0}")

    for entity, info, customize_content in zip(result, info_list, customize_content_list):
        file_ids = result[entity][1]
        minio_paths = [file_dict[int(file_id)] for file_id in file_ids]
        keywords = result[entity][0]
        merged_keywords = list({k: None for k in keywords}.keys())

        # æ„å»ºfiletext_dictï¼šæ–‡ä»¶ååˆ°æ–‡ä»¶å†…å®¹çš„æ˜ å°„
        filetext_dict = {}

        # éå†å½“å‰å®ä½“å…³è”çš„æ–‡ä»¶
        for file_id in file_ids:
            try:
                file_path = file_dict[int(file_id)]
                file_name = os.path.basename(file_path)  # æå–æ–‡ä»¶å

                # ä»æ˜ å°„ä¸­è·å–å¯¹åº”çš„æ–‡ä»¶å†…å®¹
                if file_id in file_id_to_content:
                    file_content = file_id_to_content[file_id]
                    logger.debug(f"âœ… é€šè¿‡æ˜ å°„æ‰¾åˆ°æ–‡ä»¶å†…å®¹: {file_name}")
                elif int(file_id) in file_id_to_content:
                    file_content = file_id_to_content[int(file_id)]
                    logger.debug(f"âœ… é€šè¿‡æ˜ å°„æ‰¾åˆ°æ–‡ä»¶å†…å®¹: {file_name}")
                else:
                    # å¦‚æœæ²¡æœ‰æ˜ å°„ï¼Œè®¾ä¸ºç©ºå­—ç¬¦ä¸²å¹¶è®°å½•è­¦å‘Š
                    file_content = ""
                    logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ–‡ä»¶å†…å®¹æ˜ å°„: file_id={file_id}, file_name={file_name}")

                # ç¡®ä¿å†…å®¹ä¸ºå­—ç¬¦ä¸²ç±»å‹
                if file_content is None:
                    file_content = ""
                elif not isinstance(file_content, str):
                    file_content = str(file_content)

                # è®°å½•ç©ºå†…å®¹è­¦å‘Š
                if not file_content.strip():
                    logger.warning(f"âš ï¸ æ–‡ä»¶å†…å®¹ä¸ºç©º: {file_name}")

                filetext_dict[file_name] = file_content

            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ file_id={file_id}, é”™è¯¯: {str(e)}")
                # å³ä½¿å¤±è´¥ä¹Ÿè¦ä¿æŒæ–‡ä»¶ååœ¨å­—å…¸ä¸­
                try:
                    file_path = file_dict[int(file_id)]
                    file_name = os.path.basename(file_path)
                    filetext_dict[file_name] = ""
                except:
                    filetext_dict[f"unknown_file_{file_id}"] = ""

        # æ„å»ºæ–°æ ¼å¼çš„è¾“å‡º
        output = {
            "filetext_dict": filetext_dict,
            "entity": entity,
            "entity_with_keyword": f"{entity}{len(merged_keywords)}" if merged_keywords else entity,  # æ–°æ ¼å¼ï¼šå®ä½“å+å…³é”®è¯æ•°é‡
            "info": info if info is not None else "",
            "customize_content": customize_content
        }
        output_list.append(output)

    return output_list


def updata_to_mysql_new(result, is_xiaoqi):
    db = MySQLDatabase(
        host="114.213.234.179",
        user="koroot",
        password="DMiC-4092",
        database="db_hp"
    )
    entity_file = {}

    try:
        db.connect()  # ç¡®ä¿å»ºç«‹æ•°æ®åº“è¿æ¥
        for key, value in result.items():
            if is_xiaoqi:
                data_to_insert = {
                    "key_words": str(value[0]),
                    "xiaoqi_name": str(key)
                }
                # æ’å…¥ xiaoqi_new è¡¨å¹¶è·å– ID
                xiaoqi_id = db.insert_xiaoqi_new("xiaoqi_new", data_to_insert)
                entity_file[key] = xiaoqi_id

            # æ‰¹é‡å¤„ç† xiaoqi_to_file å…³è”
            file_ids = [int(i) for i in value[1]]
            if file_ids:
                with db.connection.cursor() as cursor:
                    # ä½¿ç”¨ SELECT ... FOR UPDATE é”å®š xiaoqi_new è®°å½•
                    select_query = """
                                   SELECT xiaoqi_id
                                   FROM xiaoqi_new
                                   WHERE xiaoqi_name = %s
                                       FOR UPDATE \
                                   """
                    cursor.execute(select_query, (key,))
                    xiaoqi_record = cursor.fetchone()

                    if not xiaoqi_record:
                        raise ValueError(f"å…³è”å®ä½“ {key} ä¸å­˜åœ¨")

                    xiaoqi_id = xiaoqi_record[0]

                    # æ‰¹é‡æ’å…¥ xiaoqi_to_file
                    insert_query = """
                                   INSERT INTO xiaoqi_to_file (xiaoqi_id, file_id)
                                   VALUES (%s, %s) ON DUPLICATE KEY \
                                   UPDATE xiaoqi_id = xiaoqi_id \
                                   """
                    # ä½¿ç”¨ executemany æ‰¹é‡æ’å…¥
                    cursor.executemany(
                        insert_query,
                        [(xiaoqi_id, fid) for fid in file_ids]
                    )
        db.connection.commit()
        logger.info(f"ğŸ”¥ updata_to_mysql_new æ‰¹é‡æäº¤æˆåŠŸ")
        return entity_file

    except Exception as e:
        logger.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
        return {}

    finally:
        db.close()  # ç¡®ä¿è¿æ¥è¢«å…³é—­


def add_to_xiaoqi(file_text_list, file_id_list, entity, file_dict, file_dict_rev, userID, only_name):
    """
    å¤„ç†å®ä½“åˆ°xiaoqiçš„æ·»åŠ æ“ä½œ

    Returns:
        dict: {"status": "success/error", "message": "è¯¦ç»†ä¿¡æ¯", "data": æ•°æ®(å¯é€‰)}
    """
    db = MySQLDatabase(
        host="114.213.234.179",
        user="koroot",
        password="DMiC-4092",
        database="db_hp"
    )
    result_list = []
    customize_content = []
    driver = GraphDatabase.driver("bolt://114.213.232.140:37687", auth=("neo4j", "123456"))
    info_list = []
    xiaoqi_id = 0
    customize_content_list = []
    # æ‰‹åŠ¨ç®¡ç†Redisåˆ†å¸ƒå¼é”ï¼Œæ ¹æ®æŸ¥è¯¢ç»“æœå†³å®šé‡Šæ”¾æ—¶æœº
    lock = get_distributed_lock(entity.rstrip('0123456789'), timeout=5)
    entity_found = False
    try:
        # è·å–åˆ†å¸ƒå¼é” - é˜»å¡æ¨¡å¼ï¼Œä½¿ç”¨RedisåŸç”ŸBLPOPç­‰å¾…
        logger.info(f"â³ çº¿ç¨‹ {threading.current_thread().name} æ­£åœ¨ç­‰å¾…è·å–entityåˆ†å¸ƒå¼é”: {entity}")
        lock.acquire(blocking=True)  # ä½¿ç”¨RedisåŸç”Ÿé˜»å¡æœºåˆ¶
        logger.info(f"ğŸ”’ çº¿ç¨‹ {threading.current_thread().name} æˆåŠŸè·å–entityåˆ†å¸ƒå¼é”: {entity}")

        entity_list, entity_dict = db.query_entities_by_name(entity)
        if len(entity_list) != 0:
            entity_found = True
            # æ‰¾åˆ°å®ä½“æ—¶ï¼Œç«‹å³å°è¯•é‡Šæ”¾é”
            try:
                lock.release()
                logger.info(f"ğŸ”“ çº¿ç¨‹ {threading.current_thread().name} ç«‹å³é‡Šæ”¾entityåˆ†å¸ƒå¼é” (æ‰¾åˆ°å®ä½“): {entity}")
            except Exception as release_error:
                # é‡Šæ”¾é”å¤±è´¥ï¼Œç«‹å³åœæ­¢å¹¶è¿”å›é”™è¯¯
                error_msg = f"é‡Šæ”¾åˆ†å¸ƒå¼é”å¤±è´¥ - entity: {entity}, é”™è¯¯: {str(release_error)}"
                logger.error(f"âŒ {error_msg}")
                return {
                    "status": "error",
                    "message": error_msg,
                    "error_type": "lock_release_error",
                    "entity": entity,
                    "file_id_list": file_id_list
                }

        try:
            db.connect()
            for file_text, file_id in zip(file_text_list, file_id_list):
                file_data = {
                    'entity': entity.rstrip('0123456789'),
                    'sim': 1,
                    'file_id': file_id,
                }
                db.insert_data_without_primary("entity_to_file", file_data)

            if entity_found:
                result = {}
                for file_text, file_id in zip(file_text_list, file_id_list):
                    dictRes = xiaoqi_instance(file_text, entity_list)
                    info = db.query_dir_by_name_id(entity_dict[dictRes["entity"]])
                    customize_content = db.get_dir_private_list(entity_dict[dictRes["entity"]], userID)

                    if dictRes["entity"] in result:
                        existing_keywords = set(result[dictRes["entity"]][0])
                        new_keywords = set(dictRes["entity_with_keyword"].split(','))
                        combined_keywords = list(existing_keywords.union(new_keywords))

                        result[dictRes["entity"]][0] = combined_keywords
                        result[dictRes["entity"]][1].append(file_id)
                    else:
                        result[dictRes["entity"]] = [dictRes["entity_with_keyword"].split(','), [file_id]]

                    info_list.append(info)
                    customize_content_list.append(customize_content)
                entity_dict = updata_to_mysql_new(result, True)
                logger.info(f"âœ… å®ä½“å¤„ç†å®Œæˆ (é”å·²é‡Šæ”¾): {entity}")
            else:
                result, _, _ = jiekou_3(entity, userID)
                entity_dict = updata_to_mysql_new(result, True)
                file_dict = db.search_file(result, file_dict, file_dict_rev)
                logger.info(f"â³ çº¿ç¨‹ {threading.current_thread().name} ä¿æŒentityåˆ†å¸ƒå¼é” (æœªæ‰¾åˆ°å®ä½“): {entity}")

            final_output = generate_entity_json(result, file_text_list, file_dict, info_list, customize_content_list,
                                                only_name, file_id_list)

            try:
                from query_neo4j.rabbitmq_client_producer import send_classification_tasks_and_wait

                # å¼‚æ­¥å‘é€åˆ†ç±»ä»»åŠ¡åˆ°RabbitMQå¹¶ç­‰å¾…å®Œæˆ
                success = send_classification_tasks_and_wait(
                    final_output=final_output,
                    file_dict_rev=file_dict_rev,
                    entity_id=entity_dict[list(entity_dict.keys())[0]] if entity_dict else None,
                    user_id=userID,
                    xiaoqi_name=entity
                )

                if success:
                    logger.info(f"âœ… æ‰€æœ‰åˆ†ç±»ä»»åŠ¡å·²å®Œæˆ - entity: {entity}, payload_count: {len(final_output)}")
                else:
                    logger.warning(f"âš ï¸ åˆ†ç±»ä»»åŠ¡éƒ¨åˆ†å¤±è´¥æˆ–è¶…æ—¶ - entity: {entity}, payload_count: {len(final_output)}")

                logger.info(f"âœ… æ–‡ä»¶ä¸Šä¼ ã€å®ä½“åˆ›å»ºå’Œåˆ†ç±»ä»»åŠ¡å¤„ç†å®Œæˆ - entity: {entity}, file_id: {file_id}")

            except Exception as async_e:
                logger.error(f"âŒ åˆ†ç±»ä»»åŠ¡å¤„ç†å¤±è´¥ï¼Œé”™è¯¯: {str(async_e)}")
                # å³ä½¿åˆ†ç±»å¤±è´¥ï¼Œå®ä½“å’Œæ–‡ä»¶å·²ç»æˆåŠŸåˆ›å»ºï¼Œè¿”å›éƒ¨åˆ†æˆåŠŸçŠ¶æ€
                logger.info(f"â„¹ï¸ å®ä½“å’Œæ–‡ä»¶åˆ›å»ºæˆåŠŸï¼Œä½†åˆ†ç±»ä»»åŠ¡å¤±è´¥ - entity: {entity}, file_id: {file_id}")

        except Exception as inner_e:
            error_msg = f"å†…éƒ¨æ“ä½œå¤±è´¥ - entity: {entity}, file_id: {file_id}, é”™è¯¯: {str(inner_e)}"
            # å‘ç”Ÿå¼‚å¸¸æ—¶å°è¯•é‡Šæ”¾é”
            if not entity_found:
                try:
                    lock.release()
                    logger.info(f"ğŸ”“ çº¿ç¨‹ {threading.current_thread().name} å¼‚å¸¸é‡Šæ”¾entityåˆ†å¸ƒå¼é”: {entity}")
                except Exception as release_error:
                    # é‡Šæ”¾é”å¤±è´¥ï¼Œç«‹å³åœæ­¢å¹¶è¿”å›é”™è¯¯
                    lock_error_msg = f"å¼‚å¸¸å¤„ç†ä¸­é‡Šæ”¾åˆ†å¸ƒå¼é”å¤±è´¥ - entity: {entity}, åŸå§‹é”™è¯¯: {error_msg}, é‡Šæ”¾é”é”™è¯¯: {str(release_error)}"
                    logger.error(f"âŒ {lock_error_msg}")
                    return {
                        "status": "error",
                        "message": lock_error_msg,
                        "error_type": "lock_release_error_in_exception",
                        "entity": entity,
                        "file_id_list": file_id_list,
                        "original_error": error_msg
                    }

            return {
                "status": "error",
                "message": error_msg,
                "error_type": "inner_operation_error",
                "entity": entity,
                "file_id_list": file_id_list
            }

    except Exception as outer_e:
        error_msg = str(outer_e)
        detailed_msg = f"entityåˆ†å¸ƒå¼é”æ“ä½œå¼‚å¸¸: {error_msg}"
        logger.error(f"âŒ {detailed_msg}")

        # å¦‚æœæ˜¯Redisè¿æ¥é—®é¢˜ï¼Œæä¾›æ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
        if "Redis" in error_msg or "Connection" in error_msg:
            detailed_msg += " (RedisæœåŠ¡å™¨å¯èƒ½æœªå¯åŠ¨æˆ–è¿æ¥é…ç½®æœ‰é—®é¢˜)"
            logger.warning(f"ğŸ’¡ RedisæœåŠ¡å™¨å¯èƒ½æœªå¯åŠ¨æˆ–è¿æ¥é…ç½®æœ‰é—®é¢˜")
            logger.debug(f"ğŸ’¡ EntityåŸæ–‡: {entity}")

        # ç¡®ä¿é”è¢«é‡Šæ”¾
        try:
            lock.release()
            logger.info(f"ğŸ”“ çº¿ç¨‹ {threading.current_thread().name} å¤–å±‚å¼‚å¸¸é‡Šæ”¾entityåˆ†å¸ƒå¼é”: {entity}")
        except Exception as release_error:
            # é‡Šæ”¾é”å¤±è´¥ï¼Œç«‹å³åœæ­¢å¹¶è¿”å›é”™è¯¯
            lock_error_msg = f"å¤–å±‚å¼‚å¸¸å¤„ç†ä¸­é‡Šæ”¾åˆ†å¸ƒå¼é”å¤±è´¥ - entity: {entity}, åŸå§‹é”™è¯¯: {detailed_msg}, é‡Šæ”¾é”é”™è¯¯: {str(release_error)}"
            logger.error(f"âŒ {lock_error_msg}")
            return {
                "status": "error",
                "message": lock_error_msg,
                "error_type": "lock_release_error_in_outer_exception",
                "entity": entity,
                "file_id_list": file_id_list,
                "original_error": detailed_msg
            }

        return {
            "status": "error",
            "message": detailed_msg,
            "error_type": "redis_lock_error",
            "entity": entity,
            "file_id_list": file_id_list
        }

    finally:
        # å¦‚æœæ²¡æ‰¾åˆ°entityï¼Œåœ¨è¿™é‡Œæœ€ç»ˆé‡Šæ”¾é”
        if not entity_found:
            try:
                lock.release()
                logger.info(f"ğŸ”“ çº¿ç¨‹ {threading.current_thread().name} æœ€ç»ˆé‡Šæ”¾entityåˆ†å¸ƒå¼é” (å‡½æ•°ç»“æŸ): {entity}")
            except Exception as release_error:
                # åœ¨finallyå—ä¸­æ— æ³•é€šè¿‡returnåœæ­¢ï¼Œä½†å¯ä»¥æŠ›å‡ºå¼‚å¸¸
                final_error_msg = f"æœ€ç»ˆé‡Šæ”¾åˆ†å¸ƒå¼é”å¤±è´¥ - entity: {entity}, é”™è¯¯: {str(release_error)}"
                logger.error(f"âŒ {final_error_msg}")
                # æŠ›å‡ºå¼‚å¸¸ï¼Œè¿™ä¼šè¢«å¤–å±‚çš„process_fileæ•è·
                raise Exception(final_error_msg)
        else:
            logger.debug(f"ğŸ å‡½æ•°ç»“æŸ - entity: {entity} (é”å·²åœ¨å‰é¢é‡Šæ”¾)")

    # æˆåŠŸæ‰§è¡Œåˆ°è¿™é‡Œï¼Œè¿”å›æˆåŠŸçŠ¶æ€
    return {
        "status": "success",
        "message": f"å®ä½“å¤„ç†æˆåŠŸ - entity: {entity}, file_id_list: {file_id_list}",
        "entity": entity,
        "file_id_list": file_id_list,
        "data": {
            "result": result,
            "entity_found": entity_found,
            "final_output": locals().get('final_output', [])
        }
    }


def create_entity_and_link(tx, name, path, userid):
    import datetime
    current_date = datetime.datetime.now()
    year = current_date.year
    month = current_date.month
    day = current_date.day
    minute = current_date.minute
    formatted_date = f"{year}-{month}-{day}"
    formatted_min = f"{year}-{month}-{day}_{minute}"

    query = """
            MERGE (f:Strict {name: $filename, path: $path, private: $private, user_id: $user_id, timestamp: $time})
            WITH f
            RETURN id(f) as id
            """
    # RETURN id(f) AS file_id, id(h) AS hypernode_id, id(k) AS category_id
    result = tx.run(
        query,
        filename=name,
        path=path,
        private=1,
        user_id=userid,
        time=formatted_date,
    )
    record = result.single()
    return record["id"]


def create_strict_and_coaser(tx, id, direct):
    query = """
        MATCH (a:Strict) WHERE id(a) = $id    
        MATCH (b:coarse {name: $direct})       
        MERGE (a)-[r:edge]->(b)
         RETURN r IS NOT NULL AS exists   
        """
    # RETURN id(f) AS file_id, id(h) AS hypernode_id, id(k) AS category_id
    result = tx.run(
        query,
        id=id,
        direct=direct
    )
    return True


def search_File_name(tx, name):
    query_word = ("MATCH (h:Strict) WHERE h.name = $name RETURN h")
    result = tx.run(query_word, name=name)
    return result.data()


def add_to_mysql(result_list, file_id_list, private, url_list=None):
    db = MySQLDatabase(
        host="114.213.234.179",
        user="koroot",  # æ›¿æ¢ä¸ºæ‚¨çš„ç”¨æˆ·å
        password="DMiC-4092",  # æ›¿æ¢ä¸ºæ‚¨çš„å¯†ç 
        database="db_hp"  # æ›¿æ¢ä¸ºæ‚¨çš„æ•°æ®åº“å
    )
    for result, file_id, url in zip(result_list, file_id_list, url_list):
        try:
            db.connect()
            data_to_insert = {
                "id": str(file_id),  # å‡è®¾ id æ˜¯è‡ªå¢å­—æ®µï¼Œå¯ä»¥è®¾ç½®ä¸º None
                "name": result[0]['h']["name"],  # æ›¿æ¢ä¸ºå®é™…æ•°æ®
                "path": result[0]['h']["path"],  # æ›¿æ¢ä¸ºå®é™…æ—¶é—´æˆ³ï¼Œæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SS
                "timestamp": result[0]['h']["timestamp"],  # æ›¿æ¢ä¸ºå®é™… URL
                "private": private,
                "userid": result[0]['h']["user_id"],
                "url": url if url else None  # æ·»åŠ URLå­—æ®µï¼Œå¦‚æœæ²¡æœ‰URLåˆ™ä¸ºç©ºå­—ç¬¦ä¸²
            }

            db.insert_data("file", data_to_insert)
        finally:
            # å…³é—­æ•°æ®åº“è¿æ¥
            db.close()


def process_single_file_thread_safe(file_name_original, file_path, userid, index):
    """
    çº¿ç¨‹å®‰å…¨çš„å•æ–‡ä»¶å¤„ç†å‡½æ•°ï¼šMinioä¸Šä¼  + æ–‡ä»¶è§£æ + Neo4jæ“ä½œ

    Args:
        file_name_original: åŸå§‹æ–‡ä»¶å
        file_path: æ–‡ä»¶è·¯å¾„
        userid: ç”¨æˆ·ID
        index: æ–‡ä»¶ç´¢å¼•

    Returns:
        DictåŒ…å«å¤„ç†ç»“æœæˆ–é”™è¯¯ä¿¡æ¯
    """
    thread_name = threading.current_thread().name
    try:
        logger.info(f"ğŸš€ [{thread_name}] å¼€å§‹å¤„ç†æ–‡ä»¶ [{index}]: {file_name_original}")

        # Minioä¸Šä¼ æ­¥éª¤
        minio_address = "114.213.232.140:19000"
        minio_admin = "minioadmin"
        minio_password = "minioadmin"
        bucket = Bucket(minio_address=minio_address,
                        minio_admin=minio_admin,
                        minio_password=minio_password)

        # ç”Ÿæˆæ–‡ä»¶ä¿¡æ¯
        doc_type = file_name_original.split('.')[-1].lower()
        file_name = file_name_original.split('.')[0] + '_' + str(int(time.time() * 1000)) + '.' + \
                    doc_type
        path = get_sha1_hash('upload')[:2] + '/' + file_name

        # Minioä¸Šä¼ 
        logger.info(f"ğŸ“¤ [{thread_name}] å¼€å§‹Minioä¸Šä¼  [{index}]: {file_name}")
        bucket.upload_file_to_bucket('kofiles', path, file_path)
        logger.info(f"âœ… [{thread_name}] Minioä¸Šä¼ æˆåŠŸ [{index}]: {file_name}")

        # æ­¥éª¤1ï¼šæ–‡ä»¶å†…å®¹è§£æ
        logger.info(f"ğŸ” [{thread_name}] å¼€å§‹æ–‡ä»¶è§£æ [{index}]: {file_path} (ç±»å‹: {doc_type})")
        doc = extract_text_with_markitdown_safe(file_path, file_name_original, doc_type)

        # æ£€æŸ¥è§£æåçš„å†…å®¹
        if not doc or not doc.strip():
            logger.warning(f"âš ï¸ [{thread_name}] æ–‡ä»¶è§£æåå†…å®¹ä¸ºç©ºï¼Œä½¿ç”¨æ–‡ä»¶å [{index}]: {file_path}")
            doc = file_name_original
        elif doc == file_name_original:
            logger.info(f"ğŸ“„ [{thread_name}] ä½¿ç”¨æ–‡ä»¶åä½œä¸ºå†…å®¹ [{index}]: {file_path}")
        else:
            logger.info(f"âœ… [{thread_name}] æ–‡ä»¶è§£ææˆåŠŸ [{index}]: {file_path}, å†…å®¹é•¿åº¦: {len(str(doc))} å­—ç¬¦")

        # æ­¥éª¤2ï¼šNeo4jæ“ä½œ
        logger.info(f"ğŸ”— [{thread_name}] å¼€å§‹Neo4jæ“ä½œ [{index}]: {file_name}")
        driver = GraphDatabase.driver("bolt://114.213.232.140:37687", auth=("neo4j", "123456"))
        with driver.session() as session:
            file_id = session.write_transaction(
                create_entity_and_link,
                file_name,
                path,
                userid
            )
            search_result = session.write_transaction(search_File_name, file_name)

        logger.info(f"âœ… [{thread_name}] Neo4jæ“ä½œæˆåŠŸ [{index}]: file_id={file_id}")

        # è¿”å›æˆåŠŸç»“æœ
        return {
            "status": "success",
            "index": index,
            "data": {
                "file_name_original": file_name_original,
                "file_name": file_name,
                "file_path": file_path,
                "path": path,
                "doc_type": doc_type,
                "content": doc,
                "file_id": file_id,
                "search_result": search_result,
                "thread_name": thread_name
            }
        }

    except Exception as e:
        error_message = f"æ–‡ä»¶å¤„ç†å¤±è´¥ [{index}]: {str(e)}"
        logger.error(f"âŒ [{thread_name}] {error_message}")

        # æ£€æŸ¥é”™è¯¯ç±»å‹
        if "Minio" in str(e) or "upload" in str(e).lower():
            error_type = "minio_upload_error"
        elif "Neo4j" in str(e) or "ServiceUnavailable" in str(e):
            error_type = "neo4j_error"
        else:
            error_type = "general_error"

        return {
            "status": "error",
            "index": index,
            "message": error_message,
            "error_type": error_type,
            "file_info": {
                "file_name_original": file_name_original,
                "file_path": file_path,
                "thread_name": thread_name
            }
        }


def process_file(file_name_original_list, file_path_list, name, userid, private, url_list=None, only_name=False):
    """
    å¤„ç†æ–‡ä»¶ä¸Šä¼ å’Œå®ä½“æå–
    Returns:
        dict: {"status": "success/error", "message": "è¯¦ç»†ä¿¡æ¯", "data": æ•°æ®(å¯é€‰)}
    """
    try:
        path_list = []
        doc_list = []
        doc_type_list = []
        file_name_list = []
        file_id_list = []
        file_dict = {}
        file_dict_rev = {}
        result_list = []
        # ç¡®ä¿url_listä¸ä¸ºNoneï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºç›¸åŒé•¿åº¦çš„Noneåˆ—è¡¨
        if url_list is None:
            url_list = [None] * len(file_name_original_list)

        max_workers = min(4, len(file_name_original_list))  # é™åˆ¶æœ€å¤§çº¿ç¨‹æ•°
        logger.info(f"ğŸ”§ å¯åŠ¨å¤šçº¿ç¨‹å¤„ç†ï¼Œæ–‡ä»¶æ•°é‡: {len(file_name_original_list)}, çº¿ç¨‹æ•°: {max_workers}")

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æ–‡ä»¶å¤„ç†ä»»åŠ¡
            future_to_index = {}
            for i, (file_name_original, file_path) in enumerate(zip(file_name_original_list, file_path_list)):
                future = executor.submit(process_single_file_thread_safe, file_name_original, file_path, userid, i)
                future_to_index[future] = i

            # åˆå§‹åŒ–ç»“æœå­˜å‚¨ï¼ˆæŒ‰ç´¢å¼•é¡ºåºï¼‰
            results = [None] * len(file_name_original_list)

            # æ”¶é›†ç»“æœ - ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            failed_tasks = []
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    result = future.result()
                    results[index] = result

                    if result["status"] == "error":
                        logger.error(f"âŒ å¤šçº¿ç¨‹ä»»åŠ¡å¤±è´¥ [{index}]: {result['message']}")
                        failed_tasks.append({"index": index, "error": result})
                    else:
                        logger.info(f"âœ… å¤šçº¿ç¨‹ä»»åŠ¡å®Œæˆ [{index}]: {result['data']['file_name_original']}")

                except Exception as task_e:
                    error_message = f"å¤šçº¿ç¨‹ä»»åŠ¡å¼‚å¸¸ [{index}]: {str(task_e)}"
                    logger.error(f"âŒ {error_message}")
                    error_result = {
                        "status": "error",
                        "message": error_message,
                        "error_type": "thread_execution_error",
                        "file_info": {"index": index}
                    }
                    results[index] = error_result
                    failed_tasks.append({"index": index, "error": error_result})

            # å¦‚æœæœ‰å¤±è´¥çš„ä»»åŠ¡ï¼Œè¿”å›ç¬¬ä¸€ä¸ªé”™è¯¯
            if failed_tasks:
                first_error = failed_tasks[0]["error"]
                return {
                    "status": "error",
                    "message": first_error["message"],
                    "error_type": first_error.get("error_type", "thread_error"),
                    "file_info": first_error.get("file_info", {}),
                    "failed_count": len(failed_tasks),
                    "total_count": len(file_name_original_list)
                }

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½å®Œæˆ
        if None in results:
            error_message = "éƒ¨åˆ†å¤šçº¿ç¨‹ä»»åŠ¡æœªå®Œæˆ"
            logger.error(f"âŒ {error_message}")
            return {
                "status": "error",
                "message": error_message,
                "error_type": "incomplete_threads_error"
            }

        # æŒ‰é¡ºåºæ•´ç†ç»“æœï¼Œåªå¤„ç†æˆåŠŸçš„ä»»åŠ¡
        logger.info(f"ğŸ“‹ æ•´ç†å¤šçº¿ç¨‹å¤„ç†ç»“æœï¼Œå…± {len(results)} ä¸ªæ–‡ä»¶")
        successful_indices = []  # è®°å½•æˆåŠŸä»»åŠ¡çš„åŸå§‹ç´¢å¼•

        for i, result in enumerate(results):
            # åªå¤„ç†æˆåŠŸçš„ä»»åŠ¡
            if result["status"] == "success":
                data = result["data"]

                doc_list.append(data["content"])
                file_id_list.append(data["file_id"])
                file_name_list.append(data["file_name"])
                path_list.append(data["path"])
                doc_type_list.append(data["doc_type"])

                file_dict[data["file_id"]] = f"bb/{data['file_name']}"
                file_dict_rev[f"bb/{data['file_name']}"] = data["file_id"]
                result_list.append(data["search_result"])

                successful_indices.append(i)  # è®°å½•æˆåŠŸçš„åŸå§‹ç´¢å¼•
                logger.debug(
                    f"âœ… ç»“æœæ•´ç†å®Œæˆ [{i}]: file_id={data['file_id']}, content_length={len(str(data['content']))}")
            else:
                logger.warning(f"âš ï¸ è·³è¿‡å¤±è´¥ä»»åŠ¡ [{i}]: {result['message']}")

        if url_list:
            url_list = [url_list[i] if i < len(url_list) else None for i in successful_indices]
        else:
            url_list = [None] * len(successful_indices)

        logger.info(
            f"ğŸ¯ å¤šçº¿ç¨‹å¤„ç†å’Œç»“æœæ•´ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(successful_indices)}/{len(file_name_original_list)} ä¸ªæ–‡ä»¶")
        logger.info(f"ğŸ“Š æˆåŠŸä»»åŠ¡ç´¢å¼•é¡ºåº: {successful_indices}")

        if not successful_indices:
            return {
                "status": "error",
                "message": "æ‰€æœ‰æ–‡ä»¶å¤„ç†ä»»åŠ¡éƒ½å¤±è´¥äº†",
                "error_type": "all_tasks_failed",
                "total_count": len(file_name_original_list)
            }

        # éªŒè¯åˆ—è¡¨é•¿åº¦ä¸€è‡´æ€§
        if len(doc_list) != len(file_id_list):
            logger.error(f"âŒ ä¸¥é‡é”™è¯¯ï¼šdoc_listé•¿åº¦({len(doc_list)}) != file_id_listé•¿åº¦({len(file_id_list)})")
            return {
                "status": "error",
                "message": "æ–‡ä»¶å†…å®¹å’Œæ–‡ä»¶IDåˆ—è¡¨é•¿åº¦ä¸åŒ¹é…",
                "error_type": "data_consistency_error"
            }

        logger.info(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(file_id_list)} ä¸ªæ–‡ä»¶ï¼Œdoc_listå’Œfile_id_listé¡ºåºä¸€è‡´")
        # è°ƒç”¨ add_to_mysql å¹¶å¤„ç†å¯èƒ½çš„å¼‚å¸¸
        try:
            add_to_mysql(result_list, file_id_list, private, url_list)
        except Exception as mysql_e:
            error_message = f"MySQLæ•°æ®åº“æ“ä½œå¤±è´¥: {str(mysql_e)}"
            logger.error(f"âŒ MySQLæ“ä½œå¼‚å¸¸: {error_message}")
            return {
                "status": "error",
                "message": error_message,
                "error_type": "mysql_operation_error",
                "file_info": {
                    "file_name_original": file_name_original,
                    "file_id_list": file_id_list,
                    "path_list": path_list
                }
            }

        # è°ƒç”¨ add_to_xiaoqi å¹¶å¤„ç†è¿”å›ç»“æœ
        # ä¼ å…¥file_id_listç¡®ä¿æ­£ç¡®çš„æ–‡ä»¶å†…å®¹æ˜ å°„
        xiaoqi_result = add_to_xiaoqi(doc_list, file_id_list, name, file_dict, file_dict_rev, userid, only_name)

        # æ£€æŸ¥ add_to_xiaoqi çš„æ‰§è¡Œç»“æœ
        if xiaoqi_result["status"] == "error":
            # å¦‚æœ add_to_xiaoqi æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
            return {
                "status": "error",
                "message": f"å®ä½“å¤„ç†å¤±è´¥: {xiaoqi_result['message']}",
                "error_details": xiaoqi_result,
                "file_info": {
                    "file_name_original": file_name_original,
                    "file_id_list": file_id_list,
                    "entity": name
                }
            }

        return {
            "status": "success",
            "message": "æ–‡ä»¶å¤„ç†å®Œæˆ",
            "file_info": {
                "file_name_original": file_name_original,
                "file_id_list": file_id_list,
                "entity": name,
                "path_list": path_list
            },
            "xiaoqi_result": xiaoqi_result
        }

    except Exception as process_e:
        # æ•è·æ‰€æœ‰å…¶ä»–å¼‚å¸¸
        import traceback
        error_msg = f"process_fileæ‰§è¡Œå¤±è´¥: {str(process_e)}"
        error_trace = traceback.format_exc()
        logger.error(f"âŒ {error_msg}")
        logger.debug(f"å¼‚å¸¸å †æ ˆ: {error_trace}")

        return {
            "status": "error",
            "message": error_msg,
            "error_type": "process_file_error",
            "error_trace": error_trace,
            "file_info": {
                "file_name": file_name_original,
                "file_path": file_path,
                "entity": name,
                "userid": userid
            }
        }


def main(request):
    try:
        # ä»GETè¯·æ±‚ä¸­è·å–å‚æ•°
        name = request.GET.get("name")
        only_name = request.GET.get("only_name", "false").lower() == "true"
        userid = request.GET.get("userid")
        # remote_path = request.GET.get('path', None)
        remote_path = request.GET.get('path_list', None)
        private = int(request.GET.get('private', 1))
        url_list = request.GET.get('url_list', None)
    except Exception as param_e:
        return JsonResponse({"status": "error", "message": f"å‚æ•°è§£æå¤±è´¥: {str(param_e)}"}, status=400)

    # éªŒè¯å¿…è¦å‚æ•°
    if not name:
        return JsonResponse({"status": "error", "message": "ç¼ºå°‘å¿…è¦å‚æ•°'name'"}, status=400)
    if not userid:
        return JsonResponse({"status": "error", "message": "ç¼ºå°‘å¿…è¦å‚æ•°'userid'"}, status=400)

    try:
        userid = int(userid)
        private = int(private)
    except ValueError:
        return JsonResponse({"status": "error", "message": "useridå’Œprivateå¿…é¡»æ˜¯æ•´æ•°"}, status=400)

    head_path = 'D:/upload/'
    file_path_list = []
    file_name_original_list = []
    try:
        if remote_path:
            # ä½¿ç”¨è¿œç¨‹æœåŠ¡å™¨æä¾›çš„è·¯å¾„
            for path in remote_path:
                file_name_original = os.path.basename(path)
                file_path = os.path.join(head_path, file_name_original)
                file_name_original_list.append(file_name_original)
                file_path_list.append(file_path)
            # ä»è¿œç¨‹è·¯å¾„ç›´æ¥è¯»å–æ–‡ä»¶ï¼Œä¸éœ€è¦ä»file_obj.chunks()è¯»å–
        else:
            # åŸæœ‰é€»è¾‘ï¼šä»ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡è¯»å–
            file_obj = request.FILES.get('file', None)

            if not file_obj:
                return JsonResponse({
                    "status": "error",
                    "message": "æœªæä¾›æ–‡ä»¶",
                    "error_type": "file_missing_error"
                }, status=400)

            file_path = os.path.join(head_path, file_obj.name)
            with open(file_path, 'wb') as f:
                for chunk in file_obj.chunks():
                    f.write(chunk)
            file_name_original_list.append(file_obj.name)
            file_path_list.append(file_path)

    except IOError as io_e:
        return JsonResponse({
            "status": "error",
            "message": f"æ–‡ä»¶æ“ä½œå¤±è´¥: {str(io_e)}",
            "error_type": "file_operation_error"
        }, status=500)
    except Exception as file_e:
        return JsonResponse({
            "status": "error",
            "message": f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(file_e)}",
            "error_type": "file_handling_error"
        }, status=500)
    try:
        result = process_file(file_name_original_list, file_path_list, name, userid, private, url_list, only_name)

        # æ£€æŸ¥ process_file çš„è¿”å›ç»“æœ
        if isinstance(result, dict) and result.get("status") == "error":
            # process_file è¿”å›äº†é”™è¯¯ä¿¡æ¯
            error_type = result.get("error_type", "unknown_error")

            # æ ¹æ®é”™è¯¯ç±»å‹è®¾ç½®ä¸åŒçš„HTTPçŠ¶æ€ç 
            if error_type in ["neo4j_connection_error", "neo4j_operation_error"]:
                status_code = 503  # Service Unavailable - æ•°æ®åº“æœåŠ¡ä¸å¯ç”¨
            elif error_type in ["redis_lock_error", "lock_release_error", "lock_release_error_in_exception",
                                "lock_release_error_in_outer_exception"]:
                status_code = 503  # Service Unavailable - Redisé”æœåŠ¡ä¸å¯ç”¨
            elif error_type == "mysql_operation_error":
                status_code = 503  # Service Unavailable - MySQLæœåŠ¡ä¸å¯ç”¨
            elif error_type == "minio_upload_error":
                status_code = 502  # Bad Gateway - æ–‡ä»¶å­˜å‚¨æœåŠ¡å¼‚å¸¸
            elif error_type == "process_file_error":
                status_code = 500  # Internal Server Error
            else:
                status_code = 400  # Bad Request

            return JsonResponse({
                "status": "error",
                "message": result["message"],
                "error_type": error_type,
                "error_details": result.get("error_details", {}),
                "file_info": result.get("file_info", {}),
                "error_trace": result.get("error_trace", "")
            }, status=status_code)
        elif isinstance(result, str) and "error" in result:
            # å¤„ç†æ—§çš„JSONå­—ç¬¦ä¸²æ ¼å¼é”™è¯¯
            return JsonResponse({"status": "error", "message": "ä¸Šä¼ æ–‡ä»¶å¤±è´¥", "details": result}, status=500)
        else:
            # æˆåŠŸå¤„ç†
            return JsonResponse({
                "status": "success",
                "message": "æ–‡ä»¶å¤„ç†æˆåŠŸ",
                "data": result
            }, safe=False)

    except Exception as e:
        import traceback
        error_msg = str(e)
        error_trace = traceback.format_exc()
        logger.error(f"mainå‡½æ•°å¼‚å¸¸: {error_msg}")
        logger.debug(f"å¼‚å¸¸å †æ ˆ: {error_trace}")
        return JsonResponse({
            "status": "error",
            "message": f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {error_msg}",
            "error_trace": error_trace
        }, status=500)


def extract_text_with_pymupdf(pdf_path):
    """ä½¿ç”¨PyMuPDFè§£æPDFï¼Œå¸¦æœ‰è¯¦ç»†é”™è¯¯å¤„ç†å’Œå¤‡é€‰æ–¹æ¡ˆ"""
    try:
        # é¦–å…ˆå°è¯•ä½¿ç”¨PyMuPDFè§£æ
        logger.info(f": {pdf_path}")
        doc = fitz.open(pdf_path)
        text = ""

        # é€é¡µæå–æ–‡æœ¬ï¼Œå¹¶å¤„ç†å•é¡µé”™è¯¯
        total_pages = 1
        successful_pages = 0

        for page_num in range(total_pages):
            try:
                page = doc.load_page(page_num)
                page_text = page.get_text()
                text += page_text
                successful_pages += 1
            except Exception as page_e:
                logger.warning(f"âš ï¸ PDFç¬¬{page_num + 1}é¡µè§£æå¤±è´¥: {str(page_e)}")
                # ç»§ç»­å¤„ç†ä¸‹ä¸€é¡µ
                return None

        doc.close()

        logger.info(f"âœ… PDFè§£æå®Œæˆ: æ€»é¡µæ•°={total_pages}, æˆåŠŸé¡µæ•°={successful_pages}")

        # æ£€æŸ¥æ˜¯å¦æå–åˆ°æœ‰æ•ˆå†…å®¹
        if text and text.strip():
            logger.debug(f"ğŸ“ æå–æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
            return text
        else:
            logger.warning("âš ï¸ æœªæå–åˆ°æ–‡æœ¬å†…å®¹ï¼Œå°è¯•å¤‡é€‰æ–¹æ¡ˆ")
            return None

    except Exception as mupdf_e:
        error_msg = str(mupdf_e)
        logger.error(f"âŒ PyMuPDFè§£æå¤±è´¥: {error_msg}")

        # æ£€æŸ¥æ˜¯å¦æ˜¯å¸¸è§çš„MuPDFé”™è¯¯
        if any(keyword in error_msg.lower() for keyword in [
            'syntax error', 'content stream', 'invalid key',
            'expected object', 'damaged', 'corrupt'
        ]):
            logger.info("ğŸ” æ£€æµ‹åˆ°PDFæ ¼å¼é—®é¢˜ï¼Œå°è¯•å¤‡é€‰è§£ææ–¹æ¡ˆ...")

            # å°è¯•å¤‡é€‰æ–¹æ¡ˆ1ï¼šä½¿ç”¨pdfminer
            try:
                logger.info("ğŸ”„ å°è¯•ä½¿ç”¨pdfminerè§£æ...")
                from pdfminer.high_level import extract_text
                text = extract_text(pdf_path)
                if text and text.strip():
                    logger.info(f"âœ… pdfminerè§£ææˆåŠŸï¼Œæå–æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
                    return text
            except Exception as pdfminer_e:
                logger.warning(f"âŒ pdfminerè§£æä¹Ÿå¤±è´¥: {str(pdfminer_e)}")

            # å°è¯•å¤‡é€‰æ–¹æ¡ˆ2ï¼šä½¿ç”¨PyPDF2
            try:
                logger.info("ğŸ”„ å°è¯•ä½¿ç”¨PyPDF2è§£æ...")
                import PyPDF2
                with open(pdf_path, 'rb') as file:
                    pdf_reader = PyPDF2.PdfReader(file)
                    text = ""
                    for page_num in range(len(pdf_reader.pages)):
                        try:
                            page = pdf_reader.pages[page_num]
                            text += page.extract_text()
                        except Exception as page_e:
                            logger.warning(f"âš ï¸ PyPDF2ç¬¬{page_num + 1}é¡µè§£æå¤±è´¥: {str(page_e)}")
                            continue

                    if text and text.strip():
                        logger.info(f"âœ… PyPDF2è§£ææˆåŠŸï¼Œæå–æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
                        return text
            except Exception as pypdf2_e:
                logger.warning(f"âŒ PyPDF2è§£æä¹Ÿå¤±è´¥: {str(pypdf2_e)}")

            logger.warning("âŒ æ‰€æœ‰PDFè§£ææ–¹æ¡ˆéƒ½å¤±è´¥ï¼Œè¿”å›None")
            return None
        else:
            # å…¶ä»–ç±»å‹çš„é”™è¯¯ï¼Œç›´æ¥è¿”å›None
            logger.error(f"âŒ å…¶ä»–ç±»å‹çš„PDFè§£æé”™è¯¯: {error_msg}")
            return None


class GenericHtmlParser(HTMLParser):
    """
    é€šç”¨HTMLè§£æå™¨ï¼Œæå–æ‰€æœ‰å¯è§æ–‡æœ¬å†…å®¹
    """

    def __init__(self):
        super().__init__()
        self.text_content = []
        self.current_text = ""
        self.skip_tags = {'script', 'style', 'head', 'title', 'meta', 'link'}
        self.in_skip_tag = False

    def handle_starttag(self, tag, attrs):
        if tag.lower() in self.skip_tags:
            self.in_skip_tag = True

    def handle_endtag(self, tag):
        if tag.lower() in self.skip_tags:
            self.in_skip_tag = False
        elif not self.in_skip_tag and self.current_text.strip():
            # åœ¨å—çº§å…ƒç´ ç»“æŸæ—¶ä¿å­˜æ–‡æœ¬
            if tag.lower() in {'p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'td', 'th'}:
                self.text_content.append(self.current_text.strip())
                self.current_text = ""

    def handle_data(self, data):
        if not self.in_skip_tag:
            self.current_text += data

    def get_text_content(self):
        # æ·»åŠ æœ€åçš„æ–‡æœ¬å†…å®¹
        if self.current_text.strip():
            self.text_content.append(self.current_text.strip())

        # è¿‡æ»¤ç©ºå†…å®¹å’Œè¿‡çŸ­å†…å®¹
        filtered_content = [content for content in self.text_content
                            if content and len(content.strip()) > 10]

        return filtered_content[:10] if filtered_content else ["HTMLå†…å®¹è§£æä¸ºç©º"]