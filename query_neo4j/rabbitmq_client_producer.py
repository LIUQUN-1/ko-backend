import json
import time
import uuid
import logging
import threading
from datetime import datetime
from typing import List, Dict, Optional
import redis
import pika

logger = logging.getLogger('query_neo4j')

class RabbitMQClientProducer:
    """åŸºäºRabbitMQçš„æ¶ˆæ¯ç”Ÿäº§è€…ï¼Œæ›¿æ¢RocketMQ"""

    def __init__(self,
                 host='114.213.232.140',
                 port=15672,
                 username='admin',
                 password='admin',
                 redis_host='114.213.232.140',
                 redis_port=26379,
                 redis_db=0):
        """
        åˆå§‹åŒ–RabbitMQå®¢æˆ·ç«¯ç”Ÿäº§è€…

        å‚æ•°:
        - host: RabbitMQæœåŠ¡å™¨åœ°å€
        - port: RabbitMQç«¯å£
        - username: RabbitMQç”¨æˆ·å
        - password: RabbitMQå¯†ç 
        - redis_host: RedisæœåŠ¡å™¨åœ°å€ï¼ˆç”¨äºçŠ¶æ€è·Ÿè¸ªï¼‰
        - redis_port: Redisç«¯å£
        - redis_db: Redisæ•°æ®åº“ç¼–å·
        """
        self.host = host
        self.port = port
        self.username = username
        self.password = password

        # é…ç½®RabbitMQè¿æ¥å‚æ•°
        self.credentials = pika.PlainCredentials(username, password)
        self.parameters = pika.ConnectionParameters(
            host=host,
            port=port,
            credentials=self.credentials,
            heartbeat=600,
            blocked_connection_timeout=300
        )

        # Rediså®¢æˆ·ç«¯ç”¨äºçŠ¶æ€è·Ÿè¸ª
        self.redis_client = redis.Redis(
            host=redis_host,
            port=redis_port,
            db=redis_db,
            decode_responses=True,
            socket_connect_timeout=5,
            socket_timeout=5
        )

        # é˜Ÿåˆ—åç§°
        self.first_classification_queue = "first_classification_tasks"
        self.second_classification_queue = "second_classification_tasks"
        self.keywords_queue = "auto_get_keywords_tasks"
        self.task_status_prefix = "task_status:"

        # æµ‹è¯•è¿æ¥
        self._test_connections()

        logger.info(f"âœ… RabbitMQå®¢æˆ·ç«¯ç”Ÿäº§è€…åˆå§‹åŒ–æˆåŠŸ: {host}:{port}")

    def _test_connections(self):
        """æµ‹è¯•RabbitMQå’ŒRedisè¿æ¥"""
        # æµ‹è¯•RabbitMQè¿æ¥
        try:
            connection = pika.BlockingConnection(self.parameters)
            connection.close()
            logger.info(f"âœ… RabbitMQè¿æ¥æµ‹è¯•æˆåŠŸ: {self.host}:{self.port}")
        except Exception as e:
            logger.error(f"âŒ RabbitMQè¿æ¥å¤±è´¥: {str(e)}")
            raise

        # æµ‹è¯•Redisè¿æ¥
        try:
            self.redis_client.ping()
            logger.info(f"âœ… Redisè¿æ¥æˆåŠŸ: {self.redis_client.connection_pool.connection_kwargs['host']}:{self.redis_client.connection_pool.connection_kwargs['port']}")
        except Exception as e:
            logger.error(f"âŒ Redisè¿æ¥å¤±è´¥: {str(e)}")
            raise

    def send_classification_tasks_and_wait(self,
                                           final_output: List[Dict],
                                           xiaoqi_name: Optional[str] = None,
                                           file_dict_rev: Optional[Dict] = None,
                                           entity_id: Optional[str] = None,
                                           user_id: Optional[str] = None,
                                           timeout: int = 300) -> bool:
        """
        å‘é€åˆ†ç±»ä»»åŠ¡åˆ°RabbitMQå¹¶ç­‰å¾…å®Œæˆ

        å‚æ•°:
        - final_output: éœ€è¦åˆ†ç±»çš„æ•°æ®åˆ—è¡¨
        - file_dict_rev: æ–‡ä»¶å­—å…¸åå‘æ˜ å°„ {æ–‡ä»¶è·¯å¾„: æ–‡ä»¶ID}
        - entity_id: å®ä½“ID
        - user_id: ç”¨æˆ·ID
        - timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        è¿”å›:
        - bool: æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½æˆåŠŸå®Œæˆ
        """
        if not final_output:
            logger.warning("âš ï¸ final_outputä¸ºç©ºï¼Œæ— éœ€å‘é€åˆ†ç±»ä»»åŠ¡")
            return True

        task_id = str(uuid.uuid4())
        total_tasks = len(final_output) * 2  # æ¯ä¸ªpayloadéœ€è¦ä¸€é‡åˆ†ç±»å’ŒäºŒé‡åˆ†ç±»

        logger.info(f"ğŸš€ å¼€å§‹å‘é€åˆ†ç±»ä»»åŠ¡ - task_id: {task_id}, payload_count: {len(final_output)}, total_tasks: {total_tasks}")

        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        task_status = {
            'task_id': task_id,
            'total_tasks': total_tasks,
            'completed_tasks': 0,
            'failed_tasks': 0,
            'start_time': datetime.now().isoformat(),
            'status': 'processing',
            'entity_id': entity_id,
            'user_id': user_id,
            'operation': 'insert',
        }

        self.redis_client.hset(f"{self.task_status_prefix}{task_id}", mapping=task_status)
        self.redis_client.expire(f"{self.task_status_prefix}{task_id}", 3600)  # 1å°æ—¶è¿‡æœŸ

        try:
            # å»ºç«‹RabbitMQè¿æ¥
            connection = pika.BlockingConnection(self.parameters)
            channel = connection.channel()

            # å£°æ˜é˜Ÿåˆ—
            channel.queue_declare(queue=self.first_classification_queue, durable=True)
            channel.queue_declare(queue=self.second_classification_queue, durable=True)
            channel.queue_declare(queue=self.keywords_queue, durable=True)
            # å‘é€ä»»åŠ¡åˆ°RabbitMQ
            for i, payload in enumerate(final_output):
                # ä¸€é‡åˆ†ç±»ä»»åŠ¡
                first_task = {
                    'task_id': task_id,
                    'sub_task_id': f"{task_id}_first_{i}",
                    'payload': payload,
                    'task_type': 'first_classification',
                    'entity_id': entity_id,
                    'user_id': user_id,
                    'file_dict_rev': file_dict_rev,
                    'timestamp': datetime.now().isoformat()
                }

                # äºŒé‡åˆ†ç±»ä»»åŠ¡
                second_task = {
                    'task_id': task_id,
                    'sub_task_id': f"{task_id}_second_{i}",
                    'payload': payload,
                    'task_type': 'second_classification',
                    'entity_id': entity_id,
                    'user_id': user_id,
                    'file_dict_rev': file_dict_rev,
                    'timestamp': datetime.now().isoformat()
                }


                # å‘é€ä¸€é‡åˆ†ç±»æ¶ˆæ¯
                channel.basic_publish(
                    exchange='',
                    routing_key=self.first_classification_queue,
                    body=json.dumps(first_task, ensure_ascii=False),
                    properties=pika.BasicProperties(
                        delivery_mode=2,  # æŒä¹…åŒ–æ¶ˆæ¯
                        message_id=f"first_{task_id}_{i}",
                        headers={'task_type': 'first_classification'}
                    )
                )

                # å‘é€äºŒé‡åˆ†ç±»æ¶ˆæ¯
                channel.basic_publish(
                    exchange='',
                    routing_key=self.second_classification_queue,
                    body=json.dumps(second_task, ensure_ascii=False),
                    properties=pika.BasicProperties(
                        delivery_mode=2,  # æŒä¹…åŒ–æ¶ˆæ¯
                        message_id=f"second_{task_id}_{i}",
                        headers={'task_type': 'second_classification'}
                    )
                )


                logger.debug(f"ğŸ“¤ å·²å‘é€ä»»åŠ¡ {i+1}/{len(final_output)} - task_id: {task_id}")
            result = self._wait_for_task_completion(task_id, total_tasks, timeout)
            for i, payload in enumerate(final_output):
                # å…³é”®è¯ä»»åŠ¡
                keywords_task = {
                    'task_id': task_id,
                    'sub_task_id': f"{task_id}_keywords_{i}",
                    'payload': payload,
                    'task_type': 'auto_get_keywords',
                    'entity_id': entity_id,
                    'user_id': user_id,
                    'file_dict_rev': file_dict_rev,
                    'xiaoqi_name': xiaoqi_name,
                    'timestamp': datetime.now().isoformat()
                }
                #å‘é€key_wordsä»»åŠ¡
                channel.basic_publish(
                    exchange='',
                    routing_key=self.keywords_queue,
                    body=json.dumps(keywords_task, ensure_ascii=False),
                    properties=pika.BasicProperties(
                        delivery_mode=2,  # æŒä¹…åŒ–æ¶ˆæ¯
                        message_id=f"keywords_{task_id}_{i}",
                        headers={'task_type': 'keywords'}
                    )
                )
            connection.close()
            logger.info(f"âœ… æ‰€æœ‰ä»»åŠ¡å·²å‘é€åˆ°RabbitMQ - task_id: {task_id}")
            # ç­‰å¾…ä»»åŠ¡å®Œæˆ
            return result

        except Exception as e:
            logger.error(f"âŒ å‘é€ä»»åŠ¡åˆ°RabbitMQå¤±è´¥ - task_id: {task_id}, error: {str(e)}")
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            self.redis_client.hset(f"{self.task_status_prefix}{task_id}", "status", "failed")
            return False
    def send_first_classification_tasks_and_wait(self,
                                                 final_output: List[Dict],
                                                 file_dict_rev: Optional[Dict] = None,
                                                 entity_id: Optional[str] = None,
                                                 user_id: Optional[str] = None,
                                                 timeout: int = 300) -> bool:
        """
        ä»…å‘é€ä¸€é‡åˆ†ç±»ä»»åŠ¡åˆ°RabbitMQå¹¶ç­‰å¾…å®Œæˆ

        å‚æ•°:
        - final_output: éœ€è¦åˆ†ç±»çš„æ•°æ®åˆ—è¡¨
        - file_dict_rev: æ–‡ä»¶å­—å…¸åå‘æ˜ å°„ {æ–‡ä»¶è·¯å¾„: æ–‡ä»¶ID}
        - entity_id: å®ä½“ID
        - user_id: ç”¨æˆ·ID
        - timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        è¿”å›:
        - bool: æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½æˆåŠŸå®Œæˆ
        """
        if not final_output:
            logger.warning("âš ï¸ final_outputä¸ºç©ºï¼Œæ— éœ€å‘é€ä¸€é‡åˆ†ç±»ä»»åŠ¡")
            return True

        task_id = str(uuid.uuid4())
        # ä¸»è¦ä¿®æ”¹ç‚¹ï¼šç°åœ¨æ¯ä¸ªpayloadåªäº§ç”Ÿä¸€ä¸ªä»»åŠ¡ï¼Œæ‰€ä»¥æ€»ä»»åŠ¡æ•°å°±æ˜¯åˆ—è¡¨é•¿åº¦
        total_tasks = len(final_output)

        logger.info(f"ğŸš€ å¼€å§‹å‘é€ä¸€é‡åˆ†ç±»ä»»åŠ¡ - task_id: {task_id}, total_tasks: {total_tasks}")

        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        task_status = {
            'task_id': task_id,
            'total_tasks': total_tasks,
            'completed_tasks': 0,
            'failed_tasks': 0,
            'start_time': datetime.now().isoformat(),
            'status': 'processing',
            'entity_id': entity_id,
            'user_id': user_id,

        }
        try:
            self.redis_client.hset(f"{self.task_status_prefix}{task_id}", mapping=task_status)
            self.redis_client.expire(f"{self.task_status_prefix}{task_id}", 3600)  # 1å°æ—¶è¿‡æœŸ
        except Exception as e:
            print(e)
        try:
            # å»ºç«‹RabbitMQè¿æ¥
            connection = pika.BlockingConnection(self.parameters)
            channel = connection.channel()

            # å£°æ˜ä¸€é‡åˆ†ç±»é˜Ÿåˆ—
            channel.queue_declare(queue=self.first_classification_queue, durable=True)

            # å¾ªç¯å‘é€æ‰€æœ‰ä¸€é‡åˆ†ç±»ä»»åŠ¡
            for i, payload in enumerate(final_output):
                # åªåˆ›å»ºå’Œå‘é€ä¸€é‡åˆ†ç±»ä»»åŠ¡
                first_task = {
                    'task_id': task_id,
                    'sub_task_id': f"{task_id}_first_{i}",
                    'payload': payload,
                    'task_type': 'first_classification',
                    'entity_id': entity_id,
                    'user_id': user_id,
                    'file_dict_rev': file_dict_rev,
                    'timestamp': datetime.now().isoformat(),
                    'operation': 'update',
                }

                # å‘é€ä¸€é‡åˆ†ç±»æ¶ˆæ¯
                channel.basic_publish(
                    exchange='',
                    routing_key=self.first_classification_queue,
                    body=json.dumps(first_task, ensure_ascii=False),
                    properties=pika.BasicProperties(
                        delivery_mode=2,  # æŒä¹…åŒ–æ¶ˆæ¯
                        message_id=f"first_{task_id}_{i}",
                        headers={'task_type': 'first_classification'}
                    )
                )
                logger.debug(f"ğŸ“¤ å·²å‘é€ä¸€é‡åˆ†ç±»ä»»åŠ¡ {i+1}/{len(final_output)} - task_id: {task_id}")

            connection.close()
            logger.info(f"âœ… æ‰€æœ‰ä¸€é‡åˆ†ç±»ä»»åŠ¡å·²å‘é€åˆ°RabbitMQ - task_id: {task_id}")

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            return self._wait_for_task_completion(task_id, total_tasks, timeout)

        except Exception as e:
            logger.error(f"âŒ å‘é€ä¸€é‡åˆ†ç±»ä»»åŠ¡åˆ°RabbitMQå¤±è´¥ - task_id: {task_id}, error: {str(e)}")
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            self.redis_client.hset(f"{self.task_status_prefix}{task_id}", "status", "failed")
            return False

    def _wait_for_task_completion(self, task_id: str, total_tasks: int, timeout: int) -> bool:
        """
        ç­‰å¾…ä»»åŠ¡å®Œæˆ

        å‚æ•°:
        - task_id: ä»»åŠ¡ID
        - total_tasks: æ€»ä»»åŠ¡æ•°
        - timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        è¿”å›:
        - bool: æ˜¯å¦æˆåŠŸå®Œæˆ
        """
        start_time = time.time()
        check_interval = 1  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡

        logger.info(f"â³ å¼€å§‹ç­‰å¾…ä»»åŠ¡å®Œæˆ - task_id: {task_id}, total_tasks: {total_tasks}, timeout: {timeout}s")

        while time.time() - start_time < timeout:
            try:
                # è·å–ä»»åŠ¡çŠ¶æ€
                status_data = self.redis_client.hgetall(f"{self.task_status_prefix}{task_id}")

                if not status_data:
                    logger.warning(f"âš ï¸ ä»»åŠ¡çŠ¶æ€ä¸å­˜åœ¨ - task_id: {task_id}")
                    return False

                completed_tasks = int(status_data.get('completed_tasks', 0))
                failed_tasks = int(status_data.get('failed_tasks', 0))
                current_status = status_data.get('status', 'unknown')

                logger.debug(f"ğŸ“Š ä»»åŠ¡è¿›åº¦ - task_id: {task_id}, completed: {completed_tasks}/{total_tasks}, failed: {failed_tasks}")

                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if completed_tasks + failed_tasks >= total_tasks:
                    if failed_tasks == 0:
                        logger.info(f"âœ… æ‰€æœ‰ä»»åŠ¡å·²æˆåŠŸå®Œæˆ - task_id: {task_id}, completed: {completed_tasks}")
                        self.redis_client.hset(f"{self.task_status_prefix}{task_id}", "status", "completed")
                        return True
                    else:
                        logger.warning(f"âš ï¸ ä»»åŠ¡éƒ¨åˆ†å¤±è´¥ - task_id: {task_id}, completed: {completed_tasks}, failed: {failed_tasks}")
                        self.redis_client.hset(f"{self.task_status_prefix}{task_id}", "status", "partial_failed")
                        return completed_tasks > 0  # åªè¦æœ‰æˆåŠŸçš„å°±è¿”å›True

                # æ£€æŸ¥æ˜¯å¦è¢«æ ‡è®°ä¸ºå¤±è´¥
                if current_status == 'failed':
                    logger.error(f"âŒ ä»»åŠ¡è¢«æ ‡è®°ä¸ºå¤±è´¥ - task_id: {task_id}")
                    return False

                time.sleep(check_interval)

            except Exception as e:
                logger.error(f"âŒ æ£€æŸ¥ä»»åŠ¡çŠ¶æ€æ—¶å‡ºé”™ - task_id: {task_id}, error: {str(e)}")
                time.sleep(check_interval)

        # è¶…æ—¶å¤„ç†
        logger.warning(f"â° ç­‰å¾…ä»»åŠ¡å®Œæˆè¶…æ—¶ - task_id: {task_id}, timeout: {timeout}s")
        self.redis_client.hset(f"{self.task_status_prefix}{task_id}", "status", "timeout")
        return False

    def mark_task_completed(self, task_id: str, sub_task_id: str, success: bool = True) -> None:
        """
        æ ‡è®°å­ä»»åŠ¡å®Œæˆ

        å‚æ•°:
        - task_id: ä¸»ä»»åŠ¡ID
        - sub_task_id: å­ä»»åŠ¡ID
        - success: æ˜¯å¦æˆåŠŸ
        """
        try:
            status_key = f"{self.task_status_prefix}{task_id}"

            if success:
                self.redis_client.hincrby(status_key, "completed_tasks", 1)
                logger.debug(f"âœ… å­ä»»åŠ¡å®Œæˆ - task_id: {task_id}, sub_task_id: {sub_task_id}")
            else:
                self.redis_client.hincrby(status_key, "failed_tasks", 1)
                logger.warning(f"âŒ å­ä»»åŠ¡å¤±è´¥ - task_id: {task_id}, sub_task_id: {sub_task_id}")

        except Exception as e:
            logger.error(f"âŒ æ ‡è®°ä»»åŠ¡çŠ¶æ€æ—¶å‡ºé”™ - task_id: {task_id}, sub_task_id: {sub_task_id}, error: {str(e)}")

    def get_task_status(self, task_id: str) -> Optional[Dict]:
        """
        è·å–ä»»åŠ¡çŠ¶æ€

        å‚æ•°:
        - task_id: ä»»åŠ¡ID

        è¿”å›:
        - Dict: ä»»åŠ¡çŠ¶æ€ä¿¡æ¯
        """
        try:
            status_data = self.redis_client.hgetall(f"{self.task_status_prefix}{task_id}")
            if status_data:
                # è½¬æ¢æ•°å­—å­—æ®µ
                status_data['total_tasks'] = int(status_data.get('total_tasks', 0))
                status_data['completed_tasks'] = int(status_data.get('completed_tasks', 0))
                status_data['failed_tasks'] = int(status_data.get('failed_tasks', 0))
                return status_data
            return None
        except Exception as e:
            logger.error(f"âŒ è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥ - task_id: {task_id}, error: {str(e)}")
            return None


# åˆ›å»ºå…¨å±€ç”Ÿäº§è€…å®ä¾‹
try:
    rabbitmq_producer = RabbitMQClientProducer()
    logger.info("âœ… å…¨å±€RabbitMQç”Ÿäº§è€…åˆ›å»ºæˆåŠŸ")
except Exception as e:
    logger.error(f"âŒ åˆ›å»ºå…¨å±€RabbitMQç”Ÿäº§è€…å¤±è´¥: {str(e)}")
    rabbitmq_producer = None

# å¯¼å‡ºå…¼å®¹æ¥å£
def send_classification_tasks_and_wait(final_output: List[Dict], file_dict_rev: Optional[Dict] = None, entity_id: Optional[str] = None, user_id: Optional[str] = None, xiaoqi_name: Optional[str] = None) -> bool:
    """
    å‘é€åˆ†ç±»ä»»åŠ¡å¹¶ç­‰å¾…å®Œæˆï¼ˆå…¼å®¹åŸæ¥å£ï¼‰
    """
    if rabbitmq_producer is None:
        logger.error("âŒ RabbitMQç”Ÿäº§è€…æœªåˆå§‹åŒ–")
        return False

    return rabbitmq_producer.send_classification_tasks_and_wait(final_output,xiaoqi_name, file_dict_rev, entity_id, user_id)