import logging
import random
import time
import os
import urllib3
import pymysql
import requests
import json
import ast
from bs4 import BeautifulSoup
import re
from urllib.parse import unquote

# from elasticsearch import Elasticsearch  # ä¸å†ç›´æ¥ä½¿ç”¨ESå®¢æˆ·ç«¯ï¼Œé€šè¿‡SearXNGçš„elasticsearchå¼•æ“æŸ¥è¯¢

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# è·å–Djangoé…ç½®çš„æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger('query_neo4j')


class MySQLDatabase:
    def __init__(self, host, user, password, database, charset="utf8mb4"):
        """
        åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        """
        self.config = {
            "host": host,
            "user": user,
            "password": password,
            "database": database,
            "charset": charset
        }
        self.connection = None

    def connect(self):
        """
        å»ºç«‹æ•°æ®åº“è¿æ¥
        """
        try:
            self.connection = pymysql.connect(**self.config)
        except pymysql.MySQLError as e:
            print(f"æ•°æ®åº“è¿æ¥å¤±è´¥ï¼š{e}")
            raise


def clean_filename(title):
    """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
    return "".join(c if c.isalnum() or c in (" ", "_", "-") else "_" for c in title)


def is_meaningless_filename(filename):
    """
    æ£€æµ‹æ–‡ä»¶åæ˜¯å¦ä¸ºä¹±ç æˆ–æ— æ„ä¹‰

    å‚æ•°:
    - filename: æ–‡ä»¶åï¼ˆä¸åŒ…å«æ‰©å±•åï¼‰

    è¿”å›:
    - bool: Trueè¡¨ç¤ºæ–‡ä»¶åæ— æ„ä¹‰ï¼Œéœ€è¦æ›¿æ¢
    """
    if not filename or len(filename.strip()) == 0:
        return True

    # ç§»é™¤å¸¸è§çš„æ— æ„ä¹‰åç¼€
    cleaned = filename.strip().replace('_', '').replace('-', '').replace('.', '')

    # æ£€æŸ¥æ˜¯å¦å…¨æ˜¯æ•°å­—æˆ–å­—æ¯æ•°å­—ç»„åˆï¼ˆå¯èƒ½æ˜¯UUIDã€å“ˆå¸Œå€¼ç­‰ï¼‰
    if len(cleaned) > 10 and (cleaned.isalnum() and not any('\u4e00' <= c <= '\u9fff' for c in cleaned)):
        # æ£€æŸ¥æ˜¯å¦åƒUUIDï¼ˆåŒ…å«è¿å­—ç¬¦çš„é•¿å­—ç¬¦ä¸²ï¼‰
        if len(filename) > 25 and ('-' in filename or '_' in filename):
            return True
        # æ£€æŸ¥æ˜¯å¦å…¨æ˜¯åå…­è¿›åˆ¶å­—ç¬¦
        try:
            int(cleaned, 16)
            if len(cleaned) > 15:  # é•¿çš„åå…­è¿›åˆ¶å­—ç¬¦ä¸²
                return True
        except ValueError:
            pass

    # æ£€æŸ¥å¸¸è§çš„æ— æ„ä¹‰æ–‡ä»¶å
    meaningless_names = [
        'é™„ä»¶', 'attachment', 'file', 'document', 'doc', 'untitled',
        'æ— æ ‡é¢˜', 'æ–‡æ¡£', 'æ–°å»ºæ–‡æ¡£', 'new document'
    ]

    if cleaned.lower() in [name.lower() for name in meaningless_names]:
        return True

    # æ£€æŸ¥æ˜¯å¦ä¸»è¦åŒ…å«ç‰¹æ®Šå­—ç¬¦æˆ–ä¹±ç 
    chinese_chars = sum(1 for c in filename if '\u4e00' <= c <= '\u9fff')
    english_chars = sum(1 for c in filename if c.isalpha())
    total_meaningful = chinese_chars + english_chars

    if len(filename) > 5 and total_meaningful < len(filename) * 0.3:
        return True

    return False


def generate_filename_from_content(content, file_type=None):
    """
    ä»contentå†…å®¹ç”Ÿæˆæœ‰æ„ä¹‰çš„æ–‡ä»¶å

    å‚æ•°:
    - content: å†…å®¹æ–‡æœ¬
    - file_type: æ–‡ä»¶ç±»å‹æ‰©å±•å

    è¿”å›:
    - str: ç”Ÿæˆçš„æ–‡ä»¶å
    """
    if not content or not content.strip():
        base_name = "æœç´¢ç»“æœ"
    else:
        # æå–contentçš„å‰50ä¸ªå­—ç¬¦ä½œä¸ºæ–‡ä»¶å
        content_clean = content.strip()
        # ç§»é™¤æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼
        content_clean = ' '.join(content_clean.split())
        # æˆªå–å‰50ä¸ªå­—ç¬¦
        base_name = content_clean[:50]
        # æ¸…ç†æ–‡ä»¶å
        base_name = clean_filename(base_name)

    # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åç§°
    if not base_name.strip():
        base_name = "æœç´¢ç»“æœ"

    # æ·»åŠ æ–‡ä»¶æ‰©å±•å
    if file_type:
        return f"{base_name}.{file_type.lower()}"
    else:
        return f"{base_name}.html"


def parse_searx_html(html_content, max_results=None):
    """
    è§£æSearX HTMLæœç´¢ç»“æœ

    å‚æ•°:
    - html_content: HTMLå†…å®¹
    - max_results: æœ€å¤§ç»“æœæ•°é‡

    è¿”å›:
    - list: è§£æåçš„æœç´¢ç»“æœåˆ—è¡¨
    """
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        results = []

        # å°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
        result_selectors = [
            '.result',  # æ ‡å‡†SearXç»“æœ
            '.result-item',  # æŸäº›å®ä¾‹ä½¿ç”¨çš„ç±»å
            '.search-result',  # å¦ä¸€ç§å¯èƒ½çš„ç±»å
            'article',  # æœ‰äº›ä½¿ç”¨articleæ ‡ç­¾
            '.urls'  # å¤‡é€‰é€‰æ‹©å™¨
        ]

        result_elements = []
        used_selector = None

        for selector in result_selectors:
            result_elements = soup.select(selector)
            if result_elements:
                used_selector = selector
                logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(result_elements)} ä¸ªHTMLç»“æœ")
                break

        if not result_elements:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†ç»“æœï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰å¤–éƒ¨é“¾æ¥
            logger.info(f"æœªæ‰¾åˆ°æ ‡å‡†ç»“æœå®¹å™¨ï¼Œå°è¯•æŸ¥æ‰¾å¤–éƒ¨é“¾æ¥")
            all_links = soup.find_all('a', href=True)
            external_links = []

            for link in all_links:
                href = link.get('href', '')
                if (href.startswith('http') and
                        'searx' not in href.lower() and
                        'search' not in href.lower()):
                    external_links.append(link)

            # é™åˆ¶ç»“æœæ•°é‡
            if max_results:
                external_links = external_links[:max_results]

            for i, link in enumerate(external_links):
                href = link.get('href', '')
                title = link.get_text(strip=True) or f"æœç´¢ç»“æœ {i + 1}"
                results.append({
                    'url': href,
                    'title': title[:100] + '...' if len(title) > 100 else title,
                    'content': f"æ¥è‡ª {title} çš„æœç´¢ç»“æœ"
                })

            logger.info(f"é€šè¿‡å¤–éƒ¨é“¾æ¥è§£æåˆ° {len(results)} ä¸ªç»“æœ")
            return results

        # é™åˆ¶è§£æçš„å…ƒç´ æ•°é‡
        if max_results:
            result_elements = result_elements[:max_results]

        # è§£ææ‰¾åˆ°çš„ç»“æœ
        for i, result_elem in enumerate(result_elements):
            try:
                # æå–URL
                url = None
                link_selectors = [
                    'h3 a',  # æ ‡é¢˜é“¾æ¥
                    '.result-title a',  # ç»“æœæ ‡é¢˜é“¾æ¥
                    'a.result-link',  # ç»“æœé“¾æ¥
                    'a[href^="http"]',  # ä»»ä½•å¤–éƒ¨é“¾æ¥
                    'a:first-child'  # ç¬¬ä¸€ä¸ªé“¾æ¥
                ]

                for link_selector in link_selectors:
                    link_element = result_elem.select_one(link_selector)
                    if link_element and link_element.get('href'):
                        url = link_element.get('href')
                        break

                if not url:
                    continue

                # æ¸…ç†URL
                if '/url?q=' in url:
                    # æå–qå‚æ•°çš„å€¼
                    match = re.search(r'[?&]q=([^&]+)', url)
                    if match:
                        url = unquote(match.group(1))

                # æå–æ ‡é¢˜
                title = None
                title_selectors = ['h3', '.result-title', '.title', 'h2', 'h4']

                for title_selector in title_selectors:
                    title_element = result_elem.select_one(title_selector)
                    if title_element:
                        title = title_element.get_text(strip=True)
                        break

                if not title:
                    # å°è¯•ä»é“¾æ¥æ–‡æœ¬è·å–æ ‡é¢˜
                    link_element = result_elem.select_one('a')
                    if link_element:
                        title = link_element.get_text(strip=True)

                title = title or f"æœç´¢ç»“æœ {i + 1}"

                # æå–å†…å®¹ç‰‡æ®µ
                content = None
                content_selectors = ['.content', '.snippet', '.description', 'p']

                for content_selector in content_selectors:
                    content_element = result_elem.select_one(content_selector)
                    if content_element:
                        content = content_element.get_text(strip=True)
                        break

                content = content or f"æ¥è‡ª {title} çš„æœç´¢ç»“æœ"

                if url and url.startswith('http'):
                    results.append({
                        'url': url,
                        'title': title[:100] + '...' if len(title) > 100 else title,
                        'content': content[:200] + '...' if len(content) > 200 else content
                    })

            except Exception as e:
                logger.error(f"è§£æHTMLç»“æœ {i + 1} æ—¶å‡ºé”™: {str(e)}")
                continue

        logger.info(f"æˆåŠŸè§£æHTMLå†…å®¹ï¼Œè·å¾— {len(results)} ä¸ªæœ‰æ•ˆç»“æœ")
        return results

    except Exception as e:
        logger.error(f"HTMLè§£æå‡ºé”™: {str(e)}")
        return []


def check_url_exists_for_user_and_xiaoqi(url_list, user_id, xiaoqi_name):
    """
    æ‰¹é‡æ£€æŸ¥URLåˆ—è¡¨æ˜¯å¦åœ¨æ•°æ®åº“ä¸­å·²å­˜åœ¨ä¸”å±äºæŒ‡å®šç”¨æˆ·å’ŒæŒ‡å®šå°å¥‡é¡¹ç›®
    é€šè¿‡ä¸€æ¬¡æ€§æŸ¥è¯¢æ‰€æœ‰ç›¸å…³æ–‡ä»¶çš„URLï¼Œç„¶åä¸ä¼ å…¥çš„URLåˆ—è¡¨å–å·®é›†æ¥æé«˜æ•ˆç‡

    å‚æ•°:
    - url_list: è¦æ£€æŸ¥çš„URLåˆ—è¡¨
    - user_id: ç”¨æˆ·ID
    - xiaoqi_name: å°å¥‡é¡¹ç›®åç§°

    è¿”å›:
    - dict: {
        "duplicate_urls": [],      # é‡å¤çš„URLåŠå…¶æ–‡ä»¶ä¿¡æ¯
        "unique_urls": [],         # ä¸é‡å¤çš„URL
        "xiaoqi_duplicate": bool
      }
    """
    try:
        # è¿æ¥æ•°æ®åº“
        db = MySQLDatabase(
            host="114.213.234.179",
            user="koroot",
            password="DMiC-4092",
            database="db_hp"
        )

        db.connect()

        with db.connection.cursor() as cursor:
            # 1. æ ¹æ®xiaoqi_nameæŸ¥è¯¢xiaoqi_newè¡¨è·å–xiaoqi_id
            xiaoqi_query = "SELECT xiaoqi_id FROM xiaoqi_new WHERE xiaoqi_name = %s"
            cursor.execute(xiaoqi_query, (xiaoqi_name,))
            xiaoqi_result = cursor.fetchone()

            if not xiaoqi_result:
                logger.info(f"æœªæ‰¾åˆ°å°å¥‡é¡¹ç›®: {xiaoqi_name}ï¼Œæ‰€æœ‰URLéƒ½ä¸ºå”¯ä¸€")
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°xiaoqié¡¹ç›®ï¼Œæ‰€æœ‰URLéƒ½æ˜¯å”¯ä¸€çš„
                return {
                    "duplicate_urls": [],
                    "unique_urls": url_list,
                    "xiaoqi_duplicate": False
                }

            xiaoqi_id = xiaoqi_result[0]
            logger.info(f"æ‰¾åˆ°å°å¥‡é¡¹ç›®: {xiaoqi_name}, ID: {xiaoqi_id}")

            # 2. æ ¹æ®xiaoqi_idæŸ¥è¯¢xiaoqi_fileè¡¨è·å–å…³è”çš„file_idåˆ—è¡¨
            file_ids_query = "SELECT file_id FROM xiaoqi_to_file WHERE xiaoqi_id = %s"
            cursor.execute(file_ids_query, (xiaoqi_id,))
            file_ids_results = cursor.fetchall()

            if not file_ids_results:
                logger.info(f"å°å¥‡é¡¹ç›® {xiaoqi_name} æš‚æ— å…³è”æ–‡ä»¶ï¼Œæ‰€æœ‰URLéƒ½ä¸ºå”¯ä¸€")
                # å¦‚æœæ²¡æœ‰å…³è”æ–‡ä»¶ï¼Œæ‰€æœ‰URLéƒ½æ˜¯å”¯ä¸€çš„
                return {
                    "duplicate_urls": [],
                    "unique_urls": url_list,
                    "xiaoqi_duplicate": False
                }

            file_ids = [result[0] for result in file_ids_results]
            logger.info(f"å°å¥‡é¡¹ç›® {xiaoqi_name} å…³è”æ–‡ä»¶æ•°é‡: {len(file_ids)}")

            # 3. ä¸€æ¬¡æ€§æŸ¥è¯¢æ‰€æœ‰ç›¸å…³æ–‡ä»¶çš„URLä¿¡æ¯
            if file_ids:
                # æ„å»ºINæŸ¥è¯¢è¯­å¥ï¼Œè·å–æ‰€æœ‰ç›¸å…³æ–‡ä»¶çš„ä¿¡æ¯
                placeholders = ','.join(['%s'] * len(file_ids))
                file_query = f"""
                SELECT id, name, path, timestamp, private, userid, url 
                FROM file 
                WHERE id IN ({placeholders}) AND (private = 0 OR (private = 1 AND userid = %s))
                """

                # å‚æ•°åˆ—è¡¨ï¼šfile_ids + user_id
                params = file_ids + [str(user_id)]
                cursor.execute(file_query, params)
                results = cursor.fetchall()

                # åˆ›å»ºURLåˆ°æ–‡ä»¶ä¿¡æ¯çš„æ˜ å°„
                existing_url_to_file = {}
                for result in results:
                    file_id, file_name, minio_path, timestamp, private, userid, file_url = result
                    existing_url_to_file[file_url] = {
                        "file_id": file_id,
                        "file_name": file_name,
                        "minio_path": minio_path,
                        "timestamp": timestamp,
                        "private": private,
                        "userid": userid,
                        "url": file_url,
                        "xiaoqi_id": xiaoqi_id,
                        "xiaoqi_name": xiaoqi_name
                    }

                logger.info(f"å°å¥‡é¡¹ç›® {xiaoqi_name} ä¸­å…±æœ‰ {len(existing_url_to_file)} ä¸ªæ–‡ä»¶URL")

                # 4. ä¸ä¼ å…¥çš„URLåˆ—è¡¨å–å·®é›†
                duplicate_urls = []
                unique_urls = []

                for item in url_list:
                    url = item.get('url') if isinstance(item, dict) else item
                    if url in existing_url_to_file:
                        # URLé‡å¤ï¼Œæ·»åŠ åˆ°é‡å¤åˆ—è¡¨
                        duplicate_item = item.copy() if isinstance(item, dict) else {"url": item}
                        duplicate_item.update({
                            "duplicate_reason": f"URLå·²å­˜åœ¨äºé¡¹ç›® '{xiaoqi_name}' ä¸­",
                            "existing_file_info": existing_url_to_file[url],
                            "duplicate_type": "xiaoqi_duplicate"
                        })
                        duplicate_urls.append(duplicate_item)
                        logger.debug(f"å‘ç°é‡å¤URL: {url}")
                    else:
                        # URLå”¯ä¸€ï¼Œæ·»åŠ åˆ°å”¯ä¸€åˆ—è¡¨
                        unique_urls.append(item)

                logger.info(f"å»é‡ç»“æœ - æ€»è®¡: {len(url_list)}, é‡å¤: {len(duplicate_urls)}, å”¯ä¸€: {len(unique_urls)}")

                return {
                    "duplicate_urls": duplicate_urls,
                    "unique_urls": unique_urls,
                    "xiaoqi_duplicate": True if duplicate_urls else False
                }
            else:
                # å¦‚æœæ²¡æœ‰å…³è”æ–‡ä»¶ï¼Œæ‰€æœ‰URLéƒ½æ˜¯å”¯ä¸€çš„
                return {
                    "duplicate_urls": [],
                    "unique_urls": url_list,
                    "xiaoqi_duplicate": False
                }

    except Exception as e:
        logger.error(f"æ‰¹é‡æ£€æŸ¥URLå»é‡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        # å‡ºé”™æ—¶è¿”å›æ‰€æœ‰URLä¸ºå”¯ä¸€
        return {
            "duplicate_urls": [],
            "unique_urls": url_list,
            "xiaoqi_duplicate": False
        }
    finally:
        if 'db' in locals() and hasattr(db, 'connection') and db.connection:
            db.connection.close()


def check_url_exists_for_user(url, user_id):
    """
    æ£€æŸ¥URLæ˜¯å¦åœ¨æ•°æ®åº“ä¸­å·²å­˜åœ¨ä¸”å±äºæŒ‡å®šç”¨æˆ·

    å‚æ•°:
    - url: è¦æ£€æŸ¥çš„URL
    - user_id: ç”¨æˆ·ID
    è¿”å›:
    - dict: {"exists": bool, "file_info": dict or None}
    """
    try:
        # è¿æ¥æ•°æ®åº“
        db = MySQLDatabase(
            host="114.213.234.179",
            user="koroot",
            password="DMiC-4092",
            database="db_hp"
        )

        db.connect()

        # æŸ¥è¯¢æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨è¯¥URLä¸”ç¬¦åˆæƒé™æ¡ä»¶ï¼ˆpublicæˆ–å±äºæŒ‡å®šç”¨æˆ·ï¼‰
        with db.connection.cursor() as cursor:
            query = "SELECT id, name, path, timestamp, private, userid FROM file WHERE url = %s AND (private = 0 OR (private = 1 AND userid = %s))"
            cursor.execute(query, (url, user_id))
            result = cursor.fetchone()

            if not result:
                return {
                    "exists": False,
                    "file_info": None
                }

            file_id, file_name, minio_path, timestamp, private, userid = result
            logger.info(f"æ‰¾åˆ°ç”¨æˆ· {user_id} çš„é‡å¤URL: {url}, æ–‡ä»¶è·¯å¾„: {minio_path}")

            file_info = {
                "file_id": file_id,
                "file_name": file_name,
                "minio_path": minio_path,
                "timestamp": timestamp,
                "private": private,
                "userid": userid,
                "url": url
            }

            return {
                "exists": True,
                "file_info": file_info
            }

    except Exception as e:
        logger.error(f"æ£€æŸ¥URLå»é‡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return {
            "exists": False,
            "file_info": None
        }
    finally:
        if 'db' in locals() and hasattr(db, 'connection') and db.connection:
            db.connection.close()


def process_urls_with_user_deduplication(url_list, user_id, xiaoqi_name=None):
    """
    å¯¹URLåˆ—è¡¨è¿›è¡ŒåŸºäºç”¨æˆ·çš„å»é‡å¤„ç†ï¼Œæ”¯æŒå°å¥‡é¡¹ç›®å»é‡

    å‚æ•°:
    - url_list: URLåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«urlã€titleã€file_nameç­‰ä¿¡æ¯
    - user_id: ç”¨æˆ·ID
    - xiaoqi_name: å°å¥‡é¡¹ç›®åç§°ï¼ˆå¯é€‰ï¼‰
    è¿”å›:
    - dict: {
        "unique_urls": [],  # å»é‡åçš„URLåˆ—è¡¨
        "duplicate_urls": []  # é‡å¤çš„URLåˆ—è¡¨
      }
    """
    logger.info(f"å¼€å§‹å¯¹ç”¨æˆ· {user_id} çš„ {len(url_list)} ä¸ªURLè¿›è¡Œå»é‡æ£€æŸ¥" +
                (f"ï¼Œå°å¥‡é¡¹ç›®: {xiaoqi_name}" if xiaoqi_name else ""))

    # ä½¿ç”¨æ–°çš„æ‰¹é‡å»é‡å‡½æ•°
    check_result = check_url_exists_for_user_and_xiaoqi(url_list, user_id, xiaoqi_name)

    unique_urls = check_result.get("unique_urls", [])
    duplicate_urls = check_result.get("duplicate_urls", [])

    logger.info(f"å»é‡å®Œæˆ - ç”¨æˆ·: {user_id}" +
                (f", å°å¥‡é¡¹ç›®: {xiaoqi_name}" if xiaoqi_name else "") +
                f", æ€»æ•°: {len(url_list)}, å”¯ä¸€: {len(unique_urls)}, é‡å¤: {len(duplicate_urls)}")

    return {
        "unique_urls": unique_urls,
        "duplicate_urls": duplicate_urls
    }

def filter_search_results(search_results, query, use_first_word_only=True):
    """
    è¿‡æ»¤æœç´¢ç»“æœ
    :param search_results: åŸå§‹æœç´¢ç»“æœåˆ—è¡¨
    :param query: æŸ¥è¯¢å­—ç¬¦ä¸²
    :param use_first_word_only: æ˜¯å¦åªä½¿ç”¨ç¬¬ä¸€ä¸ªè¯è¿›è¡Œè¿‡æ»¤
    :return: è¿‡æ»¤åçš„ç»“æœåˆ—è¡¨
    """
    if not query or not query.strip():
        return search_results  # æˆ–è€…è¿”å› []

    query = query.strip()

    if use_first_word_only and ' ' in query:
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªè¯
        search_term = query.split()[0]
    else:
        # ä½¿ç”¨æ•´ä¸ªquery
        search_term = query

    return [
        result for result in search_results
        if search_term.lower() in result.get('title', '').lower()
           or search_term.lower() in result.get('content', '').lower()
    ]
def search_urls(request):
    """
    æ ¹æ®nameæœç´¢URLï¼Œå¹¶è¿”å›URLåˆ—è¡¨å’Œå¯¹åº”å°†è¦ä¿å­˜çš„æ–‡ä»¶å
    æ”¯æŒåŸºäºç”¨æˆ·IDçš„å»é‡æ“ä½œï¼šæ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨ç›¸åŒURLä¸”å±äºå½“å‰ç”¨æˆ·
    æ”¯æŒæ–‡ä»¶ç±»å‹è¿‡æ»¤ï¼šå¯ä»¥æœç´¢ç‰¹å®šç±»å‹çš„æ–‡ä»¶ï¼ˆå¦‚pdfã€docç­‰ï¼‰

    å‚æ•°:
    - name: æœç´¢å…³é”®è¯
    - num_pages_to_crawl: éœ€è¦çˆ¬å–çš„é¡µé¢æ•°é‡ï¼ˆé»˜è®¤100ï¼‰
    - user_id: ç”¨æˆ·IDï¼ˆç”¨äºå»é‡æ£€æŸ¥ï¼‰
    - enable_deduplication: æ˜¯å¦å¯ç”¨å»é‡åŠŸèƒ½ï¼ˆé»˜è®¤Trueï¼‰
    - files: æ–‡ä»¶ç±»å‹è¿‡æ»¤ï¼ˆå¯é€‰ï¼Œå¦‚"pdf"ã€"doc"ç­‰ï¼Œé»˜è®¤ä¸ºNoneè¡¨ç¤ºæœç´¢æ‰€æœ‰ç±»å‹ï¼‰

    è¿”å›:
    - åŒ…å«å»é‡åURLåˆ—è¡¨å’Œé‡å¤URLåˆ—è¡¨çš„ç»“æœ
    """
    try:
        name = request.GET.get("name")
        num_pages_to_crawl = int(request.GET.get("num_pages_to_crawl", 20))
        user_id = request.GET.get("userID")
        enable_deduplication = request.GET.get("enable_deduplication", "true").lower() == "true"
        files = request.GET.get("files", None)  # æ–‡ä»¶ç±»å‹å‚æ•°
        xiaoqi_name = request.GET.get("xiaoqi_name", None)
        enable_key_words = request.GET.get("enable_key_words", "false").lower() == "true"
        offline_search = request.GET.get("offline_search", "false").lower() == "true"

        if not name:
            return {"status": "error", "message": "ç¼ºå°‘æœç´¢å…³é”®è¯"}

        # å¦‚æœå¯ç”¨å…³é”®è¯å¢å¼ºåŠŸèƒ½ï¼Œä½¿ç”¨ä¸“ä¸šé¢†åŸŸä¿¡æ¯å¢å¼ºæœç´¢å…³é”®è¯
        if enable_key_words and xiaoqi_name:
            keywords_list = enhance_keywords_with_domain(name, xiaoqi_name)
        else:
            keywords_list = [name]

        # ä½¿ç”¨SearXNGæœç´¢ï¼ˆæ”¯æŒelasticsearchå¼•æ“ï¼‰
        search_result = fetch_search_results_with_searx(
            query=name,
            num_pages_to_crawl=num_pages_to_crawl,
            file_type=files,
            offline_search=offline_search,
        )
        # print("search_result",search_result)
        # urls_count = len(search_result)
        # print(f"fetch_search_results_with_searçš„è¼¸å‡ºsearch_resultçš„æ•°é‡: {urls_count}")

        search_result = filter_search_results(search_result, name, use_first_word_only=True)
        if not search_result:
            return {"status": "error", "message": "æœç´¢ç»“æœä¸ºç©º"}

        allowed_file_types = ['doc', 'docx', 'ppt', 'pptx', 'txt', 'pdf', 'html']
        filtered_results = []
        for result in search_result:
            url = result.get('url', '').lower()

            url_without_params = url.split('?')[0].split('#')[0]
            match = re.search(r'\.([a-zA-Z0-9]+)$', url_without_params)

            if match:
                file_ext = match.group(1).lower()
                if file_ext in allowed_file_types:
                    filtered_results.append(result)
                else:
                    logger.debug(f"ä¸¢å¼ƒä¸ç¬¦åˆç±»å‹è¦æ±‚çš„æ–‡ä»¶: {url} (æ‰©å±•å: {file_ext})")
            else:
                filtered_results.append(result)

        search_result = filtered_results
        logger.info(f"æ–‡ä»¶ç±»å‹è¿‡æ»¤åå‰©ä½™ {len(filtered_results)} ä¸ªç»“æœ")

        # å¦‚æœå¯ç”¨å»é‡åŠŸèƒ½ä¸”æä¾›äº†ç”¨æˆ·ID
        if enable_deduplication and user_id and xiaoqi_name:

            dedup_result = process_urls_with_user_deduplication(search_result, user_id, xiaoqi_name)
            unique_urls = dedup_result["unique_urls"]
            duplicate_urls = dedup_result["duplicate_urls"]

            return {
                "status": "success",
                "data": unique_urls,
                "duplicate_urls": duplicate_urls,
                "search_params": {
                    "keyword": name,
                    "file_type": files,
                    "pages_crawled": num_pages_to_crawl,
                    "xiaoqi_name": xiaoqi_name
                },
                "deduplication_summary": {
                    "total_urls": len(search_result),
                    "unique_count": len(unique_urls),
                    "duplicate_count": len(duplicate_urls),
                    "user_id": user_id,
                    "deduplication_enabled": True
                },
                "message": f"æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(unique_urls)} ä¸ªå”¯ä¸€URLï¼Œ{len(duplicate_urls)} ä¸ªé‡å¤URL" +
                           (f"ï¼Œæ–‡ä»¶ç±»å‹: {files}" if files else "") +
                           (f"ï¼Œå°å¥‡é¡¹ç›®: {xiaoqi_name}" if xiaoqi_name else "")
            }
        else:
            if enable_deduplication and not user_id:
                logger.warning("å¯ç”¨äº†å»é‡åŠŸèƒ½ä½†æœªæä¾›ç”¨æˆ·IDï¼Œå°†è¿”å›æ‰€æœ‰æœç´¢ç»“æœ")

            return {
                "status": "success",
                "data": {
                    "unique_urls": search_result,
                    "duplicate_urls": []
                },
                "deduplication_summary": {
                    "total_urls": len(search_result),
                    "unique_count": len(search_result),
                    "duplicate_count": 0,
                    "user_id": user_id,
                    "deduplication_enabled": False
                },
                "search_params": {
                    "keyword": name,
                    "file_type": files,
                    "pages_crawled": num_pages_to_crawl,
                    "xiaoqi_name": xiaoqi_name
                },
                "message": f"æœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(search_result)} ä¸ªURL" +
                           (f"ï¼Œæ–‡ä»¶ç±»å‹: {files}" if files else "") +
                           (f"ï¼Œå°å¥‡é¡¹ç›®: {xiaoqi_name}" if xiaoqi_name else "")
            }

    except Exception as e:
        logger.error(f"æœç´¢URLæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return {"status": "error", "message": f"æœç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"}



def fetch_search_results_with_searx(query: str,
                                    num_pages_to_crawl: int = 60,
                                    file_type: str = None,
                                    offline_search: bool = False):
    """
    ç›´æ¥é€šè¿‡HTTPæ¥å£è®¿é—®SearxNGè·å–æœç´¢ç»“æœ
    ä¼˜åŒ–å®ä¾‹é€‰æ‹©ç­–ç•¥ï¼Œé¿å…å®ä¾‹2é¢‘ç¹403ï¼Œå¹¶æ·»åŠ è¯¦ç»†å®ä¾‹ä½¿ç”¨è¿½è¸ª
    """
    # é…ç½®å¢å¼ºçš„User-Agentæ± 
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
    ]

    # æ„å»ºæœç´¢æŸ¥è¯¢
    if file_type in ['doc', 'docx', 'ppt', 'pptx', 'txt', 'pdf']:
        format_type = 'html'
        formatted_query = f"{query} filetype:{file_type}"
        logger.info(f"ä½¿ç”¨æ–‡ä»¶ç±»å‹è¿‡æ»¤è¿›è¡Œæœç´¢: {formatted_query}, é¢„æœŸç»“æœ: {num_pages_to_crawl}")
        engines = ['google']
        # ä¼˜å…ˆä½¿ç”¨å®ä¾‹1ï¼Œå®ä¾‹2ä½œä¸ºå¤‡ç”¨
        searx_host_list = ['http://114.213.232.140:18081/', 'https://searx.bndkt.io/']
    elif file_type in ['pdf']:
        format_type = 'html'
        formatted_query = f"{query} filetype:{file_type}"
        logger.info(f"ä½¿ç”¨æ–‡ä»¶ç±»å‹è¿‡æ»¤è¿›è¡Œæœç´¢: {formatted_query}, é¢„æœŸç»“æœ: {num_pages_to_crawl}")
        engines = ['baidu', 'bing']
        searx_host_list = ['http://114.213.232.140:18081/', 'https://searx.bndkt.io/']
    else:
        formatted_query = f"{query}"
        logger.info(f"è¿›è¡Œå¸¸è§„æœç´¢: {formatted_query}, é¢„æœŸç»“æœ: {num_pages_to_crawl}")
        if offline_search:
            format_type = 'json'
            engines = ['elasticsearch']
            # ç¦»çº¿æœç´¢åªä½¿ç”¨å®ä¾‹1
            searx_host_list = ['http://114.213.232.140:18081/']
        else:
            format_type = 'json'
            engines = ['baidu','bing','360search', 'quark', 'sogou']
        ####'bing'
            # å¸¸è§„æœç´¢ä¼˜å…ˆä½¿ç”¨å®ä¾‹1
            searx_host_list = ['http://114.213.232.140:18081/']###, 'https://searx.bndkt.io/'

    search_url_list = [searx_host + 'search' for searx_host in searx_host_list]

    # å®ä¾‹æ€§èƒ½ç»Ÿè®¡
    instance_stats = {
        url: {'success': 0, 'fail': 0, 'last_403': 0}
        for url in search_url_list
    }

    # è¯¦ç»†é¡µé¢å®ä¾‹ä½¿ç”¨è®°å½•
    page_detailed_stats = {}

    if format_type.lower() == 'html':
        accept_header = 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
    else:
        accept_header = 'application/json'

    engines_str = ','.join(engines)

    max_attempts = 3
    timeout_config = 5
    results_per_page = 20
    pages_to_fetch = max(1, (num_pages_to_crawl + results_per_page - 1) // results_per_page)

    logger.info(f"éœ€è¦è·å– {pages_to_fetch} é¡µæ¥æ»¡è¶³ {num_pages_to_crawl} ä¸ªç»“æœçš„éœ€æ±‚")
    logger.info(f"å¯ç”¨å®ä¾‹: {search_url_list}")

    # åˆ›å»ºä¼šè¯
    session = requests.Session()
    adapter = requests.adapters.HTTPAdapter(max_retries=2)
    session.mount('http://', adapter)
    session.mount('https://', adapter)

    searx_result_list = []
    page_status = {}
    consecutive_empty_pages = 0
    max_consecutive_empty_pages = 2

    for page in range(pages_to_fetch):
        current_page = page + 1
        logger.info(f"=== å¼€å§‹è·å–ç¬¬ {current_page}/{pages_to_fetch} é¡µ ===")

        # åˆå§‹åŒ–å½“å‰é¡µçš„è¯¦ç»†è®°å½•
        page_detailed_stats[current_page] = {
            'attempts': [],  # è®°å½•æ¯æ¬¡å°è¯•çš„è¯¦ç»†ä¿¡æ¯
            'final_instance': None,  # æœ€ç»ˆæˆåŠŸçš„å®ä¾‹
            'success': False,
            'results_count': 0
        }

        params = {
            'q': formatted_query,
            'format': format_type,
            'engines': engines_str,
            'categories': 'general',
            'language': 'auto',
            'pageno': current_page,
            '_': str(int(time.time() * 1000))[-6:]
        }

        page_success = False
        page_results_count = 0
        page_error = None

        # æ™ºèƒ½é€‰æ‹©å®ä¾‹ï¼šä¼˜å…ˆé€‰æ‹©æˆåŠŸç‡é«˜çš„å®ä¾‹
        sorted_instances = sorted(
            search_url_list,
            key=lambda url: (
                -instance_stats[url]['success'],  # æˆåŠŸç‡é«˜çš„ä¼˜å…ˆ
                instance_stats[url]['last_403']  # æœ€è¿‘403æ—¶é—´ä¹…çš„ä¼˜å…ˆ
            )
        )

        logger.info(f"ç¬¬{current_page}é¡µå®ä¾‹ä¼˜å…ˆçº§: {[url.split('//')[-1].split('/')[0] for url in sorted_instances]}")

        for searx_instance_idx, search_url in enumerate(sorted_instances):
            if page_success:
                break

            # å¦‚æœè¯¥å®ä¾‹æœ€è¿‘æœ‰403é”™è¯¯ï¼Œå¢åŠ ç­‰å¾…æ—¶é—´
            last_403_time = instance_stats[search_url]['last_403']
            if last_403_time > 0 and time.time() - last_403_time < 300:  # 5åˆ†é’Ÿå†…æœ‰è¿‡403
                wait_403 = 5  # ç­‰å¾…10ç§’
                logger.info(f"å®ä¾‹ {search_url} æœ€è¿‘æœ‰è¿‡403é”™è¯¯ï¼Œç­‰å¾…{wait_403}ç§’åä½¿ç”¨")
                time.sleep(wait_403)

            for attempt in range(max_attempts):
                # è®°å½•æ¯æ¬¡å°è¯•çš„è¯¦ç»†ä¿¡æ¯
                attempt_info = {
                    'instance': search_url,
                    'instance_index': searx_instance_idx + 1,
                    'attempt_number': attempt + 1,
                    'status_code': None,
                    'success': False,
                    'error': None,
                    'timestamp': time.time(),
                    'results_count': 0
                }

                try:
                    # ä¸ºå®ä¾‹2ä½¿ç”¨æ›´ä¿å®ˆçš„è¯·æ±‚å¤´
                    if 'searx.bndkt.io' in search_url:
                        # å®ä¾‹2éœ€è¦æ›´çœŸå®çš„æµè§ˆå™¨å¤´
                        headers = {
                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                            'Accept-Encoding': 'gzip, deflate, br',
                            'DNT': '1',
                            'Connection': 'keep-alive',
                            'Upgrade-Insecure-Requests': '1',
                            'Sec-Fetch-Dest': 'document',
                            'Sec-Fetch-Mode': 'navigate',
                            'Sec-Fetch-Site': 'none',
                            'Cache-Control': 'max-age=0'
                        }
                    else:
                        # å®ä¾‹1å¯ä»¥ä½¿ç”¨æ›´å®½æ¾çš„è¯·æ±‚å¤´
                        headers = {
                            'User-Agent': random.choice(user_agents),
                            'Accept': accept_header,
                            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                            'DNT': random.choice(['0', '1']),
                            'Connection': 'keep-alive'
                        }

                    logger.debug(
                        f"ç¬¬{current_page}é¡µ å°è¯•{attempt + 1}/{max_attempts} ä½¿ç”¨å®ä¾‹ {searx_instance_idx + 1}: {search_url}")

                    # å‘é€è¯·æ±‚
                    response = session.get(
                        search_url,
                        params=params,
                        headers=headers,
                        timeout=timeout_config,
                        verify=False,
                    )

                    attempt_info['status_code'] = response.status_code

                    if response.status_code == 200:
                        logger.info(f"âœ… ç¬¬{current_page}é¡µåœ¨å®ä¾‹ {search_url} ä¸ŠHTTPè¯·æ±‚æˆåŠŸ")
                        instance_stats[search_url]['success'] += 1
                        attempt_info['success'] = True

                        if format_type.lower() == 'html':
                            html_results = parse_searx_html(response.text, num_pages_to_crawl)
                            page_results_count = len(html_results) if html_results else 0
                            attempt_info['results_count'] = page_results_count

                            if html_results and page_results_count > 0:
                                logger.info(f"ç¬¬{current_page}é¡µHTMLè§£ææˆåŠŸï¼Œè·å¾— {page_results_count} ä¸ªç»“æœ")

                                processed_count = 0
                                for i, item in enumerate(html_results):
                                    url = item.get('url', '')
                                    title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')
                                    content = item.get('content', '')

                                    if file_type and not url.lower().endswith(f".{file_type.lower()}"):
                                        continue

                                    if not content.strip():
                                        content = f"æ¥è‡ª {title} çš„æœç´¢ç»“æœ"

                                    title_without_ext = title.rsplit('.', 1)[0] if '.' in title else title
                                    if is_meaningless_filename(title_without_ext):
                                        file_name = generate_filename_from_content(content, file_type)
                                    else:
                                        safe_title = clean_filename(title_without_ext)
                                        file_name = f"{safe_title}.{file_type.lower()}" if file_type else f"{safe_title}.html"

                                    searx_result_list.append({
                                        "url": url,
                                        "title": title,
                                        "file_name": file_name,
                                        "index": len(searx_result_list),
                                        "content": content.strip(),
                                        "file_type": file_type if file_type else "html",
                                        "page": current_page,
                                        "instance": search_url  # è®°å½•ä½¿ç”¨çš„å®ä¾‹
                                    })
                                    processed_count += 1

                                logger.info(f"ç¬¬{current_page}é¡µæˆåŠŸå¤„ç† {processed_count}/{page_results_count} ä¸ªç»“æœ")
                                page_success = True
                                consecutive_empty_pages = 0
                                page_detailed_stats[current_page]['final_instance'] = search_url
                                page_detailed_stats[current_page]['success'] = True
                                page_detailed_stats[current_page]['results_count'] = processed_count

                            else:
                                logger.warning(f"ç¬¬{current_page}é¡µHTMLè§£ææœªæ‰¾åˆ°ç»“æœ")
                                page_error = "HTMLè§£ææ— ç»“æœ"
                                consecutive_empty_pages += 1
                                attempt_info['error'] = page_error

                        else:
                            # JSONè§£æ
                            try:
                                data = response.json()
                                page_results_count = len(data.get('results', []))
                                attempt_info['results_count'] = page_results_count

                                if 'results' in data and page_results_count > 0:
                                    logger.info(f"ç¬¬{current_page}é¡µJSONè§£ææˆåŠŸï¼Œè·å¾— {page_results_count} ä¸ªåŸå§‹ç»“æœ")

                                    parsed_results = parse_searx_results(data['results'], num_pages_to_crawl, file_type)
                                    processed_count = len(parsed_results)

                                    for result in parsed_results:
                                        result['index'] = len(searx_result_list)
                                        result['page'] = current_page
                                        result['instance'] = search_url  # è®°å½•ä½¿ç”¨çš„å®ä¾‹
                                        searx_result_list.append(result)

                                    logger.info(
                                        f"ç¬¬{current_page}é¡µæˆåŠŸå¤„ç† {processed_count}/{page_results_count} ä¸ªç»“æœ")
                                    page_success = True
                                    consecutive_empty_pages = 0
                                    page_detailed_stats[current_page]['final_instance'] = search_url
                                    page_detailed_stats[current_page]['success'] = True
                                    page_detailed_stats[current_page]['results_count'] = processed_count

                                else:
                                    logger.warning(f"ç¬¬{current_page}é¡µJSONè§£ææœªæ‰¾åˆ°ç»“æœ")
                                    page_error = "JSONè§£ææ— ç»“æœ"
                                    consecutive_empty_pages += 1
                                    attempt_info['error'] = page_error

                            except ValueError as json_error:
                                logger.error(f"ç¬¬{current_page}é¡µJSONè§£æå¤±è´¥: {json_error}")
                                page_error = f"JSONè§£æé”™è¯¯: {json_error}"
                                attempt_info['error'] = page_error

                    elif response.status_code == 403:
                        logger.warning(f"âŒ ç¬¬{current_page}é¡µåœ¨å®ä¾‹ {search_url} ä¸Šè¿”å›403ç¦æ­¢è®¿é—®")
                        instance_stats[search_url]['fail'] += 1
                        instance_stats[search_url]['last_403'] = time.time()  # è®°å½•403æ—¶é—´
                        page_error = f"HTTPé”™è¯¯: 403ç¦æ­¢è®¿é—®"
                        attempt_info['error'] = page_error

                        # å¦‚æœæ˜¯å®ä¾‹2ï¼Œå‡å°‘ä½¿ç”¨é¢‘ç‡
                        if 'searx.bndkt.io' in search_url:
                            logger.info("å®ä¾‹2è¢«é™åˆ¶ï¼Œå°†é™ä½å…¶ä¼˜å…ˆçº§")

                        break  # ç«‹å³åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå®ä¾‹

                    elif response.status_code in [429, 503]:
                        logger.warning(f"âš ï¸ ç¬¬{current_page}é¡µåœ¨å®ä¾‹ {search_url} ä¸Šè¿”å›{response.status_code}é™æµ")
                        instance_stats[search_url]['fail'] += 1
                        page_error = f"HTTPé”™è¯¯: {response.status_code}"
                        attempt_info['error'] = page_error
                        wait_time = (attempt + 1) * 5
                        logger.info(f"é‡åˆ°é™æµï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                        time.sleep(wait_time)

                    else:
                        logger.error(f"âŒ ç¬¬{current_page}é¡µåœ¨å®ä¾‹ {search_url} ä¸ŠHTTPå¤±è´¥: {response.status_code}")
                        instance_stats[search_url]['fail'] += 1
                        page_error = f"HTTPé”™è¯¯: {response.status_code}"
                        attempt_info['error'] = page_error

                    if page_success:
                        break

                except requests.exceptions.Timeout:
                    logger.error(f"â° ç¬¬{current_page}é¡µåœ¨å®ä¾‹ {search_url} ä¸Šè¯·æ±‚è¶…æ—¶")
                    instance_stats[search_url]['fail'] += 1
                    page_error = "è¯·æ±‚è¶…æ—¶"
                    attempt_info['error'] = page_error
                    if attempt < max_attempts - 1:
                        wait_time = (attempt + 1) * 2
                        logger.info(f"ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                        time.sleep(wait_time)

                except requests.exceptions.ConnectionError:
                    logger.error(f"ğŸ”Œ ç¬¬{current_page}é¡µåœ¨å®ä¾‹ {search_url} ä¸Šè¿æ¥é”™è¯¯")
                    instance_stats[search_url]['fail'] += 1
                    page_error = "è¿æ¥é”™è¯¯"
                    attempt_info['error'] = page_error
                    if attempt < max_attempts - 1:
                        wait_time = (attempt + 1) * 3
                        logger.info(f"ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                        time.sleep(wait_time)

                except Exception as e:
                    logger.error(f"ğŸ’¥ ç¬¬{current_page}é¡µåœ¨å®ä¾‹ {search_url} ä¸Šå¼‚å¸¸: {str(e)}")
                    instance_stats[search_url]['fail'] += 1
                    page_error = f"å¼‚å¸¸: {str(e)}"
                    attempt_info['error'] = page_error
                    if attempt < max_attempts - 1:
                        time.sleep(1)

                finally:
                    # è®°å½•è¿™æ¬¡å°è¯•çš„è¯¦ç»†ä¿¡æ¯
                    page_detailed_stats[current_page]['attempts'].append(attempt_info)

        # è®°å½•é¡µé¢çŠ¶æ€
        page_status[current_page] = {
            'success': page_success,
            'results_count': page_results_count,
            'error': page_error,
            'total_so_far': len(searx_result_list)
        }

        # è®°å½•å½“å‰é¡µçš„æœ€ç»ˆçŠ¶æ€
        logger.info(f"=== ç¬¬{current_page}é¡µå¤„ç†æ±‡æ€» ===")
        logger.info(f"æœ€ç»ˆçŠ¶æ€: {'âœ… æˆåŠŸ' if page_success else 'âŒ å¤±è´¥'}")
        if page_success:
            logger.info(f"æˆåŠŸå®ä¾‹: {page_detailed_stats[current_page]['final_instance']}")
        logger.info(f"æœ¬é¡µè·å¾—ç»“æœæ•°: {page_detailed_stats[current_page]['results_count']}")
        logger.info(f"ç´¯è®¡ç»“æœæ•°: {len(searx_result_list)}")

        # è¾“å‡ºå½“å‰é¡µçš„è¯¦ç»†å°è¯•è®°å½•
        logger.info(f"ç¬¬{current_page}é¡µè¯¦ç»†å°è¯•è®°å½•:")
        for attempt in page_detailed_stats[current_page]['attempts']:
            status = "âœ… æˆåŠŸ" if attempt['success'] else f"âŒ å¤±è´¥(çŠ¶æ€ç : {attempt['status_code']})"
            if attempt['error']:
                status += f" - é”™è¯¯: {attempt['error']}"
            logger.info(
                f"  å®ä¾‹{attempt['instance_index']}({attempt['instance']}) å°è¯•{attempt['attempt_number']}: {status}")

        # æ£€æŸ¥åœæ­¢æ¡ä»¶
        if consecutive_empty_pages >= max_consecutive_empty_pages:
            logger.warning(f"è¿ç»­ {consecutive_empty_pages} é¡µæ— ç»“æœï¼Œåœæ­¢åˆ†é¡µ")
            break

        if len(searx_result_list) >= num_pages_to_crawl:
            logger.info(f"å·²æ”¶é›†è¶³å¤Ÿç»“æœ ({len(searx_result_list)}/{num_pages_to_crawl})ï¼Œåœæ­¢åˆ†é¡µ")
            break

        # é¡µé¢é—´å»¶è¿Ÿ
        delay = random.uniform(3.0, 6.0)  # å¢åŠ å»¶è¿Ÿé¿å…é¢‘ç¹è¯·æ±‚
        logger.info(f"ç­‰å¾…{delay:.2f}ç§’åè·å–ä¸‹ä¸€é¡µ...")
        time.sleep(delay)

    # å…³é—­ä¼šè¯å¹¶è¾“å‡ºè¯¦ç»†ç»Ÿè®¡
    session.close()

    # è¾“å‡ºå®ä¾‹ä½¿ç”¨ç»Ÿè®¡
    logger.info("=== å®ä¾‹ä½¿ç”¨ç»Ÿè®¡ ===")
    for url, stats in instance_stats.items():
        total = stats['success'] + stats['fail']
        if total > 0:
            success_rate = stats['success'] / total * 100
        else:
            success_rate = 0
        logger.info(f"å®ä¾‹ {url}: æˆåŠŸ {stats['success']}, å¤±è´¥ {stats['fail']}, æˆåŠŸç‡ {success_rate:.1f}%")

    # è¾“å‡ºè¯¦ç»†é¡µé¢å®ä¾‹ä½¿ç”¨ç»Ÿè®¡
    logger.info("=== è¯¦ç»†é¡µé¢å®ä¾‹ä½¿ç”¨ç»Ÿè®¡ ===")
    successful_pages = 0
    failed_pages = 0

    for page_num, stats in page_detailed_stats.items():
        if stats['success']:
            successful_pages += 1
            logger.info(f"ç¬¬{page_num}é¡µ: âœ… æˆåŠŸ - å®ä¾‹: {stats['final_instance']} - ç»“æœæ•°: {stats['results_count']}")
        else:
            failed_pages += 1
            logger.info(f"ç¬¬{page_num}é¡µ: âŒ å¤±è´¥")

        # è¾“å‡ºè¯¥é¡µçš„æ‰€æœ‰å°è¯•è®°å½•
        for attempt in stats['attempts']:
            status = "æˆåŠŸ" if attempt['success'] else f"å¤±è´¥(çŠ¶æ€ç : {attempt['status_code']})"
            error_info = f" - é”™è¯¯: {attempt['error']}" if attempt['error'] else ""
            logger.info(f"  â†’ å°è¯•{attempt['attempt_number']}: å®ä¾‹{attempt['instance_index']} - {status}{error_info}")

    logger.info(f"=== æœç´¢æ€»ç»“ ===")
    logger.info(f"æˆåŠŸé¡µé¢: {successful_pages}, å¤±è´¥é¡µé¢: {failed_pages}, æ€»é¡µé¢: {pages_to_fetch}")
    logger.info(f"æœ€ç»ˆè·å–ç»“æœæ•°: {len(searx_result_list)}/{num_pages_to_crawl}")

    # æœ€ç»ˆç»“æœå¤„ç†
    if searx_result_list:
        actual_count = len(searx_result_list)
        if actual_count > num_pages_to_crawl:
            searx_result_list = searx_result_list[:num_pages_to_crawl]
            logger.info(f"æˆªæ–­ç»“æœåˆ° {num_pages_to_crawl} ä¸ª")

        return searx_result_list
    else:
        logger.warning("æ‰€æœ‰é¡µé¢æœç´¢å®Œæˆåæœªæ‰¾åˆ°ä»»ä½•ç»“æœ")
        return []



#
#
# def fetch_search_results_with_searx(query: str,
#                                     num_pages_to_crawl: int = 120,
#                                     file_type: str = None,
#                                     offline_search: bool = False):
#     """
#     ç›´æ¥é€šè¿‡HTTPæ¥å£è®¿é—®SearxNGè·å–æœç´¢ç»“æœ
#     æ”¯æŒæ–‡ä»¶ç±»å‹è¿‡æ»¤åŠŸèƒ½å’ŒåŠ¨æ€æ ¼å¼é€‰æ‹©
#
#     å‚æ•°:
#     - query: æœç´¢å…³é”®è¯
#     - num_pages_to_crawl: è¿”å›ç»“æœæ•°é‡
#     - file_type: æ–‡ä»¶ç±»å‹è¿‡æ»¤ï¼ˆå¦‚"pdf"ã€"doc"ç­‰ï¼ŒNoneè¡¨ç¤ºä¸è¿‡æ»¤ï¼‰
#     - engines: æœç´¢å¼•æ“ï¼Œå¦‚ "baidu,bing" æˆ– "elasticsearch" æˆ– "baidu,bing,elasticsearch"
#     """
#     # SearXNGæœåŠ¡å™¨é…ç½®
#     search_url_list = []
#     searx_host_list = []
#     searx_result_list = []
#     accept_header = ''
#     engines = []
#     format_type = ''
#
#     # æ„å»ºæœç´¢æŸ¥è¯¢ï¼Œå¦‚æœæŒ‡å®šäº†æ–‡ä»¶ç±»å‹åˆ™æ·»åŠ filetypeè¿‡æ»¤
#     if file_type in ['doc', 'docx', 'ppt', 'pptx', 'txt', 'pdf']:
#         format_type = 'html'
#         formatted_query = f"{query} filetype:{file_type}"
#         logger.info(f"ä½¿ç”¨æ–‡ä»¶ç±»å‹è¿‡æ»¤è¿›è¡Œæœç´¢: {formatted_query}, é¢„æœŸç»“æœ: {num_pages_to_crawl}")
#         engines = ['google']
#         searx_host_list = ['https://searx.bndkt.io/', 'http://114.213.232.140:18081/']
#     elif file_type in ['pdf']:
#         format_type = 'html'
#         formatted_query = f"{query} filetype:{file_type}"
#         logger.info(f"ä½¿ç”¨æ–‡ä»¶ç±»å‹è¿‡æ»¤è¿›è¡Œæœç´¢: {formatted_query}, é¢„æœŸç»“æœ: {num_pages_to_crawl}")
#         engines = ['baidu', 'bing']
#         searx_host_list = ['http://114.213.232.140:18081/', 'https://searx.bndkt.io/']
#     else:
#         formatted_query = f"{query}"
#         logger.info(f"è¿›è¡Œå¸¸è§„æœç´¢: {formatted_query}, é¢„æœŸç»“æœ: {num_pages_to_crawl}")
#         if offline_search:
#             format_type = 'json'
#             engines.append('elasticsearch')
#             searx_host_list = ['http://114.213.232.140:18081/']
#         else:
#             format_type = 'json'
#             engines.append('360search')
#             engines.append('quark')
#             engines.append('sogou')
#             engines.append('baidu')
#             engines.append('bing')
#             searx_host_list = ['http://114.213.232.140:18081/']#, 'https://searx.bndkt.io/'
#
#     search_url_list = [searx_host + 'search' for searx_host in searx_host_list]
#
#     # ç»Ÿä¸€å¤„ç†æ‰€æœ‰å¼•æ“ï¼ˆåŒ…æ‹¬elasticsearchï¼‰
#     if format_type.lower() == 'html':
#         accept_header = 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
#     else:
#         accept_header = 'application/json'
#
#     engines_str = ','.join(engines)
#
#     # ä½¿ç”¨å¤šä¸ªUser-Agentè½®æ¢
#     user_agents = [
#         'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
#         'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
#         'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
#         'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'
#     ]
#
#     headers = {
#         'User-Agent': user_agents[0],
#         'Accept': accept_header,
#         'Accept-Language': 'en-US,en;q=0.5',
#         'DNT': '1',
#         'Connection': 'keep-alive',
#     }
#
#     max_attempts = 3  # å¢åŠ æœ€å¤§å°è¯•æ¬¡æ•°
#     timeout_config = 3  # å¢åŠ è¶…æ—¶æ—¶é—´
#
#     # è®¡ç®—éœ€è¦è·å–çš„é¡µæ•°
#     results_per_page = 20
#     pages_to_fetch = max(1, (num_pages_to_crawl + results_per_page - 1) // results_per_page)
#
#     logger.info(f"éœ€è¦è·å– {pages_to_fetch} é¡µæ¥æ»¡è¶³ {num_pages_to_crawl} ä¸ªç»“æœçš„éœ€æ±‚")
#
#     # æ·»åŠ é¡µé¢çŠ¶æ€è·Ÿè¸ª
#     page_status = {}  # è®°å½•æ¯é¡µçš„è·å–çŠ¶æ€
#     consecutive_empty_pages = 0  # è¿ç»­ç©ºé¡µè®¡æ•°å™¨
#     max_consecutive_empty_pages = 2  # æœ€å¤§å…è®¸è¿ç»­ç©ºé¡µæ•°
#
#     for page in range(pages_to_fetch):
#         current_page = page + 1
#         logger.info(f"=== å¼€å§‹è·å–ç¬¬ {current_page}/{pages_to_fetch} é¡µ ===")
#
#         params = {
#             'q': formatted_query,
#             'format': format_type,
#             'engines': engines_str,
#             'categories': 'general',
#             'language': 'auto',
#             'pageno': current_page
#         }
#
#         page_success = False
#         page_results_count = 0
#         page_error = None
#
#         # å¯¹æ¯ä¸ªé¡µé¢å°è¯•å¤šä¸ªSearxNGå®ä¾‹
#         for searx_instance_idx, search_url in enumerate(search_url_list):
#             if page_success:
#                 break
#
#             for attempt in range(max_attempts):
#                 try:
#                     # æ¯æ¬¡å°è¯•ä½¿ç”¨ä¸åŒçš„User-Agent
#                     headers['User-Agent'] = user_agents[attempt % len(user_agents)]
#
#                     logger.debug(
#                         f"ç¬¬{current_page}é¡µ å°è¯•{attempt + 1}/{max_attempts} ä½¿ç”¨å®ä¾‹ {searx_instance_idx + 1}/{len(search_url_list)}: {search_url}")
#
#                     # å‘é€HTTPè¯·æ±‚
#                     response = requests.get(
#                         search_url,
#                         params=params,
#                         headers=headers,
#                         timeout=timeout_config,
#                         verify=False,
#                     )
#
#                     # æ£€æŸ¥å“åº”çŠ¶æ€
#                     if response.status_code == 200:
#                         logger.info(f"ç¬¬{current_page}é¡µ HTTPè¯·æ±‚æˆåŠŸï¼ŒçŠ¶æ€ç : 200")
#
#                         if format_type.lower() == 'html':
#                             html_results = parse_searx_html(response.text, num_pages_to_crawl)
#                             page_results_count = len(html_results) if html_results else 0
#
#                             if html_results and page_results_count > 0:
#                                 logger.info(f"ç¬¬{current_page}é¡µHTMLè§£ææˆåŠŸï¼Œè·å¾— {page_results_count} ä¸ªç»“æœ")
#
#                                 # å¤„ç†HTMLç»“æœ
#                                 processed_count = 0
#                                 for i, item in enumerate(html_results):
#                                     url = item.get('url', '')
#                                     title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')
#                                     content = item.get('content', '')
#
#                                     # æ–‡ä»¶ç±»å‹è¿‡æ»¤
#                                     if file_type and not url.lower().endswith(f".{file_type.lower()}"):
#                                         continue
#
#                                     if not content.strip():
#                                         content = f"æ¥è‡ª {title} çš„æœç´¢ç»“æœ"
#
#                                     title_without_ext = title.rsplit('.', 1)[0] if '.' in title else title
#                                     if is_meaningless_filename(title_without_ext):
#                                         file_name = generate_filename_from_content(content, file_type)
#                                     else:
#                                         safe_title = clean_filename(title_without_ext)
#                                         file_name = f"{safe_title}.{file_type.lower()}" if file_type else f"{safe_title}.html"
#
#                                     searx_result_list.append({
#                                         "url": url,
#                                         "title": title,
#                                         "file_name": file_name,
#                                         "index": len(searx_result_list),
#                                         "content": content.strip(),
#                                         "file_type": file_type if file_type else "html",
#                                         "page": current_page  # æ·»åŠ é¡µç ä¿¡æ¯
#                                     })
#                                     processed_count += 1
#
#                                 logger.info(f"ç¬¬{current_page}é¡µæˆåŠŸå¤„ç† {processed_count}/{page_results_count} ä¸ªç»“æœ")
#                                 page_success = True
#
#                             else:
#                                 logger.warning(f"ç¬¬{current_page}é¡µHTMLè§£ææœªæ‰¾åˆ°ç»“æœæˆ–ç»“æœä¸ºç©º")
#                                 page_error = "HTMLè§£ææ— ç»“æœ"
#                                 consecutive_empty_pages += 1
#
#                         else:
#                             # JSONè§£æ
#                             try:
#                                 data = response.json()
#                                 page_results_count = len(data.get('results', []))
#
#                                 if 'results' in data and page_results_count > 0:
#                                     logger.info(f"ç¬¬{current_page}é¡µJSONè§£ææˆåŠŸï¼Œè·å¾— {page_results_count} ä¸ªåŸå§‹ç»“æœ")
#
#                                     # è§£ææœç´¢ç»“æœ
#                                     parsed_results = parse_searx_results(data['results'], num_pages_to_crawl, file_type)
#                                     processed_count = len(parsed_results)
#
#                                     for result in parsed_results:
#                                         result['index'] = len(searx_result_list)
#                                         result['page'] = current_page  # æ·»åŠ é¡µç ä¿¡æ¯
#                                         searx_result_list.append(result)
#
#                                     logger.info(
#                                         f"ç¬¬{current_page}é¡µæˆåŠŸå¤„ç† {processed_count}/{page_results_count} ä¸ªç»“æœ")
#                                     page_success = True
#                                     consecutive_empty_pages = 0  # é‡ç½®è¿ç»­ç©ºé¡µè®¡æ•°
#
#                             except ValueError as json_error:
#                                 logger.error(f"ç¬¬{current_page}é¡µJSONè§£æå¤±è´¥: {json_error}")
#                                 page_error = f"JSONè§£æé”™è¯¯: {json_error}"
#                                 # æ˜¾ç¤ºå“åº”å†…å®¹å‰200å­—ç¬¦ç”¨äºè°ƒè¯•
#                                 logger.debug(f"å“åº”å†…å®¹é¢„è§ˆ: {response.text[:200]}...")
#
#                     elif response.status_code == 403:
#                         logger.warning(
#                             f"ç¬¬{current_page}é¡µ å®ä¾‹ {searx_instance_idx + 1} è¿”å›403ç¦æ­¢è®¿é—®ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå®ä¾‹")
#                         page_error = f"HTTPé”™è¯¯: 403ç¦æ­¢è®¿é—®"
#                         # è·³è¿‡å½“å‰å®ä¾‹ï¼Œå°è¯•ä¸‹ä¸€ä¸ª
#                         break
#
#                     else:
#                         logger.error(f"ç¬¬{current_page}é¡µHTTPè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
#                         page_error = f"HTTPé”™è¯¯: {response.status_code}"
#                         logger.debug(f"é”™è¯¯å“åº”: {response.text[:200]}...")
#
#                     # å¦‚æœæœ¬æ¬¡å°è¯•æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
#                     if page_success:
#                         break
#
#                 except requests.exceptions.Timeout:
#                     logger.error(f"ç¬¬{current_page}é¡µ å°è¯•{attempt + 1} è¯·æ±‚è¶…æ—¶")
#                     page_error = "è¯·æ±‚è¶…æ—¶"
#                     if attempt < max_attempts - 1:
#                         wait_time = (attempt + 1) * 1  # æŒ‡æ•°é€€é¿ç­–ç•¥ æ”¹æˆ2
#                         logger.info(f"ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
#                         time.sleep(wait_time)
#
#                 except requests.exceptions.ConnectionError:
#                     logger.error(f"ç¬¬{current_page}é¡µ å°è¯•{attempt + 1} è¿æ¥é”™è¯¯")
#                     page_error = "è¿æ¥é”™è¯¯"
#                     if attempt < max_attempts - 1:
#                         wait_time = (attempt + 1) * 1  # æŒ‡æ•°é€€é¿ç­–ç•¥ æ”¹æˆ3
#                         logger.info(f"ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
#                         time.sleep(wait_time)
#
#                 except requests.exceptions.RequestException as e:
#                     logger.error(f"ç¬¬{current_page}é¡µ å°è¯•{attempt + 1} ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {str(e)}")
#                     page_error = f"ç½‘ç»œå¼‚å¸¸: {str(e)}"
#                     if attempt < max_attempts - 1:
#                         wait_time = (attempt + 1) * 1  # æŒ‡æ•°é€€é¿ç­–ç•¥
#                         logger.info(f"ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
#                         time.sleep(wait_time)
#
#                 except Exception as e:
#                     logger.error(f"ç¬¬{current_page}é¡µ å°è¯•{attempt + 1} æœªçŸ¥å¼‚å¸¸: {str(e)}")
#                     page_error = f"æœªçŸ¥å¼‚å¸¸: {str(e)}"
#                     if attempt < max_attempts - 1:
#                         logger.info(f"ç­‰å¾…1ç§’åé‡è¯•...")
#                         time.sleep(1)
#
#         # è®°å½•é¡µé¢çŠ¶æ€
#         page_status[current_page] = {
#             'success': page_success,
#             'results_count': page_results_count,
#             'error': page_error,
#             'total_so_far': len(searx_result_list)
#         }
#
#         logger.info(f"ç¬¬{current_page}é¡µçŠ¶æ€: {'æˆåŠŸ' if page_success else 'å¤±è´¥'}, "
#                     f"æœ¬é¡µç»“æœ: {page_results_count}, ç´¯è®¡ç»“æœ: {len(searx_result_list)}")
#
#         if page_error:
#             logger.warning(f"ç¬¬{current_page}é¡µé”™è¯¯è¯¦æƒ…: {page_error}")
#
#         # æ£€æŸ¥åœæ­¢æ¡ä»¶
#         if consecutive_empty_pages >= max_consecutive_empty_pages:
#             logger.warning(f"è¿ç»­ {consecutive_empty_pages} é¡µæ— ç»“æœï¼Œåœæ­¢åˆ†é¡µ")
#             break
#
#         if len(searx_result_list) >= num_pages_to_crawl:
#             logger.info(f"å·²æ”¶é›†è¶³å¤Ÿç»“æœ ({len(searx_result_list)}/{num_pages_to_crawl})ï¼Œåœæ­¢åˆ†é¡µ")
#             break
#
#         # é¡µé¢é—´å»¶è¿Ÿ - å¢åŠ éšæœºæ€§é¿å…è¢«æ£€æµ‹ä¸ºæœºå™¨äºº
#         delay = random.uniform(1, 3)#####1.5   3
#         logger.info(f"ç­‰å¾…{delay:.2f}ç§’åè·å–ä¸‹ä¸€é¡µ...")
#         time.sleep(delay)
#
#     # ç”Ÿæˆè¯¦ç»†çš„é¡µé¢çŠ¶æ€æŠ¥å‘Š
#     logger.info("=== åˆ†é¡µæœç´¢å®Œæˆ ===")
#     logger.info(f"æ€»å…±å°è¯•è·å– {len(page_status)} é¡µ")
#
#     successful_pages = sum(1 for status in page_status.values() if status['success'])
#     failed_pages = len(page_status) - successful_pages
#
#     logger.info(f"æˆåŠŸé¡µæ•°: {successful_pages}, å¤±è´¥é¡µæ•°: {failed_pages}")
#     logger.info(f"æœ€ç»ˆç»“æœæ€»æ•°: {len(searx_result_list)}")
#
#     # è¾“å‡ºè¯¦ç»†çš„é¡µé¢çŠ¶æ€
#     for page_num, status in page_status.items():
#         status_symbol = "âœ“" if status['success'] else "âœ—"
#         logger.info(f"ç¬¬{page_num}é¡µ {status_symbol} - ç»“æœ: {status['results_count']}, "
#                     f"ç´¯è®¡: {status['total_so_far']}" +
#                     (f", é”™è¯¯: {status['error']}" if status['error'] else ""))
#
#     # æœ€ç»ˆç»“æœå¤„ç†
#     if searx_result_list:
#         actual_count = len(searx_result_list)
#         if actual_count > num_pages_to_crawl:
#             searx_result_list = searx_result_list[:num_pages_to_crawl]
#             logger.info(f"æˆªæ–­ç»“æœåˆ° {num_pages_to_crawl} ä¸ª")
#
#         return searx_result_list
#     else:
#         logger.warning("æ‰€æœ‰é¡µé¢æœç´¢å®Œæˆåæœªæ‰¾åˆ°ä»»ä½•ç»“æœ")
#         # è¾“å‡ºæ‰€æœ‰é¡µé¢çš„é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
#         for page_num, status in page_status.items():
#             if status['error']:
#                 logger.debug(f"ç¬¬{page_num}é¡µé”™è¯¯: {status['error']}")
#         return []


#
#
# def fetch_search_results_with_searx(query: str,
#                                     num_pages_to_crawl: int = 100,
#                                     file_type: str = None,
#                                     offline_search: bool = False):
#     """
#     ç›´æ¥é€šè¿‡HTTPæ¥å£è®¿é—®SearxNGè·å–æœç´¢ç»“æœ
#     æ”¯æŒæ–‡ä»¶ç±»å‹è¿‡æ»¤åŠŸèƒ½å’ŒåŠ¨æ€æ ¼å¼é€‰æ‹©
#
#     å‚æ•°:
#     - query: æœç´¢å…³é”®è¯
#     - num_pages_to_crawl: è¿”å›ç»“æœæ•°é‡
#     - file_type: æ–‡ä»¶ç±»å‹è¿‡æ»¤ï¼ˆå¦‚"pdf"ã€"doc"ç­‰ï¼ŒNoneè¡¨ç¤ºä¸è¿‡æ»¤ï¼‰
#     - engines: æœç´¢å¼•æ“ï¼Œå¦‚ "baidu,bing" æˆ– "elasticsearch" æˆ– "baidu,bing,elasticsearch"
#     """
#     # SearXNGæœåŠ¡å™¨é…ç½®
#     search_url_list = []
#     searx_host_list = []
#     searx_result_list = []
#     accept_header = ''
#     engines = []
#     format_type = ''
#     # æ„å»ºæœç´¢æŸ¥è¯¢ï¼Œå¦‚æœæŒ‡å®šäº†æ–‡ä»¶ç±»å‹åˆ™æ·»åŠ filetypeè¿‡æ»¤
#     if file_type in ['doc', 'docx', 'ppt', 'pptx', 'txt', 'pdf']:
#         format_type = 'html'
#         formatted_query = f"{query} filetype:{file_type}"
#         logger.info(f"ä½¿ç”¨æ–‡ä»¶ç±»å‹è¿‡æ»¤è¿›è¡Œæœç´¢: {formatted_query}, é¢„æœŸç»“æœ: {num_pages_to_crawl}")
#         engines = ['google']
#         searx_host_list = ['https://searx.bndkt.io/', 'http://114.213.232.140:18081/']
#     elif file_type in ['pdf']:
#         format_type = 'html'
#         formatted_query = f"{query} filetype:{file_type}"
#         logger.info(f"ä½¿ç”¨æ–‡ä»¶ç±»å‹è¿‡æ»¤è¿›è¡Œæœç´¢: {formatted_query}, é¢„æœŸç»“æœ: {num_pages_to_crawl}")
#         engines = ['baidu', 'bing']
#         searx_host_list = ['http://114.213.232.140:18081/', 'https://searx.bndkt.io/']
#     else:
#         formatted_query = f"{query}"
#         logger.info(f"è¿›è¡Œå¸¸è§„æœç´¢: {formatted_query}, é¢„æœŸç»“æœ: {num_pages_to_crawl}")
#         if offline_search:
#             format_type = 'json'
#             engines.append('elasticsearch')
#             searx_host_list = ['http://114.213.232.140:18081/']
#         else:
#             format_type = 'json'
#             engines.append('baidu')
#             engines.append('bing')
#             searx_host_list = ['http://114.213.232.140:18081/', 'https://searx.bndkt.io/']
#     search_url_list = [searx_host + 'search' for searx_host in searx_host_list]
#     # ç»Ÿä¸€å¤„ç†æ‰€æœ‰å¼•æ“ï¼ˆåŒ…æ‹¬elasticsearchï¼‰
#     if format_type.lower() == 'html':
#         accept_header = 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
#     else:
#         accept_header = 'application/json'
#     engines_str = ','.join(engines)
#     params = {
#         'q': formatted_query,
#         'format': format_type,
#         'engines': engines_str,
#         'categories': 'general',
#         'language': 'auto',
#     }
#     headers = {
#         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
#         'Accept': accept_header,
#     }
#
#     max_attempts = 3
#     timeout_config = 10
#     for attempt in range(max_attempts):
#         try:
#             search_url = search_url_list[attempt % len(search_url_list)]
#
#             # å‘é€HTTPè¯·æ±‚
#             response = requests.get(
#                 search_url,
#                 params=params,
#                 headers=headers,
#                 timeout=timeout_config,
#                 verify=False,  # ç¦ç”¨SSLéªŒè¯ï¼Œé€‚ç”¨äºæœ¬åœ°å¼€å‘
#             )
#
#             # æ£€æŸ¥å“åº”çŠ¶æ€
#             if response.status_code == 200:
#                 logger.info(f"æˆåŠŸè·å–SearxNGå“åº”ï¼ŒçŠ¶æ€ç : {response.status_code}, æ ¼å¼: {format_type}")
#
#                 if format_type.lower() == 'html':
#                     html_results = parse_searx_html(response.text, num_pages_to_crawl)
#
#                     if html_results:
#                         logger.info(f"HTMLè§£ææˆåŠŸï¼Œè·å¾— {len(html_results)} ä¸ªåˆæ­¥ç»“æœ")
#
#                         # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼å¹¶è¿›è¡Œæ–‡ä»¶ç±»å‹è¿‡æ»¤
#                         result_list = []
#                         for i, item in enumerate(html_results):
#                             url = item.get('url', '')
#                             title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')
#                             content = item.get('content', '')
#
#                             # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶ç±»å‹ï¼Œè¿›è¡Œè¿‡æ»¤
#                             if file_type:
#                                 if not url.lower().endswith(f".{file_type.lower()}"):
#                                     logger.debug(f"è·³è¿‡ä¸åŒ¹é…æ–‡ä»¶ç±»å‹çš„URL: {url} (æœŸæœ›: {file_type})")
#                                     continue
#
#                             # ç¡®ä¿contentä¸ä¸ºç©º
#                             if not content.strip():
#                                 content = f"æ¥è‡ª {title} çš„æœç´¢ç»“æœ"
#
#                             title_without_ext = title.rsplit('.', 1)[0] if '.' in title else title
#                             if is_meaningless_filename(title_without_ext):
#                                 logger.debug(f"æ£€æµ‹åˆ°æ— æ„ä¹‰æ–‡ä»¶å '{title}'ï¼Œä½¿ç”¨contentç”Ÿæˆæ–‡ä»¶å")
#                                 file_name = generate_filename_from_content(content, file_type)
#                             else:
#                                 # ä½¿ç”¨åŸå§‹titleä½œä¸ºæ–‡ä»¶å
#                                 safe_title = clean_filename(title_without_ext)
#                                 if file_type:
#                                     file_name = f"{safe_title}.{file_type.lower()}"
#                                 else:
#                                     file_name = f"{safe_title}.html"
#
#                             searx_result_list.append({
#                                 "url": url,
#                                 "title": title,
#                                 "file_name": file_name,
#                                 "index": i,
#                                 "content": content.strip(),
#                                 "file_type": file_type if file_type else "html"
#                             })
#
#                         if searx_result_list:
#                             actual_count = len(searx_result_list)
#                             logger.info(f"HTMLè§£ææˆåŠŸï¼Œè·å¾— {actual_count}/{num_pages_to_crawl} ä¸ªæœç´¢ç»“æœ" +
#                                         (f"ï¼ˆæ–‡ä»¶ç±»å‹: {file_type}ï¼‰" if file_type else ""))
#                             return searx_result_list
#                         else:
#                             logger.warning(f"æ–‡ä»¶ç±»å‹è¿‡æ»¤åç»“æœä¸ºç©º: {formatted_query}" +
#                                            (f"ï¼ˆæ–‡ä»¶ç±»å‹: {file_type}ï¼‰" if file_type else ""))
#                     else:
#                         logger.warning(f"HTMLè§£ææœªæ‰¾åˆ°ç»“æœ")
#                         # æ˜¾ç¤ºéƒ¨åˆ†HTMLå†…å®¹ç”¨äºè°ƒè¯•
#                         html_preview = response.text[:500].replace('\n', ' ')
#                         logger.debug(f"HTMLé¢„è§ˆ: {html_preview}...")
#
#                 else:
#                     # JSONè§£æï¼ˆé»˜è®¤ï¼‰
#                     try:
#                         # è§£æJSONå“åº”
#                         data = response.json()
#
#                         # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœ
#                         if 'results' in data:
#                             search_results = data['results']
#                             logger.info(f"SearxNGåŸå§‹è¿”å›ç»“æœæ•°é‡: {len(search_results)}")
#
#                             # åˆ†æåŸå§‹ç»“æœçš„åŸºæœ¬ä¿¡æ¯
#                             valid_count = 0
#                             invalid_count = 0
#                             for i, item in enumerate(search_results):
#                                 if isinstance(item, dict):
#                                     url = item.get('url') or item.get('link') or item.get('href')
#                                     if url and url.startswith("http"):
#                                         valid_count += 1
#                                     else:
#                                         invalid_count += 1
#                                         logger.debug(f"å‘ç°æ— æ•ˆURL (ç´¢å¼•{i}): {url}")
#
#                             logger.info(f"åŸå§‹ç»“æœåˆ†æ - æœ‰æ•ˆURL: {valid_count}, æ— æ•ˆURL: {invalid_count}")
#
#                             # è§£ææœç´¢ç»“æœ
#                             searx_result_list.extend(parse_searx_results(search_results, num_pages_to_crawl, file_type))
#
#                             if searx_result_list:
#                                 actual_count = len(searx_result_list)
#                                 logger.info(f"JSONè§£ææˆåŠŸï¼Œè·å¾— {actual_count}/{num_pages_to_crawl} ä¸ªæœç´¢ç»“æœ" +
#                                             (f"ï¼ˆæ–‡ä»¶ç±»å‹: {file_type}ï¼‰" if file_type else ""))
#
#                                 return searx_result_list
#                             else:
#                                 logger.warning(f"æœç´¢ç»“æœè§£æåä¸ºç©º: {formatted_query}")
#                         else:
#                             logger.warning(f"SearxNGå“åº”ä¸­æ²¡æœ‰æ‰¾åˆ°resultså­—æ®µ")
#                             logger.debug(f"å“åº”å†…å®¹: {data}")
#
#                     except ValueError as json_error:
#                         logger.error(f"è§£æSearxNGå“åº”JSONå¤±è´¥: {json_error}")
#                         logger.debug(f"å“åº”å†…å®¹: {response.text[:500]}...")
#
#             else:
#                 logger.error(f"SearxNG HTTPè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
#                 logger.debug(f"å“åº”å†…å®¹: {response.text[:500]}...")
#
#             # å¦‚æœæœ¬æ¬¡å°è¯•å¤±è´¥ï¼Œç­‰å¾…åé‡è¯•
#             if attempt < max_attempts - 1:
#                 logger.info(f"ç­‰å¾… 1 ç§’åé‡è¯•...")
#
#         except requests.exceptions.RequestException as e:
#             logger.error(f"SearxNG HTTPè¯·æ±‚å¼‚å¸¸ ({attempt + 1}/{max_attempts}): {str(e)}")
#             if attempt < max_attempts - 1:
#                 logger.info(f"ç­‰å¾… 1 ç§’åé‡è¯•...")
#                 time.sleep(1)
#         except Exception as e:
#             logger.error(f"SearxNGæœç´¢è¿‡ç¨‹å¼‚å¸¸ ({attempt + 1}/{max_attempts}): {str(e)}")
#             if attempt < max_attempts - 1:
#                 logger.info(f"ç­‰å¾… 1 ç§’åé‡è¯•...")
#                 time.sleep(1)
#
#     logger.error(f"åœ¨ {max_attempts} æ¬¡å°è¯•åä»æ— æ³•è·å–æœç´¢ç»“æœ")
#     return []


def parse_searx_results(search_results, num_pages_to_crawl: int, file_type: str = None):
    """
    è§£æSearXNGæœç´¢ç»“æœ
    å¢åŠ æ–‡ä»¶ç±»å‹éªŒè¯åŠŸèƒ½

    å‚æ•°:
    - search_results: æœç´¢ç»“æœæ•°æ®
    - num_pages_to_crawl: éœ€è¦çš„ç»“æœæ•°é‡
    - file_type: æŒ‡å®šçš„æ–‡ä»¶ç±»å‹ï¼Œç”¨äºé¢å¤–éªŒè¯
    """
    try:
        result_list = []

        # SearxSearchWrapper.results()ç›´æ¥è¿”å›ç»“æ„åŒ–çš„ç»“æœåˆ—è¡¨
        if isinstance(search_results, list):
            results = search_results[:num_pages_to_crawl]
        elif isinstance(search_results, dict) and 'results' in search_results:
            results = search_results['results'][:num_pages_to_crawl]
        elif isinstance(search_results, str):
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºJSON
            try:
                import json
                data = json.loads(search_results)
                if isinstance(data, dict) and 'results' in data:
                    results = data['results'][:num_pages_to_crawl]
                elif isinstance(data, list):
                    results = data[:num_pages_to_crawl]
                else:
                    logger.error(f"æœªçŸ¥çš„JSONæœç´¢ç»“æœæ ¼å¼: {type(data)}")
                    return []
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯JSONï¼Œä½¿ç”¨æ–‡æœ¬è§£æ
                return parse_text_results(search_results, num_pages_to_crawl, file_type)
        else:
            logger.error(f"æœªçŸ¥çš„æœç´¢ç»“æœæ ¼å¼: {type(search_results)}")
            return []

        for i, item in enumerate(results):
            if isinstance(item, dict):
                engine = item.get('engine', 'unknown')

                # å¤„ç†elasticsearchå¼•æ“çš„ç‰¹æ®Šæ ¼å¼
                if engine == 'elasticsearch' and item.get('template') == 'keyvalue.html' and 'kvmap' in item:
                    # ä»kvmapä¸­æå–æ•°æ®
                    kvmap = item['kvmap']
                    title = kvmap.get('title', 'æœªçŸ¥æ ‡é¢˜')
                    url = kvmap.get('url', '')
                    content = kvmap.get('content', '')
                    file_type_es = kvmap.get('file_type', 'html')
                    minio_path = kvmap.get('minio_path', '')
                    upload_time = kvmap.get('upload_time', '')
                    file_name = kvmap.get('minio_path', '').split('/')[-1] if minio_path else title

                    logger.debug(f"[ES] å¤„ç†kvmapæ ¼å¼æ•°æ®: {title}")
                else:
                    # å¤„ç†æ ‡å‡†æ ¼å¼çš„æœç´¢ç»“æœ
                    # å°è¯•å¤šä¸ªå¯èƒ½çš„URLå­—æ®µå
                    url = item.get('link') or item.get('url') or item.get('href')
                    title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')

                    content = (item.get('snippet') or
                               item.get('content') or
                               item.get('description') or
                               item.get('abstract') or
                               f"æ¥è‡ª {title} çš„æœç´¢ç»“æœ")  # é»˜è®¤å†…å®¹

                    file_name = None  # å°†åœ¨åé¢å¤„ç†

                if url:
                    if url.startswith('http:http'):
                        url = url.replace('http:http', 'http')
                    elif url.startswith('https:https'):
                        url = url.replace('https:https', 'https')

                    # URLè§£ç 
                    try:
                        from urllib.parse import unquote
                        if '%' in url:
                            url = unquote(url)
                    except:
                        pass

                    # éªŒè¯URLæ ¼å¼
                    if url.startswith("http"):
                        # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶ç±»å‹ï¼Œè¿›è¡Œé¢å¤–éªŒè¯ï¼ˆå‚è€ƒtest.pyçš„é€»è¾‘ï¼‰
                        if file_type:
                            if not url.lower().endswith(f".{file_type.lower()}"):
                                logger.debug(f"è·³è¿‡ä¸åŒ¹é…æ–‡ä»¶ç±»å‹çš„URL: {url} (æœŸæœ›: {file_type})")
                                continue

                        # ç¡®ä¿contentä¸ä¸ºç©º
                        if not content.strip():
                            content = f"æ¥è‡ª {title} çš„æœç´¢ç»“æœ"

                        # åªå¯¹éelasticsearchå¼•æ“è¿›è¡Œæ™ºèƒ½æ–‡ä»¶å‘½å
                        if engine != 'elasticsearch' and file_name is None:
                            # æ™ºèƒ½æ–‡ä»¶å‘½åï¼šæ£€æŸ¥titleæ˜¯å¦ä¸ºä¹±ç ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨content
                            title_without_ext = title.rsplit('.', 1)[0] if '.' in title else title
                            if is_meaningless_filename(title_without_ext):
                                logger.debug(f"æ£€æµ‹åˆ°æ— æ„ä¹‰æ–‡ä»¶å '{title}'ï¼Œä½¿ç”¨contentç”Ÿæˆæ–‡ä»¶å")
                                file_name = generate_filename_from_content(content, file_type)
                            else:
                                # ä½¿ç”¨åŸå§‹titleä½œä¸ºæ–‡ä»¶å
                                safe_title = clean_filename(title_without_ext)
                                if file_type:
                                    file_name = f"{safe_title}.{file_type.lower()}"
                                else:
                                    file_name = f"{safe_title}.html"

                        # ç¡®å®šæ¥æºæ ‡è¯†å’Œæ–‡ä»¶ç±»å‹
                        if engine == 'elasticsearch':
                            source = "elasticsearch"
                            # å¯¹äºelasticsearchå¼•æ“ï¼Œä½¿ç”¨kvmapä¸­çš„file_type
                            result_file_type = locals().get('file_type_es', 'html')
                        else:
                            source = f"web-{engine}"
                            result_file_type = file_type if file_type else "html"

                        result_list.append({
                            "url": url,
                            "title": title,
                            "file_name": file_name,
                            "index": i,
                            "content": content.strip(),
                            "file_type": result_file_type,
                            "source": source,  # æ ‡è¯†æ¥æº
                            "engine": engine,  # æœç´¢å¼•æ“ä¿¡æ¯
                            "score": item.get('score', 0)  # æ·»åŠ è¯„åˆ†ä¿¡æ¯
                        })
                        logger.debug(f"[{engine.upper()}] æ·»åŠ æœç´¢ç»“æœ: {title} - {url}" +
                                     (f" (æ–‡ä»¶ç±»å‹: {result_file_type})" if result_file_type else ""))
                    else:
                        logger.warning(f"è·³è¿‡æ— æ•ˆURL: {url}")
                else:
                    logger.warning(f"è·³è¿‡ç¼ºå¤±URLçš„ç»“æœ: {title}")
            else:
                logger.warning(f"è·³è¿‡éå­—å…¸æ ¼å¼çš„ç»“æœé¡¹: {type(item)}")

        logger.info(f"æˆåŠŸè§£æ {len(result_list)} ä¸ªæœ‰æ•ˆæœç´¢ç»“æœ" +
                    (f"ï¼ˆæ–‡ä»¶ç±»å‹: {file_type}ï¼‰" if file_type else ""))

        # æŒ‰ç…§scoreè¿›è¡Œé™åºæ’åºï¼ˆåˆ†æ•°è¶Šé«˜æ’åœ¨å‰é¢ï¼‰
        result_list.sort(key=lambda x: x.get('score', 0), reverse=True)

        return result_list

    except Exception as e:
        logger.error(f"è§£ææœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}")
        return []


def parse_text_results(text_results: str, num_pages_to_crawl: int, file_type: str = None):
    """
    è§£ææ–‡æœ¬æ ¼å¼çš„æœç´¢ç»“æœ
    æ”¯æŒæ–‡ä»¶ç±»å‹è¿‡æ»¤

    å‚æ•°:
    - text_results: æ–‡æœ¬æ ¼å¼çš„æœç´¢ç»“æœ
    - num_pages_to_crawl: éœ€è¦çš„ç»“æœæ•°é‡
    - file_type: æŒ‡å®šçš„æ–‡ä»¶ç±»å‹ï¼Œç”¨äºè®¾ç½®æ–‡ä»¶æ‰©å±•å
    """
    try:
        result_list = []
        lines = text_results.split('\n')

        current_result = {}
        result_count = 0

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # ç®€å•çš„æ–‡æœ¬è§£æé€»è¾‘ï¼Œæ ¹æ®å®é™…è¿”å›æ ¼å¼è°ƒæ•´
            if line.startswith('Title:'):
                if current_result and 'title' in current_result:
                    # ä¿å­˜ä¸Šä¸€ä¸ªç»“æœ
                    if result_count < num_pages_to_crawl:
                        result_list.append(format_result(current_result, result_count, file_type))
                        result_count += 1

                current_result = {'title': line[6:].strip()}
            elif line.startswith('URL:') or line.startswith('Link:'):
                current_result['url'] = line.split(':', 1)[1].strip()

            elif line.startswith('Content:') or line.startswith('Snippet:'):
                current_result['content'] = line.split(':', 1)[1].strip()

        if current_result and 'title' in current_result and result_count < num_pages_to_crawl:
            result_list.append(format_result(current_result, result_count, file_type))

        # æŒ‰ç…§scoreè¿›è¡Œé™åºæ’åºï¼ˆåˆ†æ•°è¶Šé«˜æ’åœ¨å‰é¢ï¼‰
        result_list.sort(key=lambda x: x.get('score', 0), reverse=True)
        logger.info(f"æ–‡æœ¬ç»“æœå·²æŒ‰è¯„åˆ†æ’åºï¼Œå…± {len(result_list)} ä¸ªç»“æœ")

        return result_list

    except Exception as e:
        logger.error(f"è§£ææ–‡æœ¬æœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}")
        return []


def format_result(result_data: dict, index: int, file_type: str = None):
    """
    æ ¼å¼åŒ–å•ä¸ªæœç´¢ç»“æœ
    æ”¯æŒæ ¹æ®æ–‡ä»¶ç±»å‹è®¾ç½®åˆé€‚çš„æ–‡ä»¶æ‰©å±•å

    å‚æ•°:
    - result_data: æœç´¢ç»“æœæ•°æ®
    - index: ç»“æœç´¢å¼•
    - file_type: æ–‡ä»¶ç±»å‹ï¼Œç”¨äºè®¾ç½®æ–‡ä»¶æ‰©å±•å
    """
    title = result_data.get('title', 'æœªçŸ¥æ ‡é¢˜')
    url = result_data.get('url', '')
    content = result_data.get('content', '')

    # ç¡®ä¿contentä¸ä¸ºç©º
    if not content.strip():
        content = f"æ¥è‡ª {title} çš„æœç´¢ç»“æœ"

    # æ™ºèƒ½æ–‡ä»¶å‘½åï¼šæ£€æŸ¥titleæ˜¯å¦ä¸ºä¹±ç ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨content
    title_without_ext = title.rsplit('.', 1)[0] if '.' in title else title
    if is_meaningless_filename(title_without_ext):
        file_name = generate_filename_from_content(content, file_type)
    else:
        # ä½¿ç”¨åŸå§‹titleä½œä¸ºæ–‡ä»¶å
        safe_title = clean_filename(title_without_ext)
        if file_type:
            file_name = f"{safe_title}.{file_type.lower()}"
        else:
            file_name = f"{safe_title}.html"

    return {
        "url": url,
        "title": title,
        "file_name": file_name,
        "index": index,
        "content": content,
        "file_type": file_type if file_type else "html",
        "score": 0  # æ–‡æœ¬æ ¼å¼è§£æçš„ç»“æœé»˜è®¤åˆ†æ•°ä¸º0
    }


def enhance_keywords_with_domain(name, xiaoqi_name):
    """
    æ ¹æ®å°å¥‡é¡¹ç›®åç§°æŸ¥è¯¢æ•°æ®åº“ï¼Œè·å–ä¸“ä¸šé¢†åŸŸä¿¡æ¯å¹¶ä¸å…³é”®è¯æ‹¼æ¥

    å‚æ•°:
    - name: åŸå§‹æœç´¢å…³é”®è¯
    - xiaoqi_name: å°å¥‡é¡¹ç›®åç§°

    è¿”å›:
    - str: å¢å¼ºåçš„å…³é”®è¯ï¼Œå¦‚æœæŸ¥è¯¢å¤±è´¥åˆ™è¿”å›åŸå§‹å…³é”®è¯
    """
    if not xiaoqi_name:
        logger.warning("æœªæä¾›å°å¥‡é¡¹ç›®åç§°ï¼Œä½¿ç”¨åŸå§‹å…³é”®è¯")
        return name

    try:
        # è¿æ¥æ•°æ®åº“
        db = MySQLDatabase(
            host="114.213.234.179",
            user="koroot",
            password="DMiC-4092",
            database="db_hp"
        )
        db.connect()

        with db.connection.cursor() as cursor:
            # æŸ¥è¯¢xiaoqi_newè¡¨çš„key_wordså­—æ®µ
            query_sql = "SELECT key_words FROM xiaoqi_new WHERE xiaoqi_name = %s"
            cursor.execute(query_sql, (xiaoqi_name,))
            result = cursor.fetchone()

            if result and result[0]:
                # æ£€æŸ¥key_wordså­—æ®µæ˜¯å¦ä¸ºç©ºæˆ–None
                keywords_str = result[0]
                if not keywords_str or keywords_str.strip() == '':
                    logger.warning(f"é¡¹ç›® {xiaoqi_name} çš„å…³é”®è¯ä¿¡æ¯ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹å…³é”®è¯")
                    return name

                # æ¸…ç†å…³é”®è¯å­—ç¬¦ä¸²
                cleaned_keywords = keywords_str.strip()
                # å»æ‰å¤–å±‚å¼•å· (åŒ…æ‹¬å„ç§Unicodeå¼•å·)
                if (cleaned_keywords.startswith(("'", '"', ''', '"', ''', '"')) and
                        cleaned_keywords.endswith(("'", '"', ''', '"', ''', '"'))):
                    cleaned_keywords = cleaned_keywords[1:-1]

                # å»æ‰æ–¹æ‹¬å·
                if cleaned_keywords.startswith('[') and cleaned_keywords.endswith(']'):
                    cleaned_keywords = cleaned_keywords[1:-1]

        db.connection.close()

    except Exception as e:
        logger.error(f"æŸ¥è¯¢ä¸“ä¸šé¢†åŸŸä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        # æŸ¥è¯¢å¤±è´¥æ—¶ç»§ç»­ä½¿ç”¨åŸå§‹å…³é”®è¯
        return name

    return name


# # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿè¯·æ±‚ç±»
# class MockRequest:
#     def __init__(self, params_dict):
#         self.GET = params_dict
# #
# # è®¾ç½®è¯·æ±‚å‚æ•°
# params = {
#     "name": "å´ä¿¡ä¸œ",
#     "num_pages_to_crawl": "40",
# }
# #å®‰å¾½å¤§å­¦
# # åˆ›å»ºè¯·æ±‚å¯¹è±¡å¹¶è°ƒç”¨å‡½æ•°
# request = MockRequest(params)
# t0 = time.time()
# result = search_urls(request)
# t1 = time.time()
# print("æœç´¢æ—¶é—´:", t1-t0)
# print("æœç´¢çŠ¶æ€:", result["status"])
# print("æœç´¢ç»“æœ:", result)
# unique_urls_count = len(result["data"]["unique_urls"])
# print(f"å”¯ä¸€URLæ•°é‡: {unique_urls_count}")



